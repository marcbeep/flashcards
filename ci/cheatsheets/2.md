---
## ğŸ§  Neural Networks Cheat Sheet â€“ COMP 575 (Chapter 2)
---

### 1. ğŸ§© **Structure of a Neuron**

Each neuron is a processing unit with three main components:

- **Synapses / Connections**:

  - Inputs $x_j$ come through synapses, each with a weight $w_{kj}$.
  - Each synapse applies a scaling: $x_j \cdot w_{kj}$.

- **Adder (Summing Junction)**:

  - All weighted inputs are summed:
    $u_k = \sum_{j=1}^{p} w_{kj} x_j$
    or $u_k = \mathbf{w}_k^T \mathbf{x}$

- **Activation Function**:

  - Applies non-linearity:
    $y_k = \phi(u_k)$

---

### 2. âš–ï¸ **Bias Term ($\theta_k$)**

- **Purpose**: Shifts activation threshold, improves flexibility.

- **Modified Equation**:
  $y_k = \phi(\mathbf{w}_k^T \mathbf{x} - \theta_k)$

- **Alternative Form** (with fixed input $x_0 = -1$):

  - Let $w_{k0} = \theta_k$ â†’ included in the weight vector.
  - New input vector: $\mathbf{x} = [-1, x_1, ..., x_p]$

- **Effect**:
  $v_k = u_k - \theta_k$: the _induced local field_

---

### 3. ğŸ”€ **Activation Functions**

> I wonder if we have to learn these.

#### ğŸ“Œ Heaviside (Threshold)

- $\phi(v) = \begin{cases} 1 & v \geq 0 \\ 0 & v < 0 \end{cases}$
- Binary, non-differentiable, good for theoretical models

#### ğŸ“Œ Piecewise Linear

- $\phi(v) = \begin{cases} 1 & v \geq \frac{1}{2} \\ v + \frac{1}{2} & -\frac{1}{2} < v < \frac{1}{2} \\ 0 & v \leq -\frac{1}{2} \end{cases}$
- Smooth approximation of threshold

#### ğŸ“Œ Sigmoid (Logistic)

- $\phi(v) = \frac{1}{1 + e^{-\alpha v}}$
- Differentiable, output in (0,1), used for binary classification

#### ğŸ“Œ Tanh

- $\phi(v) = \tanh(v) = \frac{e^{2v} - 1}{e^{2v} + 1}$
- Output in (â€“1, 1), zero-centered

#### ğŸ“Œ ReLU

- $\phi(v) = \max(0, v)$
- Fast to compute, used in modern deep nets (e.g., CNNs)
- **Issue**: Dead neurons (if $v < 0$ permanently)

---

### 4. ğŸ—ï¸ **Network Architectures**

#### ğŸ“˜ Single-Layer Feedforward

- One input layer directly connected to one output layer.
- No hidden layers, no cycles.
- Simple computation:
  $y_k = \phi\left( \sum w_{kj} x_j \right)$

#### ğŸ“˜ Multi-Layer Feedforward (MLP)

- Input â†’ Hidden â†’ Output layers.
- Each layer receives signals from previous.
- Fully connected (dense): all nodes connect to all in next layer.

#### ğŸ“˜ CNN (Convolutional Neural Network)

- Uses **convolutional filters** for spatial pattern detection.
- Includes:

  - **Convolution layers** (with ReLU)
  - **Pooling** (e.g., max pooling)
  - **Fully connected layers**
  - **Softmax output**

- Great for image classification and recognition

#### ğŸ“˜ RNN (Recurrent Neural Network)

- Has **feedback loops**, enabling memory of previous outputs.
- Uses **unit delay** elements $z^{-1}$
- Suitable for sequences, e.g., time series, language.

---

### 5. ğŸ“ **Mathematical Expressions by Topology**

| Network Type     | Expression                                                                                                                                     |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| **Single-Layer** | $y_k(\mathbf{x}) = \phi\left( \sum_{j=0}^{p} w_{kj} x_j \right)$                                                                               |
| **Multi-Layer**  | $y_k(\mathbf{x}) = \phi\left( \sum_{j=0}^{p_{\text{hidden}}} w_{kj} \cdot \phi\left( \sum_{i=0}^{p_{\text{input}}} w_{ji} x_i \right) \right)$ |
| **Recurrent**    | $y_k(n) = \phi\left( \sum_{i=0}^{p_{\text{input}}} w_{ki} x_i(n) + \sum_{j=0}^{p_{\text{output}}} w_{kj} y_j(n-1) \right)$                     |

---

### 6. ğŸ§  **Knowledge Representation**

- **NNs aim to model knowledge** through learning.

- Knowledge must come from **observations/samples**, which can be:

  - **Noisy / incomplete / redundant**
  - **Labelled** (with output $y$) or **unlabelled**

- **Labelled data**: Used in supervised learning
  e.g. $x =$ features, $y =$ diagnosis

---

### 7. ğŸ§ª **Training and Testing Workflow**

1. **Choose appropriate architecture**:

   - Based on task and input/output size

2. **Prepare Training Dataset**:

   - Subset of data used to adjust weights

3. **Testing Dataset**:

   - New/unseen data to evaluate generalization

4. **Assess model**:

   - Metrics: **accuracy**, **generalization**

---

### 8. ğŸ“Š **Applications**

| **Field**            | **x (Input)**      | **y (Output)**   |
| -------------------- | ------------------ | ---------------- |
| **Medical**          | Patient data       | Disease yes/no   |
| **OCR**              | Pixel strokes      | Letter (Aâ€“Z)     |
| **Maths/Regression** | $x$                | $f(x)$           |
| **Defence**          | Sensor data        | Threat category  |
| **Weather**          | Current conditions | Future forecast  |
| **Finance**          | Prices now         | Prices tomorrow  |
| **Vehicle Guidance** | Sensors            | Direction, speed |
| **Electronics**      | Specs              | Optimal circuit  |
| **Games**            | Character state    | Enemy reaction   |

---
