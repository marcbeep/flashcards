---

## üß† **Mathematical Optimisation Overview**

**Optimisation** is the process of finding the **best** design or solution for a system or model. It involves:

- Choosing the **right parameters**,
- Following **mathematical relationships**, goals, or constraints,
- And maximising or minimising an **objective function**.

Optimisation is useful in almost every scientific and engineering field.

---

### üìä **Types of Optimisation Problems**

#### ‚úÖ **Unconstrained Optimisation**

- Find the best value of a function with **no limits** on the parameters.
- Example: Maximise \( f(x, y) = x \cdot y \)
  - You can pick any values for \( x \) and \( y \).
  - Shown using 3D and contour plots.

#### ‚úÖ **Constrained Optimisation**

- Find the best value of a function **under specific conditions**.
- Example:
  - Maximise \( f(x, y) = x \cdot y \)
  - Subject to:
    - \( g(x, y) = 2(x + y) - 8.2 \leq 0 \)
    - \( h(x, y) = 0.5\sqrt{x^2 + y^2} - 1.8 = 0 \)
  - The best point must:
    - Be on the surface defined by \( h \),
    - Not go above the surface defined by \( g \),
    - And still maximise \( f \).

---

## ‚öôÔ∏è **Optimisation Methods**

### 1. **Enumerative Methods**

- Try **all possible values** of the parameters.
- For continuous parameters, create a **discretised version** (fixed intervals).
- ‚úÖ **Simple**, guarantees finding the best.
- ‚ùå **Very slow** and **impractical** for high-dimensional problems (many variables).
- Suffers from the **curse of dimensionality**: performance drops as variables increase.

---

### 2. **Gradient-based Methods**

- Use the **gradient** (slope) of the function to guide the search.
- Formula:
  \[
  \vec{x}(t+1) = \vec{x}(t) - \eta \nabla f[\vec{x}(t)]
  \]
  - \( \eta \) = step size (learning rate),
  - \( \nabla f \) = gradient (shows direction to move).
- ‚úÖ **Fast** for smooth, simple problems.
- ‚ùå Can get stuck in **local minima** and miss the **global minimum** if not started near it.
- Good at solving problems using local information.

---

### 3. **Stochastic Methods**

- Use **randomness** to explore different solutions.
- Don‚Äôt rely only on local information‚Äîcan explore wider space.
- ‚úÖ Better at finding the **global optimum**, avoids getting stuck.
- ‚ùå May be slower or require tuning.

#### Types:

- **Random walk**: Try completely random directions (simple but weak).
- **Genetic algorithms**: Mimic evolution‚Äîselect, combine, and mutate solutions over generations.
- Called **directed random search** when randomness is used in a smart, guided way.

---

## üîö Summary Comparison

| Method         | Strategy                     | Strengths                     | Weaknesses                        |
| -------------- | ---------------------------- | ----------------------------- | --------------------------------- |
| Enumerative    | Try everything               | Guaranteed best result        | Extremely slow for large problems |
| Gradient-based | Follow the slope             | Fast and efficient            | Can miss global best              |
| Stochastic     | Try guided random directions | Explores better, finds global | May need many tries               |

---
