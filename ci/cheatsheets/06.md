---
# üß† Understanding RBF Networks ‚Äì A Full Guide
---

## 1. üéØ **Pattern Separability**

- **Goal**: Separate data points into different classes.
- **Challenge**: Some data cannot be linearly separated in its original space.
- **Solution**: Use a mapping function $\varphi(\mathbf{x})$ to transform the data into a **higher-dimensional space** where linear separation is possible.

### ‚úÖ Cover‚Äôs Theorem

> Any complex (non-linearly separable) pattern set can become linearly separable when mapped to a high-enough dimension.

---

## 2. üí° **œÜ-Separability Example: XOR Problem**

### XOR input:

- (0,0), (1,1) ‚Üí Class 0
- (0,1), (1,0) ‚Üí Class 1

In 2D, this is **not linearly separable**.

### Trick:

Map each point using 2 radial functions:

- $\varphi_1(\mathbf{x}) = \exp(-\|\mathbf{x} - \mathbf{c}_1\|^2), \quad \mathbf{c}_1 = (1,1)$
- $\varphi_2(\mathbf{x}) = \exp(-\|\mathbf{x} - \mathbf{c}_2\|^2), \quad \mathbf{c}_2 = (0,0)$

In the new space $[\varphi_1(x), \varphi_2(x)]$, the classes are now linearly separable!

---

## 3. üìà **Interpolation with RBFs**

> Fit a smooth surface through given data points exactly.

Given:

- Points $\mathbf{x}_i$
- Target values $y_i$

### Define function:

$$
F(\mathbf{x}) = \sum_{i=1}^{N} w_i \, \varphi(\|\mathbf{x} - \mathbf{x}_i\|)
$$

Each $\varphi$ is an **RBF**, often a Gaussian or similar function.

### Matrix form:

$$
\Phi \mathbf{w} = \mathbf{y} \quad \Rightarrow \quad \mathbf{w} = \Phi^{-1} \mathbf{y}
$$

Where $\Phi$ is an N√óN matrix of RBF activations.

---

## 4. üîç **Types of RBFs**

### Common choices:

| Type                  | Formula                                                |
| --------------------- | ------------------------------------------------------ |
| Multiquadrics         | $\varphi(r) = \sqrt{r^2 + c^2}$                        |
| Inverse multiquadrics | $\varphi(r) = \frac{1}{\sqrt{r^2 + c^2}}$              |
| Gaussian              | $\varphi(r) = \exp\left(-\frac{r^2}{2\sigma^2}\right)$ |

- **Gaussian and inverse multiquadrics** are **localized** ‚Äî they respond strongly only near their centers.

---

## 5. üßÆ **RBF Neural Networks**

### Structure:

1. **Input layer**: Inputs go in.
2. **Hidden layer**: Applies RBFs (usually Gaussian) at various centres.
3. **Output layer**: Computes a weighted sum of hidden layer outputs.

### Function:

$$
F(\mathbf{x}) = \sum_{i=1}^{M} w_i \, \exp\left(-\frac{\|\mathbf{x} - \mathbf{c}_i\|^2}{2\sigma_i^2}\right)
$$

---

## 6. üõ†Ô∏è **Training an RBF Network**

### Objective:

Minimize total error:

$$
E(\mathbf{w}) = \sum \left[F(\mathbf{x}_i) - y_i\right]^2 + \lambda \|D\mathbf{F}\|^2
$$

- First term: fitting error.
- Second term: regularization (helps avoid overfitting).
- Solved using:

$$
\mathbf{w} = (\Phi^T\Phi + \lambda I)^{-1} \Phi^T \mathbf{y}
$$

üëâ Uses the **pseudoinverse**.

---

## 7. ‚öôÔ∏è **RBF Learning Strategies**

### Strategy A: **Fixed Random Centres**

- Randomly choose M data points as RBF centres.
- Set $\sigma = d_{\text{max}} / \sqrt{2M}$
- Solve weights using least squares.

### Strategy B: **Self-Organised Centres (Clustering)**

- Use K-means or similar to find clusters.
- Cluster centers become RBF centres.
- Spread of cluster = RBF width.
- Solve weights as before.

---

## 8. üéì **Supervised Centre Selection**

> Learn **all** parameters (weights, centres, widths) using gradient descent.

### Updates:

1. **Weights**:

   $$
   w_i(n+1) = w_i(n) - \eta_1 \cdot \frac{\partial E}{\partial w_i}
   $$

2. **Centres**:

   $$
   c_{ik}(n+1) = c_{ik}(n) - \eta_2 \cdot \frac{\partial E}{\partial c_{ik}}
   $$

3. **Widths**:

   $$
   \sigma_i(n+1) = \sigma_i(n) - \eta_3 \cdot \frac{\partial E}{\partial \sigma_i}
   $$

Each parameter is updated based on its gradient and a learning rate $\eta$.

---

## 9. üîÅ **RBFs vs MLPs (Multilayer Perceptrons)**

| Feature                 | RBF Network                            | MLP Network                               |
| ----------------------- | -------------------------------------- | ----------------------------------------- |
| Purpose                 | Nonlinear mapping                      | Nonlinear mapping                         |
| Approximation Type      | Localized (only responds near centers) | Global (neuron affects large input areas) |
| Structure               | Typically 1 hidden layer               | 1 or more hidden layers                   |
| Hidden Layer Activation | Based on distance to center            | Based on dot product with weights         |
| Hidden Nodes            | Nonlinear (RBFs)                       | Nonlinear                                 |
| Output Nodes            | Linear                                 | Nonlinear (usually)                       |
| Training Focus          | Centres & widths first, then weights   | Weights throughout all layers             |

---

## ‚úÖ Final Summary

- RBFs use **localized** nonlinear activations and are great for **interpretable** and **smooth** mappings.
- MLPs use **global** activation patterns and are better for more **complex or deep architectures**.
- RBF networks can **learn well with fewer layers** and offer good **control over smoothness** via widths and regularization.

---
