---

# üß† Neural Network Learning: Concepts, Mechanisms, and Paradigms

## üî∑ **1. General Concept of Learning in Neural Networks**

Learning in neural networks (NNs) involves **adapting internal parameters** (e.g., synaptic weights) in response to the environment in order to improve performance. The process typically involves:

- **Stimulation** from the environment.
- **Parameter updates** during training (training mode).
- **Adapted responses** in deployment (online mode).

### üîÄ Learning can be classified by:

- **Rules**: How weights are updated (e.g., error correction, Hebbian).
- **Paradigms**: How data is presented (e.g., supervised, unsupervised, reinforcement).

---

## üî∑ **2. Core Learning Rules**

### ‚úÖ **Error-Correction Learning**

- Compares the output $y_k(n)$ to the desired $d_k(n)$, producing an **error signal** $e_k(n) = d_k(n) - y_k(n)$.
- Updates weights to minimize error using the **Delta Rule (Widrow-Hoff)**:

  $$
  \Delta w_{kj}(n) = \eta e_k(n) x_j(n)
  $$

- Learning progresses by **minimizing a cost function** $E(n) = \frac{1}{2} e_k^2(n)$.

---

### ‚úÖ **Memory-Based Learning**

- Stores all training pairs $D_{\text{train}} = \{(x_i, d_i)\}$.
- For new inputs, retrieves closest past example (e.g., **1-Nearest Neighbour**):

  $$
  x^* = \arg\min_{x \in D_{\text{train}}} \|x - x_{\text{test}}\|_2
  $$

- Predicts using the stored output $d^*$.
- Extended to k-NN, weighted k-NN, and Parzen windows.

---

### ‚úÖ **Hebbian Learning**

- Based on biological neurons:

  > ‚ÄúNeurons that fire together, wire together.‚Äù

- Weight update:

  $$
  \Delta w_{kj}(n) = \eta y_k(n) x_j(n)
  $$

- Encourages strengthening of connections when neurons activate simultaneously.
- **Limitation**: Unbounded weight growth ‚Üí **synaptic saturation**.

#### üõ† **Covariance Hypothesis** (Improved Hebbian Rule)

- Incorporates mean activity:

  $$
  \Delta w_{kj}(n) = \eta (y_k - \bar{y})(x_j - \bar{x})
  $$

- Allows both **synaptic strengthening and weakening**.
- Prevents saturation; leads to more stable learning.

---

### ‚úÖ **Competitive Learning**

- Neurons **compete** to respond to inputs. Only **one wins**:

  $$
  y_k = \begin{cases} 1 & \text{if } v_k > v_j, \forall j \ne k \\ 0 & \text{otherwise} \end{cases}
  $$

- Weight update for winning neuron:

  $$
  \Delta w_{ki} = \eta(x_i - w_{ki})
  $$

- Encourages neurons to specialize in different input regions.
- Used in **clustering** and **Self-Organizing Maps (Kohonen networks)**.

---

### ‚úÖ **Boltzmann Learning**

- Inspired by **statistical mechanics**; neurons are binary ($+1/-1$).
- Defines an **energy function**:

  $$
  E = -\frac{1}{2} \sum_{k \ne j} w_{kj} x_k x_j
  $$

- Neurons stochastically flip states with probability:

  $$
  p(x_k \rightarrow -x_k) = \frac{1}{1 + \exp(-\Delta E_k / T)}
  $$

- Learning updates weights using **correlations**:

  $$
  \Delta w_{kj} = \eta (\rho^+_{kj} - \rho^-_{kj})
  $$

  - $\rho^+_{kj}$: Clamped state correlation.
  - $\rho^-_{kj}$: Free-running state correlation.

---

## üî∑ **3. Credit Assignment Problem**

Refers to identifying **which internal decisions** deserve **credit or blame** for an outcome.

### Two forms:

- **Temporal**: Assigning credit to the right **moment** of action.
- **Structural**: Assigning credit to the **right part** of the system (e.g., a neuron or layer).

### Relevance:

- Crucial in **multi-layer NNs**, where hidden units indirectly influence results.

---

## üî∑ **4. Learning Paradigms**

### üéì **Supervised Learning** (Learning with a Teacher)

- Training set: $D_{\text{train}} = \{(x_i, d_i)\}$
- Uses **labeled examples** to guide the network.
- Learns by **minimizing error** between actual and desired output.
- After training, operates without the teacher in **online mode**.

---

### üîç **Unsupervised Learning** (Learning without a Teacher)

- No target labels.
- Learns by **identifying structure** (e.g., clusters) in the input data.
- Uses internal measures (e.g., similarity).
- **Competitive Learning** is a classic example.

---

### üïπÔ∏è **Reinforcement Learning**

- Also without a teacher but guided by **rewards**.
- Agent learns through **trial and error**:

  - Observes state.
  - Takes action.
  - Receives reward.

- Objective: Learn a **policy** that **maximizes long-term reward**.
- Components:

  - **Policy**: Decision-making function.
  - **Learning algorithm**: Optimizes the policy.

---

## üß† Final Summary Table

| **Aspect**          | **Supervised**              | **Unsupervised**              | **Reinforcement**            |
| ------------------- | --------------------------- | ----------------------------- | ---------------------------- |
| **Teacher Present** | Yes                         | No                            | No                           |
| **Feedback Type**   | Error signal                | Data structure/statistics     | Reward signal                |
| **Goal**            | Learn input-output mapping  | Find hidden patterns/features | Maximize cumulative reward   |
| **Example Methods** | Backpropagation, Delta Rule | Competitive learning, k-NN    | Q-learning, Policy Gradients |

---
