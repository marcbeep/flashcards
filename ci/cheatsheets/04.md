---
## **Adaptive Filtering & Learning Algorithms**
---

### ðŸ”¹ **1. Problem Setup: System Identification**

- We are modeling a dynamic system using discrete-time input-output pairs:

  $$
  D = \left\{(\mathbf{x}(i), d(i))\right\}, \quad \mathbf{x}(i) \in \mathbb{R}^p, \quad d(i) \in \mathbb{R}
  $$

- Input vector $\mathbf{x}(i) = (x_1(i), ..., x_p(i))^T$ is applied across $p$ nodes to produce an output $d(i)$.

#### **Two perspectives on input $\mathbf{x}(i)$:**

- **Spatial:** Inputs from different physical sources.
- **Temporal:** Current and past values of a signal.

---

### ðŸ”¹ **2. Linear Neuron Model**

- Uses a weighted sum:

  $$
  y(i) = \mathbf{x}(i)^T \mathbf{w}(i)
  $$

- The **error** is:

  $$
  e(i) = d(i) - y(i)
  $$

- The task is to **adapt the weights $\mathbf{w}$** to minimize this error.

---

## **Linear Least Squares (LLS)**

---

### ðŸ”¹ **3. Batch Learning with LLS**

- Minimize total squared error over all $n$ samples:

  $$
  E(\mathbf{w}) = \frac{1}{2} \sum_{i=1}^{n} e(i)^2 = \frac{1}{2} \| \mathbf{d}(n) - \mathbf{X}(n)\mathbf{w} \|^2
  $$

- Optimal weights:

  $$
  \mathbf{w}(n+1) = \arg\min_{\mathbf{w}} E(\mathbf{w})
  $$

#### **Gauss-Newton Method:**

- In the linear case, converges in one step:

  $$
  \mathbf{w}(n+1) = \left(\mathbf{X}(n)^T \mathbf{X}(n)\right)^{-1} \mathbf{X}(n)^T \mathbf{d}(n)
  $$

- Or compactly, using the **Moore-Penrose pseudo-inverse**:

  $$
  \boxed{\mathbf{w}(n+1) = \mathbf{X}(n)^+ \mathbf{d}(n)}
  $$

---

## **Least Mean Square (LMS)**

---

### ðŸ”¹ **4. Online Learning with LMS**

- Updates weights using **one sample at a time**:

  $$
  E(n) = \frac{1}{2} e(n)^2
  $$

- Gradient descent update:

  $$
  \mathbf{w}(n+1) = \mathbf{w}(n) + \eta e(n) \mathbf{x}(n)
  $$

- $\eta$: learning rate â€” balances speed and stability.

  - **Small** $\eta$ â†’ smooth adaptation, more memory.
  - **Large** $\eta$ â†’ faster adaptation, less stability.

---

## **Perceptrons**

---

### ðŸ”¹ **5. Perceptron Model**

- Binary classifier based on:

  $$
  v_j = \mathbf{w}_j^T \mathbf{x} - \theta, \quad y_j = \phi(v_j) \in \{-1, +1\}
  $$

- $\phi(v) = \text{signum}(v)$: threshold-based **nonlinear activation**.

#### **Goal:**

- Classify data into two classes $C_1$ and $C_2$ by learning a **linear decision boundary** (hyperplane).

---

### ðŸ”¹ **6. Linear Separability**

- Training dataset $D_{\text{train}}$ is split into:

  - $D_1$ (class $C_1$): $\mathbf{w}^T \mathbf{x} > 0$
  - $D_2$ (class $C_2$): $\mathbf{w}^T \mathbf{x} \leq 0$

- If such a $\mathbf{w}$ exists, the data is **linearly separable**.

---

### ðŸ”¹ **7. Perceptron Training Algorithm**

1. **Initialize**: $\mathbf{w}(1) = 0$, learning rate $\eta \in (0,1]$
2. **Input**: Present $\mathbf{x}(t)$
3. **Response**:

   $$
   y(t) = \text{signum}(\mathbf{w}(t)^T \mathbf{x}(t))
   $$

4. **Update**:

   $$
   \mathbf{w}(t+1) = \mathbf{w}(t) + \eta \left[d(t) - y(t)\right] \mathbf{x}(t)
   $$

5. **Repeat** until all samples are correctly classified.

---

### ðŸ”¹ **8. Perceptron Convergence Theorem**

- **Guarantees** convergence in finite steps **if the data is linearly separable**.
- Each misclassification increases the alignment of $\mathbf{w}$ with the correct classification:

  $$
  \mathbf{w}(t+1)^T \mathbf{x}(t) > \mathbf{w}(t)^T \mathbf{x}(t)
  $$

---

## âœ… **Conclusion**

- **LLS**: Optimal, batch-based learning â€” closed-form solution.
- **LMS**: Efficient, online learning â€” sample-by-sample gradient descent.
- **Perceptron**: Nonlinear activation, binary classification â€” uses error correction for weight update.
