---
# âœ… Support Vector Machines (SVMs) â€“ A Complete Summary
---

## 1. **Goal of SVMs**

SVMs are supervised learning models used for **binary classification**.  
They work by finding the **optimal hyperplane** that separates data points of two different classes with the **maximum margin**.

- Dataset:  
  \( D = \{ (\mathbf{x}\_i, d_i) \} \), where:

  - \( \mathbf{x}\_i \in \mathbb{R}^d \) is a feature vector,
  - \( d_i \in \{+1, -1\} \) is the class label.

- If the data is **linearly separable**, we aim to find:
  \[
  g(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + b = 0
  \]
  which separates the two classes.

---

## 2. **Margin and Separation**

- The **margin** \( \rho \) is the distance between the decision boundary and the closest data points from each class.
- Our goal is to **maximize** this margin to improve generalization.

**Constraints:**
\[
\begin{cases}
\mathbf{w}^T \mathbf{x}\_i + b \geq +1 & \text{if } d_i = +1 \\
\mathbf{w}^T \mathbf{x}\_i + b \leq -1 & \text{if } d_i = -1
\end{cases}
\Rightarrow d_i(\mathbf{w}^T \mathbf{x}\_i + b) \geq 1
\]

---

## 3. **Support Vectors**

- Points that **lie on the margin boundaries** are called **support vectors**.
- These are critical for determining the optimal hyperplane.
- The margin is:
  \[
  \rho = \frac{1}{\|\mathbf{w}\|\_2}
  \Rightarrow \text{maximize } \rho \equiv \text{minimize } \frac{1}{2} \|\mathbf{w}\|^2
  \]

---

## 4. **SVM Optimization (Primal and Dual)**

### Primal Problem:

\[
\min\_{\mathbf{w}, b} \frac{1}{2} \|\mathbf{w}\|^2 \quad \text{s.t.} \quad d_i(\mathbf{w}^T \mathbf{x}\_i + b) \geq 1
\]

### Dual Formulation (using Lagrange multipliers \( \lambda_i \)):

\[
\max*{\lambda} \sum*{i=1}^N \lambda*i - \frac{1}{2} \sum*{i,j} \lambda_i \lambda_j d_i d_j \mathbf{x}\_i^T \mathbf{x}\_j
\]
\[
\text{s.t.} \quad \sum \lambda_i d_i = 0,\quad \lambda_i \geq 0
\]

- Once solved, we recover \( \mathbf{w} = \sum \lambda_i d_i \mathbf{x}\_i \)

---

## 5. **Soft-Margin SVM for Non-Separable Data**

Real-world data often can't be separated perfectly. To handle this, we allow for some errors using **slack variables** \( \xi_i \geq 0 \).

### Modified Primal Problem:

\[
\min\_{\mathbf{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum \xi_i
\quad \text{s.t.} \quad d_i(\mathbf{w}^T \mathbf{x}\_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\]

- \( C \) is a **regularization parameter**:
  - Large \( C \): penalizes errors heavily (fit the training data).
  - Small \( C \): allows more flexibility (generalizes better).

---

## 6. **Kernels for Nonlinear Boundaries**

In many problems, even soft-margin SVMs canâ€™t separate the classes linearly.  
We use **Kernels** to map data into a **higher-dimensional feature space** where separation **is** possible.

\[
K(\mathbf{x}\_i, \mathbf{x}\_j) = \varphi(\mathbf{x}\_i)^T \varphi(\mathbf{x}\_j)
\]

### Updated Dual Problem:

\[
\max \sum*i \lambda_i - \frac{1}{2} \sum*{i,j} \lambda_i \lambda_j d_i d_j K(\mathbf{x}\_i, \mathbf{x}\_j)
\]

> We **never compute** \( \varphi(\mathbf{x}) \) explicitly. We only define the **kernel function** \( K \).

---

### Common Kernels:

| Kernel Type    | Expression \( K(\mathbf{x}, \mathbf{y}) \)                                | Comments                       |
| -------------- | ------------------------------------------------------------------------- | ------------------------------ |
| Polynomial     | \( (\mathbf{x}^T \mathbf{y} + 1)^p \)                                     | \( p \) = degree               |
| RBF (Gaussian) | \( \exp\left( -\frac{\|\mathbf{x} - \mathbf{y}\|^2}{2\sigma^2} \right) \) | \( \sigma \) = spread width    |
| Tanh           | \( \tanh(\alpha \mathbf{x}^T \mathbf{y} + \beta) \)                       | \( \alpha, \beta \): constants |

---

## ðŸŽ¯ Final Takeaways

- SVMs aim to find the **widest possible margin** between classes.
- They work great for both **linearly and non-linearly separable data**.
- Soft margins allow for **real-world errors**.
- Kernels allow for **nonlinear boundaries** without complex math.
- Support vectors are the **only points** that determine the final classifier.

---
