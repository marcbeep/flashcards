[
  {
    "q": "What is Accuracy in binary classification and how is it calculated?",
    "a": "Accuracy measures the proportion of all predictions that were correct:\n\n\\[ \\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} \\]\n\nWhere:\n- TP = True Positives (correctly predicted positive cases)\n- TN = True Negatives (correctly predicted negative cases)\n- FP = False Positives (incorrectly predicted positive cases)\n- FN = False Negatives (incorrectly predicted negative cases)"
  },
  {
    "q": "Define Precision and Recall in binary classification. What do they measure?",
    "a": "Precision and Recall are two fundamental metrics:\n\nPrecision:\n\\[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\]\nMeasures: Out of all cases predicted as positive, what proportion were actually positive.\n\nRecall:\n\\[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\]\nMeasures: Out of all actual positive cases, what proportion did we correctly identify."
  },
  {
    "q": "What is the F-score and how is it calculated?",
    "a": "The F-score is a balanced measure that combines precision and recall:\n\n\\[ \\text{F-score} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n\nIt gives equal weight to both precision and recall, providing a single score that balances both metrics."
  },
  {
    "q": "What is the activation score in a Perceptron and how is it calculated?",
    "a": "The activation score is calculated as:\n\n\\[ a = W^T X + b \\]\n\nWhere:\n- W = Weight vector\n- X = Input feature vector\n- b = Bias term\n- a = Activation score\n\nIn plain English: Computes a weighted sum of inputs plus a bias term to determine the neuron's activation."
  },
  {
    "q": "How does a Perceptron make predictions and update its weights?",
    "a": "Prediction:\n\\[ y = \\text{sign}(a) \\]\n\nUpdate Rule (on misclassification):\n\\[ W = W + yX \\]\n\\[ b = b + y \\]\n\nWhere:\n- W = Weight vector\n- X = Input feature vector\n- y = True class label\n- b = Bias term\n\nIn plain English: When a mistake is made, adjust weights and bias in the direction of the correct class."
  },
  {
    "q": "What is the sigmoid function in Logistic Regression and how is it used for probability prediction?",
    "a": "Sigmoid Function:\n\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\]\n\nProbability Prediction:\n\\[ P(y = +1 | X) = \\sigma(W^T X + b) \\]\n\\[ P(y = -1 | X) = \\sigma(-(W^T X + b)) \\]\n\nWhere:\n- W = Weight vector\n- X = Input feature vector\n- b = Bias term\n- σ = Sigmoid function\n\nIn plain English: Transforms any real number into a value between 0 and 1, useful for probability predictions."
  },
  {
    "q": "What is the loss function in Logistic Regression and how is it calculated?",
    "a": "The loss function (Negative Log-Likelihood) is:\n\n\\[ L = -\\sum \\log(\\sigma(y_i \\cdot (W^T X_i + b))) \\]\n\nWhere:\n- y_i = True class label for instance i\n- X_i = Feature vector for instance i\n- W = Weight vector\n- b = Bias term\n- σ = Sigmoid function\n\nIn plain English: Measures how well our predictions match the true labels, with better predictions giving lower loss."
  },
  {
    "q": "What is the objective function in k-Means clustering and how are centroids updated?",
    "a": "Objective Function (Within-Cluster Sum of Squares):\n\\[ \\text{WCSS} = \\sum\\sum \\|X - Y_j\\|^2 \\]\n\nCentroid Update:\n\\[ Y_j = \\frac{1}{|C_j|} \\sum_{X \\in C_j} X \\]\n\nWhere:\n- X = Data point\n- Y_j = Centroid of cluster j\n- C_j = Set of points in cluster j\n- |C_j| = Number of points in cluster j\n\nIn plain English: Minimizes total squared distance between points and their cluster centers, updating centers to be the mean of their points."
  },
  {
    "q": "How does k-Medians clustering differ from k-Means in terms of objective function and updates?",
    "a": "k-Medians uses L1 distance and median updates:\n\nObjective Function:\n\\[ \\sum\\sum \\|X - Y_j\\|_1 \\]\n\nMedian Update:\n\\[ Y_j^{(i)} = \\text{median}(X_1^{(i)}, X_2^{(i)}, ..., X_s^{(i)}) \\]\n\nWhere:\n- Y_j^(i) = i-th coordinate of cluster j's median\n- X_k^(i) = i-th coordinate of k-th point in cluster j\n\nIn plain English: Uses Manhattan distance instead of Euclidean, and updates cluster representatives to be coordinate-wise medians."
  },
  {
    "q": "What are Support and Confidence in Association Rule Mining?",
    "a": "Support of Itemset I:\n\\[ \\text{sup}(I) = \\frac{|\\{T \\in D : I \\subseteq T\\}|}{|D|} \\]\n\nConfidence of Rule X → Y:\n\\[ \\text{conf}(X \\rightarrow Y) = \\frac{\\text{sup}(X \\cup Y)}{\\text{sup}(X)} \\]\n\nWhere:\n- I = Itemset\n- D = Dataset (set of transactions)\n- T = Transaction\n- X = Antecedent itemset\n- Y = Consequent itemset\n\nIn plain English: Support measures how frequent an itemset is, while confidence measures how often Y appears when X is present."
  },
  {
    "q": "What are the three main centrality measures in graph analysis and how are they calculated?",
    "a": "1. Degree Centrality:\n\\[ C_D(i) = \\frac{\\text{deg}(i)}{n - 1} \\]\n\n2. Closeness Centrality:\n\\[ C_C(i) = \\frac{1}{\\text{AvDist}(i)} = \\frac{n - 1}{\\sum \\text{dist}(i,j)} \\]\n\n3. Betweenness Centrality:\n\\[ C_B(i) = \\sum \\frac{q_{jk}(i)}{q_{jk}} / \\frac{n(n-1)}{2} \\]\n\nWhere:\n- deg(i) = Number of edges connected to node i\n- n = Total number of nodes\n- dist(i,j) = Shortest path distance between nodes i and j\n- q_jk = Total number of shortest paths between j and k\n- q_jk(i) = Number of those paths passing through i"
  },
  {
    "q": "What is PageRank and how is it calculated?",
    "a": "Basic PageRank:\n\\[ P(a) = \\sum_{(x,a) \\in E} \\frac{P(x)}{O_x} \\]\n\nMatrix Form:\n\\[ P = A^T P \\]\n\nPower Iteration:\n\\[ P^{(i)} = A P^{(i-1)} \\]\n\nWhere:\n- P(a) = PageRank of page a\n- O_x = Number of outgoing links from page x\n- E = Set of edges (links) in the graph\n- A = Adjacency matrix (normalized by outgoing links)\n\nIn plain English: A page's importance is the sum of the importance of pages linking to it, weighted by how many links those pages have."
  },
  {
    "q": "What are the main distance metrics for vectors and how are they calculated?",
    "a": "1. Euclidean (L2):\n\\[ d(X,Y) = \\sqrt{\\sum(x_i - y_i)^2} \\]\n\n2. Manhattan (L1):\n\\[ d(X,Y) = \\sum|x_i - y_i| \\]\n\n3. Cosine Similarity:\n\\[ \\cos(X,Y) = \\frac{X \\cdot Y}{\\|X\\| \\|Y\\|} \\]\n\nWhere:\n- X, Y = Input vectors\n- x_i, y_i = i-th components of vectors X and Y\n- X·Y = Dot product of vectors X and Y\n- ||X|| = Length (magnitude) of vector X"
  },
  {
    "q": "What are Jaccard Similarity and Overlap Coefficient for sets?",
    "a": "Jaccard Similarity:\n\\[ J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} \\]\n\nOverlap Coefficient:\n\\[ \\text{Overlap}(A,B) = \\frac{|A \\cap B|}{\\min(|A|, |B|)} \\]\n\nWhere:\n- A, B = Sets\n- |A ∩ B| = Size of intersection\n- |A ∪ B| = Size of union\n- min(|A|, |B|) = Size of smaller set\n\nIn plain English: Jaccard measures intersection over union, while Overlap measures intersection over the size of the smaller set."
  }
]
