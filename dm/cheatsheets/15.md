---
### 📌 **Goal of Lecture**

Improve k-means clustering results by **carefully choosing initial centroids** (starting cluster centers).
---

### 🟡 **Method 1: Random Initialization**

- Choose **k** points from the data at random to start.
- Run k-means.
- May get **lucky** (good clustering), or **unlucky** (poor results).
- Example shown: 4 iterations each of lucky and unlucky choices.

💡 **Problem**: Random chance can lead to bad initial centroids → poor clustering.

---

### 🟡 **Method 2: Random Initialization + Repetition**

Steps:

1. Randomly choose initial centroids.
2. Run k-means.
3. Repeat multiple times.
4. Pick the result with the **best performance** (e.g. lowest error).

📝 Pros:

- Improves over pure random selection.

👎 Cons:

- Still not guaranteed to be optimal.
- Can be computationally expensive.

---

### 🟡 **Method 3: Sampling + Hierarchical Clustering**

Steps:

1. Take a **small sample** of the dataset.
2. Run **hierarchical clustering** on this sample.
3. Use **means of resulting clusters** as initial centroids.
4. Run standard k-means on the full dataset.

✅ Good if:

- Sample is **small** (hundreds or a few thousand points).
- Number of clusters **k** is small.

👎 Hierarchical clustering is slow on large data, so sample must be small.

---

### 🟡 **Method 4: Furthest-Point Heuristic**

Steps:

1. Pick one point randomly from dataset.
2. Pick next point that is **furthest away** from any chosen point.
3. Repeat until k points are selected.

📌 Let:

- $D$: dataset
- $R(X)$: distance from point $X$ to nearest chosen center

✅ Encourages spread-out centroids.

👎 Risk: May choose **outliers**, which hurt clustering.

---

### 🟡 **Method 5: k-means++ (Smart Seeding)**

**Key Idea**: Choose next centroid with **probability proportional to the square of its distance** from existing centroids.

Steps:

1. Choose one point at random.
2. For each point $X$, compute $R(X)^2$ = distance to closest chosen centroid squared.
3. Choose next centroid with probability:

   $$
   P(X) = \frac{R(X)^2}{\sum R(X)^2}
   $$

4. Repeat until k points are chosen.
5. Run k-means.

✅ **Avoids poor initializations.**
✅ **Theoretically proven** to give better results on average.

---

### 🧪 **Experimental Results: k-means++ vs. k-means**

**Synthetic Data:**

- Datasets called **Norm-10** and **Norm-25** were created with well-separated clusters.
- k-means often merges clusters wrongly due to bad seeds.
- k-means++ consistently finds better results (closer to real centers).

**Real Data:**

- **Cloud dataset**: k-means++ was \~2x faster and gave \~20% better clustering.
- **Intrusion dataset**: k-means++ gave **10x–1000x better clustering** and was up to **70% faster**.

✅ k-means++ offers **speed and accuracy** benefits in both synthetic and real scenarios.

---

### ✅ Summary of Methods

| Method                      | Pros                                        | Cons                         |
| --------------------------- | ------------------------------------------- | ---------------------------- |
| Random                      | Simple                                      | Often poor results           |
| Random + Repeat             | Better than single random                   | Still based on luck, slower  |
| Sampling + Hierarchical     | Effective with small data/sample            | Expensive for large datasets |
| Furthest-Point              | Ensures spread out centroids                | May pick outliers            |
| **k-means++ (recommended)** | Best mix of accuracy, speed, and simplicity | More complex to implement    |

---
