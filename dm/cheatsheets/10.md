---

### 📘 Bayes’ Rule

Bayes’ Rule helps us calculate the probability of a hypothesis $H$ given evidence $E$:

$$
P(H | E) = \frac{P(E | H) \cdot P(H)}{P(E)}
$$

Where:

- $P(H | E)$: Posterior — probability of the hypothesis after seeing the evidence.
- $P(E | H)$: Likelihood — how likely the evidence is, assuming the hypothesis is true.
- $P(H)$: Prior — the original probability of the hypothesis.
- $P(E)$: Marginal — total probability of the evidence.

This is helpful when direct estimation of $P(H | E)$ is hard but the other values are easier to calculate.

---

### 📊 Example: Medical Diagnosis

Suppose:

- Meningitis causes a stiff neck 50% of the time.
- Probability of meningitis: 1 in 50,000 = 0.00002
- Probability of stiff neck: 1 in 20 = 0.05

We use Bayes’ Rule to find the chance of having meningitis if a patient has a stiff neck:

$$
P(H | E) = \frac{0.5 \cdot 0.00002}{0.05} = 0.0002
$$

So there's a 0.02% chance the patient has meningitis.

---

### 🧠 High Dimensions and Naive Bayes

When data has multiple features (like symptoms, measurements, etc.), directly calculating $P(H | X)$ for combinations becomes hard. For example, in a car diagnosis:

- $H$ = engine doesn't start
- $A$ = weak battery
- $B$ = no gas

Directly estimating $P(H | A, B)$ could be inaccurate if data is rare.

Instead, Bayes' Rule helps:

$$
P(H | A, B) = \frac{P(A, B | H) \cdot P(H)}{P(A, B)}
$$

---

### 🤖 Naive Bayes Approximation

Assume features are conditionally independent (this is the "naive" part). That means we can say:

$$
P(x_1, x_2, ..., x_d | C) = \prod_{i=1}^d P(x_i | C)
$$

So:

$$
P(C | x_1, x_2, ..., x_d) \propto P(C) \cdot \prod_{i=1}^d P(x_i | C)
$$

Where:

- $P(C)$: proportion of data in class $C$
- $P(x_i | C)$: proportion of class $C$ examples with feature $x_i$

This simplification makes it easy to compute.

---

### 💡 Independence Refresher

If two events A and B are independent:

$$
P(A \text{ and } B) = P(A) \cdot P(B)
$$

If more than 2 events are mutually independent:

$$
P(A_1, A_2, ..., A_n) = P(A_1) \cdot P(A_2) \cdot ... \cdot P(A_n)
$$

Naive Bayes assumes this for features given the class.

---

### 📌 Putting It Together (2-Feature Example)

Using our car diagnosis again:

$$
P(H | A, B) = \frac{P(A | H) \cdot P(B | H) \cdot P(H)}{P(A, B)}
$$

Each conditional probability like $P(A | H)$ is estimated from the data.

---

### 🔍 Classification: Proportional Form

We want the class $C$ that maximizes:

$$
P(C) \cdot \prod_{i=1}^d P(x_i | C)
$$

We don’t need the denominator $P(X)$, since it's the same for all classes.

---

### 🏏 Example: Should We Play?

Given test instance:

- Outlook: Sunny
- Temperature: Cool
- Humidity: High
- Windy: True

We compare:

- **Play = yes**:

$$
2/9 \cdot 3/9 \cdot 3/9 \cdot 3/9 \cdot 9/14 \approx 0.0053
$$

- **Play = no**:

$$
3/5 \cdot 1/5 \cdot 4/5 \cdot 3/5 \cdot 5/14 \approx 0.0206
$$

Since 0.0206 > 0.0053 → predict **Play = no**

---

### ✅ Summary

- **Bayes’ Rule** helps update probabilities with new evidence.
- **Naive Bayes** simplifies calculations by assuming feature independence.
- It's useful in high-dimensional data where direct probability estimates are hard.
- **Classification** is done by choosing the class with the highest posterior probability.
