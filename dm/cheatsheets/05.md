---

## **1. Loss Function Minimization in Perceptron**

### **Training Dataset and Parameters**

- Dataset:  
  𝒟 = {(X₁, y₁), ..., (Xₙ, yₙ)} where each Xₖ is a vector of features and yₖ is the label.
- Model Parameters:  
  W = (w₀, w₁, ..., w_d) including bias term.

### **Objective**

- Minimize the **loss function** L(W, 𝒟) by adjusting weights W using optimization techniques.

---

## **2. Perceptron Training Algorithm**

### **Initialization**

- Set all weights to zero:  
  wi = 0 for i = 1 to d  
  b = 0

### **Training Loop**

- Repeat for MaxIter iterations:
  - For each training sample (X, y):
    - Compute activation: a = WᵀX + b
    - If misclassified (y · a ≤ 0):
      - Update weights: wi ← wi + y · xi for all i
      - Update bias: b ← b + y

---

## **3. Loss Functions**

### **A. Step Loss Function (Misclassification Count)**

- For a single point:  
  L(b, W, Xₖ, yₖ) = 1 if misclassified, 0 otherwise
- For dataset:  
  L(b, W, 𝒟) = Total number of misclassifications

#### **Drawbacks**

- Piecewise constant → Not differentiable
- Gradient = 0 → Gradient descent not usable

---

### **B. Hinge-like Loss Function: h(t) = max(0, t)**

- Activation score:  
  aₖ = b + Σᵢ wᵢ xₖ⁽ⁱ⁾
- Individual loss:  
  L(b, W, Xₖ, yₖ) = h(−yₖ · aₖ)
- Dataset loss:  
  L(b, W, 𝒟) = Σₖ h(−yₖ · aₖ)

#### **Properties**

- Loss increases with misclassification severity
- Differentiable (almost everywhere)

---

## **4. Gradient Descent**

### **Gradient Computation**

For each (Xₖ, yₖ):

- ∂L/∂b = −yₖ if misclassified, else 0
- ∂L/∂wᵢ = −yₖ · xₖ⁽ⁱ⁾ if misclassified, else 0

Gradient Vector:

- ∇ = −yₖ · (1, xₖ⁽¹⁾, ..., xₖ⁽ᵈ⁾)ᵀ for misclassified samples
- Else, ∇ = 0 vector

### **Update Rule**

- Full batch gradient descent:  
  Update weights using all misclassified points
- Online gradient descent:  
  Update immediately after each misclassification:
  - (b, w₁, ..., w_d) ← (b, w₁, ..., w_d) + μ · y · (1, x₁, ..., x_d)ᵀ
  - Usually μ = 1

---

## **5. Final Training Algorithm (Simplified)**

1. Initialize weights and bias to 0
2. For each iteration:
   - For each sample (X, y):
     - Compute a = WᵀX + b
     - If y · a ≤ 0:
       - wi ← wi + y · xi
       - b ← b + y

---
