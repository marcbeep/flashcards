## **1. Linear Algebra**

This section introduces basic linear algebra concepts used to represent and manipulate data.

### Key Concepts:

- **Vectors**: Represent data points as ordered lists of numbers. Denoted by bold or bar-marked uppercase letters (e.g., **X**, **Y**). Vectors are typically column vectors.
- **Matrices**: Collections of vectors organized by rows or columns (e.g., **M ∈ ℝⁿˣᵐ**). Used to represent datasets.
- **Vector Arithmetic**:
  - Addition, dot (inner) product, and outer product.
- **Matrix Arithmetic**:
  - Addition, multiplication (requires matching dimensions).
- **Transpose and Inverse**:
  - Transpose: flips rows and columns.
  - Inverse: only for full-rank square matrices; used to "undo" matrix multiplication.
- **Linear Independence**:
  - Vectors are linearly dependent if one is a combination of others.
- **Rank**:
  - The number of linearly independent rows or columns. Determines invertibility.
- **Trace**:
  - The sum of a matrix’s diagonal elements.
- **Eigenvalues & Eigenvectors**:
  - Eigenvectors remain in the same direction under transformation by the matrix; eigenvalues scale them.

---

## **2. Differential Calculus**

Essential for optimization and understanding function changes.

### Key Concepts:

- **Derivatives of Basic Functions**:
  - Includes polynomials, exponentials, logarithmic, sine, cosine.
- **Differentiation Rules**:
  - Sum, product, quotient, and chain rule.
- **Partial Derivatives**:
  - Used for functions with multiple variables; derivative with respect to one variable while keeping others constant.
- **Gradient**:
  - Vector of all partial derivatives; indicates the direction of steepest ascent.

---

## **3. Optimisation**

Used in model training and parameter tuning.

### Subtopics:

- **Unconstrained Optimisation (Gradient Descent)**:
  - Iterative method for finding a local minimum by moving in the direction of the negative gradient.
  - Update rule:  
    \[
    X\_{i+1} = X_i - \gamma_i \cdot \nabla f(X_i)
    \]
- **Constrained Optimisation (Lagrange Multipliers)**:
  - Handles optimisation with constraints.
  - Uses a Lagrangian function:  
    \[
    \mathcal{L}(X, \lambda) = f(X) - \lambda \cdot g(X)
    \]
  - Solves by finding points where all partial derivatives equal zero.

---

## **4. Probability**

Introduces key probability distributions for modeling randomness.

### Key Distributions:

- **Bernoulli**: Binary outcomes (e.g., coin toss).
- **Generalised Bernoulli**: Multiple discrete outcomes (e.g., dice).
- **Binomial**: Multiple trials of binary outcomes (e.g., number of heads in coin flips).
- **Multinomial**: Multiple trials of multi-outcome experiments (e.g., dice rolls).
