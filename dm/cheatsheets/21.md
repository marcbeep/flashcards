---

## **Logistic Regression Lecture Summary**

### **1. Classifier Types**

- **Ordinary classifier:** Predicts a fixed class label (e.g., cat/dog).
- **Probabilistic classifier:** Predicts a probability distribution over all class labels. For input $X$, it gives:

  $$
  p_i = P(c_i \mid X), \quad \sum p_i = 1
  $$

### **2. Discriminative vs. Generative Models**

- **Discriminative models** learn $P(C \mid X)$: directly model the probability of a class given input.
  Examples: Logistic Regression, Neural Networks.
- **Generative models** learn $P(X, C)$: model how data is generated.
  Examples: Naive Bayes.

Bayes' Rule connects both:

$$
P(H \mid E) = \frac{P(E \mid H) \cdot P(H)}{P(E)}
$$

---

### **3. Problem Setup**

- **Binary classification problem**: Class labels are $y \in \{-1, +1\}$.
- Goal: Build a **probabilistic model** that predicts the probability $P(y = +1 \mid X)$.

---

### **4. Main Idea of Logistic Regression**

- Define a **hyperplane**:

  $$
  b + W^T X = 0
  $$

  where:

  - $W = (w_1, w_2, ..., w_d)$: feature weights
  - $b$: bias
  - $X = (x_1, ..., x_d)$: input features

- In a **Perceptron**, classification depends on the **sign** of $b + W^T X$.

- In **Logistic Regression**, this value (called the "score") also indicates **confidence** in the prediction â€” the larger its absolute value, the further the point is from the decision boundary.

---

### **5. Mapping Scores to Probabilities**

- Use the **logistic sigmoid function**:

  $$
  \sigma(x) = \frac{1}{1 + e^{-x}}, \quad \text{outputs in } (0,1)
  $$

#### Properties:

- $\sigma(-x) = 1 - \sigma(x)$
- Derivative:

  $$
  \frac{d\sigma}{dx} = \sigma(x)(1 - \sigma(x))
  $$

---

### **6. Logistic Regression Model**

- For input $X$, with score $a = b + W^T X$:

  $$
  P(y = +1 \mid X) = \sigma(a) = \frac{1}{1 + e^{-a}}, \quad
  P(y = -1 \mid X) = \sigma(-a) = \frac{1}{1 + e^a}
  $$

- Compactly:

  $$
  P(y = t \mid X) = \sigma(t \cdot a), \quad t \in \{-1, +1\}
  $$

---

### **7. Training via Maximum Likelihood**

- **Training data**: $\mathcal{D} = \{(X_i, y_i)\}_{i=1}^n$

- **Likelihood function**:

  $$
  \mathcal{L}(b, W) = \prod_{i=1}^n \sigma(y_i (b + W^T X_i))
  $$

- **Negative log-likelihood (loss function)**:

  $$
  -\ell = - \sum_{i=1}^n \log \sigma(y_i (b + W^T X_i))
  $$

---

### **8. Gradient Computation**

Let $a_i = b + W^T X_i$. The gradients are:

- With respect to bias $b$:

  $$
  \frac{\partial \ell}{\partial b} = \sum_{i=1}^n y_i \cdot \sigma(-y_i \cdot a_i)
  $$

- With respect to each weight $w_k$:

  $$
  \frac{\partial \ell}{\partial w_k} = \sum_{i=1}^n y_i \cdot \sigma(-y_i \cdot a_i) \cdot x_k^{(i)}
  $$

These help update the weights and bias using gradient descent.

---

### **9. Gradient Descent Update Rule**

- Basic formula:

  $$
  Z_{i+1} = Z_i - \gamma_i \cdot \nabla_Z f(Z_i)
  $$

- Logistic Regression updates:

  $$
  W \leftarrow W + \mu \sum_{i=1}^n y_i \cdot \sigma(-y_i a_i) X_i
  $$

  $$
  b \leftarrow b + \mu \sum_{i=1}^n y_i \cdot \sigma(-y_i a_i)
  $$

---

### **10. Online vs Batch Learning**

- **Batch**: Use the entire dataset for each weight update.

  - More accurate but slower.
  - Common optimizer: **L-BFGS**.

- **Online**: Use one data point at a time.

  - Faster but less stable.
  - Uses **Stochastic Gradient Descent (SGD)**.

---

### **11. Prediction (After Training)**

Given trained $W$, $b$, and new input $X$:

- Compute:

  $$
  a = b + W^T X
  $$

- If $a > 0$, predict $+1$; else $-1$
- Probability/confidence:

  $$
  P(y = +1 \mid X) = \sigma(a), \quad P(y = -1 \mid X) = 1 - \sigma(a)
  $$

---

### **12. Interpretation as a Neuron**

- Model behaves like a **neuron**:

  $$
  a = b + \sum_{i=1}^d w_i x_i
  $$

  Apply sigmoid to get probability. Output depends on sign of $a$.

---

### **13. L2 Regularisation**

- Penalises large weights to prevent overfitting.

- Objective becomes:

  $$
  J(\mathcal{D}, W) = L(\mathcal{D}, W) + \lambda \|W\|^2 = L + \lambda \sum w_i^2
  $$

  where $\lambda$ is a regularisation parameter.

- Gradient becomes:

  $$
  \nabla_W J = \nabla_W L + 2\lambda W
  $$

- Update rule with regularisation:

  $$
  W \leftarrow (1 - 2\mu \lambda)W + \mu y \cdot \sigma(-y \cdot a) X
  $$

---
