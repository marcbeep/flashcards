## 1. Introduction & Motivation for Data Mining

### 1.1 Why we need data mining

- **Data explosion** – organisations now collect data at petabyte scale; humans cannot inspect it manually.
- **Pervasive sources** – social media, YouTube, sensor networks, web logs, surveys; collection is cheaper than ever.
- Result: we require **automated analysis of massive data** (the very definition of data mining).

### 1.2 The data-mining pipeline

1. **Data collection** (application-specific; quality decisions here propagate downstream).
2. **Data preprocessing**
   - _Feature extraction_ – reshape raw data into algorithm-friendly forms (tables, time-series, etc.).
   - _Data cleaning_ – fix or impute missing/erroneous values.
   - _Feature selection & transformation_ – drop irrelevant variables; rescale or discretise others.
3. **Analytical processing** – apply algorithms (association mining, clustering, classification, outlier detection).
4. _Optional_ feedback loop, then **output to analysts**.

### 1.3 Feature concepts

- **Features** (a.k.a. attributes, variables) describe **objects** (rows, instances).
- Good features enable rules that generalise beyond the training set; crafting them is an “art” (feature engineering) – although modern deep learning often discovers features automatically.
- **Feature pruning** removes low-variance or irrelevant features (e.g., rare words in text, flat numeric columns).

---

## 2. Types of Data

### 2.1 High-level split

| Class                                 | Description                                                                                    |
| ------------------------------------- | ---------------------------------------------------------------------------------------------- |
| **Non-dependency-oriented (tabular)** | Objects are assumed independent; representable as an _n × d_ matrix of features.               |
| **Dependency-oriented**               | Objects are linked by **implicit** (order, space, time) or **explicit** (edges) relationships. |

### 2.2 Attribute-level data types inside tabular data

- **Numerical** – integer or real, with natural ordering.
- **Categorical** – unordered discrete values (e.g., colour).
- **Binary** – 0/1; treatable as numeric or two-category categorical and handy for set representation.
- **Text** – raw string (dependency-oriented) or vector-space (bag-of-words) representation.

### 2.3 Dependency-oriented data in detail

- **Implicit** dependencies
  - _Time-series_ (sensor readings over time).
  - _Discrete sequences / strings_ (categorical analogue of time-series).
  - _Spatial_ (each record tagged with coordinates).
  - _Spatiotemporal_ (both space & time).
- **Explicit** dependencies – **graphs/networks** with nodes, edges, and optional node/edge attributes (e.g., social networks, web graphs).

---

## 3. Data Representation Choices

### 3.1 Why representation matters

- It is the _first_ hands-on step after data collection.
- The structures you choose dictate which algorithms are valid and which patterns are even discoverable; there is no universally “best” representation.

### 3.2 Encoding categorical variables

- **One-hot / indicator vectors** – allocate one dimension per category; set to 1 if the record contains that category, else 0.
  - Example words: “awesome”, “burger”, “terrible”
    - (1, 1, 0) might represent a sentence containing “awesome” & “burger” but not “terrible”.

### 3.3 Multiple representations for the same text

Sentence: _“The burger I ate was an awesome burger!”_

1. **Word list (sequence)** – keeps order and duplicates.
2. **Word set** – unique tokens only.
3. **Bag-of-words vector** – frequency per word (TF/TF-IDF).
4. **Character-frequency vector** – stylistic signal useful for language ID, spam detection, etc.  
   Each encoding keeps some structure, discards other structure, and therefore fits different model families.

---

## 4. Four Fundamental Data-Mining Problem Types

| #     | Problem                         | Essence                                                                                                              | Typical outputs & use-cases                                                                            |
| ----- | ------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| **1** | **Association-pattern mining**  | Find features/items that co-occur above a support threshold. Special case: _frequent-pattern mining_ on binary data. | Market-basket rules (e.g., {Milk, Butter, Bread} appear together in ≥ 65 % of baskets).                |
| **2** | **Classification** (supervised) | Learn mapping from features → **class label** using labelled data; predict labels for unseen objects.                | Targeted marketing, spam/ham filtering, text recognition. Algorithms: Decision Tree, Naïve Bayes, etc. |
| **3** | **Clustering** (unsupervised)   | Partition data into _k_ clusters with high intra-cluster similarity & low inter-cluster similarity.                  | Customer segmentation, prototype-based data summarisation.                                             |
| **4** | **Outlier detection**           | Identify objects markedly different from the bulk—could be noise or critical exceptions.                             | Credit-card fraud, sensor fault detection, medical anomaly spotting, extreme earth-science events.     |
