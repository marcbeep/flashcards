---

### 1. **Purpose of Classifier Evaluation**

- We need to assess how good a classifier or model is.
- **Absolute Goodness**: Measured when the model is used in real-world settings—but we don’t know this before deployment.
- **Relative Goodness**: Measured using a test dataset (also called a gold standard) where we already know the correct labels.

---

### 2. **Gold Standard (Test Data)**

- Used only for evaluation, never training.
- Each item in the test set has a known correct label.
- Allows comparison between predicted and true labels using various metrics.

---

### 3. **Confusion Matrix**

A table layout for binary classification:

|                   | Actual YES (+)       | Actual NO (–)        |
| ----------------- | -------------------- | -------------------- |
| Predicted YES (+) | True Positives (TP)  | False Positives (FP) |
| Predicted NO (–)  | False Negatives (FN) | True Negatives (TN)  |

- Makes it easier to see if the model confuses two classes.

**Definitions**:

- **TP**: Correctly predicted positive.
- **TN**: Correctly predicted negative.
- **FP**: Incorrectly predicted positive.
- **FN**: Incorrectly predicted negative.

---

### 4. **Examples**

- **Car Detection**:
  - Positive: image has a car.
  - Negative: image does not.
- **Cancer Detection**:
  - FP: Healthy person wrongly predicted as having cancer.
  - FN: Person with cancer wrongly predicted as healthy.
  - Importance of FP vs FN depends on the application.

---

### 5. **Evaluation Measures**

- **Accuracy**: Proportion of total correct predictions.

  \[
  \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
  \]

- **Precision**: Among predicted positives, how many are actually positive.

  \[
  \text{Precision} = \frac{TP}{TP + FP}
  \]

- **Recall**: Among actual positives, how many did we correctly predict.

  \[
  \text{Recall} = \frac{TP}{TP + FN}
  \]

- **F-score**: Harmonic mean of precision and recall.

  \[
  \text{F-score} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
  \]

---

### 6. **Precision vs Recall (Trade-off)**

- **High Recall**: Important in applications like cancer detection.
- **High Precision**: Important in applications like product recommendation.
- Changing classification threshold affects this balance.

---

### 7. **Multi-class Evaluation**

- Metrics are calculated per class.
- **Precision for Class A**:

  \[
  \frac{\text{Correctly classified A}}{\text{Total predicted A}}
  \]

- **Recall for Class A**:

  \[
  \frac{\text{Correctly classified A}}{\text{Total actual A}}
  \]

- **F-score for Class A**:

  \[
  \frac{2 \cdot \text{Precision}\_A \cdot \text{Recall}\_A}{\text{Precision}\_A + \text{Recall}\_A}
  \]

- **Macro F-score**:
  - Average of F-scores across all classes.

---

### 1. **Types of Classification Algorithms**

- **Multiclass Classifiers**:

  - **k-Nearest Neighbors (k-NN)**
  - **Naive Bayes**

- **Binary Classifiers**:
  - **Perceptron**
  - **Logistic Regression**

---

### 2. **Converting Binary to Multiclass Classification**

To use a binary classifier for multiclass classification (with _k_ classes), we use **meta-algorithms**.

#### Two Main Strategies:

- **One-vs-One (OvO)**
- **One-vs-Rest (OvR)**

---

### 3. **One-vs-One (OvO) Approach**

1. For each pair of classes (i, j), train a binary classifier \( A\_{i,j} \) using only data from class i and class j.
2. This creates \( \frac{k(k - 1)}{2} \) classifiers.
3. To classify a new object \( X \), each model \( A\_{i,j} \) votes for either class i or j.
4. Count all votes and assign \( X \) to the class with the most votes.

**Drawbacks**:

- If there's a tie (equal number of votes), you need a confidence score to resolve it (if available from the binary classifier).

---

### 4. **One-vs-Rest (OvR) Approach**

1. For each class i:
   - Treat class i as the positive class and all others as negative.
   - Train a binary classifier \( A_i \).
2. You get **k** classifiers: \( A_1, A_2, ..., A_k \).
3. For a new object \( X \), compute the score from each \( A_i \).
4. Predict the class \( y \) where the score is highest:
   \[
   y = \arg\max\_{i \in \{1, 2, ..., k\}} A_i(X)
   \]

**Confidence Scores** (examples depend on algorithm):

- **Perceptron**: Use the activation score \( a = b + W^T X \)
- **Logistic Regression**: Use the sigmoid function \( \sigma(a) = \frac{1}{1 + e^{-a}} \)

**Drawbacks**:

- Classifiers may produce scores on different scales.
- Class imbalance: each classifier is trained on one small positive set vs a large negative set.

---
