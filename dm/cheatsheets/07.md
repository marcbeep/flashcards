
---

## ğŸŒŸ Regularisation Overview

**Purpose**: To reduce **overfitting** by controlling model complexity.

* Overfitting = Model fits training data too closely and performs poorly on new data.
* Regularisation adds a penalty to large model parameters (like weights).

### ğŸ”§ Types of Regularisation:

* **L2 Regularisation**: Penalises the **square** of weights (also called **ridge** or **Tikhonov** regularisation).
* **L1 Regularisation**: Penalises the **absolute value** of weights (leads to sparsity).
* **L1 + L2**: A combination (e.g. ElasticNet).

---

## ğŸ“ˆ Example: Polynomial Curve Fitting

We are given noisy data sampled from:

$$
f(x) = x^3 - 4x^2 + 3x - 2
$$

We try to fit a polynomial of degree $d$ using weights $W = [w_0, w_1, ..., w_d]$, where:

$$
\hat{y}(x, W) = w_0 + w_1 x + w_2 x^2 + \dots + w_d x^d
$$

### ğŸ“‰ Without Regularisation

Loss function (residual sum of squares):

$$
L(D, W) = \sum_{i=1}^n (\hat{y}(x_i, W) - y_i)^2
$$

Problem: As $d$ increases, weights grow large â†’ **overfitting**.

---

## âœ… L2 Regularisation

### ğŸ§® New Loss Function (with regularisation):

$$
J(D, W) = L(D, W) + \lambda \|W\|^2 = \sum_{i=1}^n (\hat{y}(x_i, W) - y_i)^2 + \lambda \sum_{j=1}^d w_j^2
$$

* $\lambda$: regularisation coefficient (controls penalty strength).
* $\|W\|^2$: sum of squares of weights.
* Choose $\lambda$ via **cross-validation**.

---

## âš™ï¸ L2-Regularised Perceptron

### ğŸ§  Model

Perceptron learns a weight vector $W = [w_1, ..., w_d]$ and bias $b$. Prediction:

$$
a = b + \sum_{j=1}^d w_j x_j
$$

$$
\text{Predicted label} = \text{sign}(a)
$$

### ğŸ§® Perceptron Loss Function:

For input $(X_k, y_k)$, loss:

$$
L(b, W, X_k, y_k) = h(-y_k \cdot a_k) \quad \text{where } h(t) = \max(0, t)
$$

Sum over all data points for full loss.

---

## ğŸ” Gradient Descent with L2 Regularisation

We compute gradients and update weights as:

$$
\nabla J = \nabla L + 2\lambda (0, w_1, ..., w_d)^T
$$

This means:

* **No penalty on bias** $b$
* **Penalty on weights** to keep them small

### ğŸ§¾ Update Rule (SGD version)

If a data point $(X, y)$ is **misclassified**:

$$
\begin{aligned}
w_i &\leftarrow w_i \cdot (1 - 2\lambda) + y \cdot x_i \\
b &\leftarrow b + y
\end{aligned}
$$

If **correctly classified**:

$$
\begin{aligned}
w_i &\leftarrow w_i \cdot (1 - 2\lambda) \\
b &\leftarrow b
\end{aligned}
$$

This update gradually **shrinks the weights** toward zero unless supported by the data.

---

## ğŸ§ª Choosing Î» (Hyperparameter Tuning)

1. Split data into **training** and **validation** sets.
2. Try various $\lambda$ values (e.g., $10^{-5}, 10^{-4}, ..., 10^5$)
3. Train models with each and pick the best based on performance on the validation set.

---

## ğŸ“Œ Summary

* Regularisation helps generalisation by preventing weight explosion.
* L2 adds a **quadratic penalty** on weights.
* Works well with gradient-based methods like SGD.
* Can be naturally integrated into Perceptron and other models.

