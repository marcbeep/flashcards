---

## **1. Loss Function Minimization in Perceptron**

### **Training Dataset and Parameters**

- Dataset:  
  ğ’Ÿ = {(Xâ‚, yâ‚), ..., (Xâ‚™, yâ‚™)} where each Xâ‚– is a vector of features and yâ‚– is the label.
- Model Parameters:  
  W = (wâ‚€, wâ‚, ..., w_d) including bias term.

### **Objective**

- Minimize the **loss function** L(W, ğ’Ÿ) by adjusting weights W using optimization techniques.

---

## **2. Perceptron Training Algorithm**

### **Initialization**

- Set all weights to zero:  
  wi = 0 for i = 1 to d  
  b = 0

### **Training Loop**

- Repeat for MaxIter iterations:
  - For each training sample (X, y):
    - Compute activation: a = Wáµ€X + b
    - If misclassified (y Â· a â‰¤ 0):
      - Update weights: wi â† wi + y Â· xi for all i
      - Update bias: b â† b + y

---

## **3. Loss Functions**

### **A. Step Loss Function (Misclassification Count)**

- For a single point:  
  L(b, W, Xâ‚–, yâ‚–) = 1 if misclassified, 0 otherwise
- For dataset:  
  L(b, W, ğ’Ÿ) = Total number of misclassifications

#### **Drawbacks**

- Piecewise constant â†’ Not differentiable
- Gradient = 0 â†’ Gradient descent not usable

---

### **B. Hinge-like Loss Function: h(t) = max(0, t)**

- Activation score:  
  aâ‚– = b + Î£áµ¢ wáµ¢ xâ‚–â½â±â¾
- Individual loss:  
  L(b, W, Xâ‚–, yâ‚–) = h(âˆ’yâ‚– Â· aâ‚–)
- Dataset loss:  
  L(b, W, ğ’Ÿ) = Î£â‚– h(âˆ’yâ‚– Â· aâ‚–)

#### **Properties**

- Loss increases with misclassification severity
- Differentiable (almost everywhere)

---

## **4. Gradient Descent**

### **Gradient Computation**

For each (Xâ‚–, yâ‚–):

- âˆ‚L/âˆ‚b = âˆ’yâ‚– if misclassified, else 0
- âˆ‚L/âˆ‚wáµ¢ = âˆ’yâ‚– Â· xâ‚–â½â±â¾ if misclassified, else 0

Gradient Vector:

- âˆ‡ = âˆ’yâ‚– Â· (1, xâ‚–â½Â¹â¾, ..., xâ‚–â½áµˆâ¾)áµ€ for misclassified samples
- Else, âˆ‡ = 0 vector

### **Update Rule**

- Full batch gradient descent:  
  Update weights using all misclassified points
- Online gradient descent:  
  Update immediately after each misclassification:
  - (b, wâ‚, ..., w_d) â† (b, wâ‚, ..., w_d) + Î¼ Â· y Â· (1, xâ‚, ..., x_d)áµ€
  - Usually Î¼ = 1

---

## **5. Final Training Algorithm (Simplified)**

1. Initialize weights and bias to 0
2. For each iteration:
   - For each sample (X, y):
     - Compute a = Wáµ€X + b
     - If y Â· a â‰¤ 0:
       - wi â† wi + y Â· xi
       - b â† b + y

---
