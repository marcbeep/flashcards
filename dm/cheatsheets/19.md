---

## Divisive Clustering (Top-Down Clustering)

### Core Idea

- **Divisive clustering** starts with all data points in a single large cluster (the root) and **repeatedly splits** them into smaller clusters.
- This creates a **tree-like structure** (a hierarchy), going from general to specific.

### Method Used for Splitting

- Any **flat clustering algorithm** can be used to do the splitting at each step.

  - Example: **k-means**.
  - It does **not** have to be a distance-based method.

---

## Trade-Off Strategies for Splitting

You can control how the tree grows based on your goal:

### Strategy 1: Balance number of objects

- Always split the **cluster with the most points**.
- Result: Clusters (leaf nodes) will tend to have **similar sizes**, but the tree might not be balanced.

### Strategy 2: Balance tree structure

- Always split **every cluster into the same number of subclusters**.
- Result: The **tree is balanced** (same number of children per node), but some clusters will have **more data points** than others.

---

## Generic Divisive Clustering Algorithm

**Input:**

- `𝒟`: Dataset
- `𝒜`: Flat clustering algorithm

**Steps:**

1. Start with a **tree `𝒯`** containing just one node (the root) with all the data (`𝒟`).
2. Repeat:

   - Select a **leaf node `L`** to split (based on a rule, like the biggest cluster).
   - Use `𝒜` to split `L` into smaller clusters: `L₁, L₂, ..., Lₖ`.
   - Add these `L₁...Lₖ` as **children of `L`** in the tree.

3. Stop when a **termination condition** is met (e.g., enough clusters made, or clusters are small enough).
4. Return the clustering or the full hierarchy (tree).

---

## Bisecting k-Means Algorithm (A Special Case)

This is a popular way to do divisive clustering.

**Input:**

- `𝒟`: Dataset
- `s`: Desired number of clusters

**Steps:**

1. Start with a **tree `𝒯`** containing one node with all data.
2. Repeat:

   - Choose the **leaf node (cluster) `L`** with the **largest total squared distance** between its points.

     - Mathematically:

       $$
       \sum_{\text{all pairs } X, Y \in L} \text{dist}(X, Y)^2
       $$

       This measures **how spread out** the cluster is.

   - Use **k-means with k=2** to split `L` into `L₁` and `L₂`.
   - Add `L₁` and `L₂` as children of `L` in the tree.

3. Stop when there are `s` leaf clusters.
4. Return the leaf clusters (the final groups).

---

## Visual Example (from slides)

Multiple slides visually show how the **bisecting k-means** process works in steps:

- Start with one big cluster.
- Repeatedly choose the widest-spread cluster to split.
- Use k-means to divide it.
- Grow the tree and stop when you hit the target number of clusters.

---
