---

## **k-Medoids Clustering Algorithm**

### â— Problems with k-Means (why k-Medoids is useful)

- **Sensitive to initialisation**: Results vary depending on the starting points.
- **Can get stuck in local minima**: May not find the best (global) clustering.
- **Outliers affect results**: Because k-means uses the mean, which is easily skewed.
- **Cluster centers are not actual data points**: In k-means, centers can be "imaginary".
- **Not suitable for categorical data**: Euclidean distance (used by k-means) doesnâ€™t work well with categories.

---

## âœ… Key Features of k-Medoids

- A **representative-based algorithm**: Like k-means, it tries to find 'k' representatives (medoids) to minimize a total distance cost.
- **Medoids are real data points**: Cluster centers are selected from actual data.
- **Can use any distance (dissimilarity) function**: Not limited to Euclidean distance.
- **More robust to noise/outliers** than k-means.

---

## ğŸ¯ Objective Function

The goal is to minimize the sum of distances from each point to the closest medoid:

$$
\sum_{i=1}^{n} \min_{j} d(X_i, Y_j)
$$

- **$X_i$** = i-th data point
- **$Y_j$** = j-th medoid (cluster representative)
- **$d(â‹…,â‹…)$** = distance (dissimilarity) function (e.g., Euclidean, Manhattan, etc.)

---

## ğŸ§— Hill-Climbing Strategy

Used during the optimization phase:

1. Start with a random solution (set of medoids).
2. Make a small change (swap a medoid with a non-medoid).
3. If the change improves the result (i.e., reduces the objective), accept it.
4. Repeat until no further improvement is possible.

---

## ğŸ§® k-Medoids Clustering Algorithm: Step-by-Step

Given:

- Dataset $\mathcal{D} = \{X_1, ..., X_n\}$
- Number of clusters $k$

### **1. Initialization**

Randomly choose $k$ data points as the initial medoids: $Y_1, ..., Y_k$

### **2. Assignment**

Assign each data point to the nearest medoid â†’ creates clusters $C_1, ..., C_k$

### **3. Optimization (Hill-Climbing)**

- For each possible pair $(X, Y)$ where:

  - $X \in \mathcal{D}$ (a data point)
  - $Y \in \{Y_1, ..., Y_k\}$ (a medoid)

- Try replacing $Y$ with $X$
- If the total distance decreases, update the medoid and repeat the assignment step.
- Stop when no better swap exists.

---

## âœ”ï¸ Pros of k-Medoids

- Medoids are actual data â†’ easy to interpret.
- Robust to **noise and outliers**.
- Works with any **distance function** â†’ suitable for **categorical, mixed, time-series** data.

---

## âŒ Cons of k-Medoids

- **Still sensitive to initialisation**: May need multiple runs.
- **Can get trapped in local optima**.
- **Slower than k-means**: Due to complex medoid-swapping step.

---

## â±ï¸ Time Complexity Issue

- At each optimization step, checking **every possible swap** requires:

  $$
  k \cdot n \text{ computations}
  $$

  (where $n$ = number of data points, $k$ = number of medoids)

- This is expensive, especially for large datasets.

### ğŸ”§ Solution:

Instead of checking all possible $k \cdot n$ swaps:

- Randomly sample $r$ pairs $(X, Y)$, where:

  - $X$ is a data point
  - $Y$ is a current medoid

- Only compute improvements for these $r$ pairs â†’ saves time.

---
