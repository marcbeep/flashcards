---

## **1. Introduction to Perceptron**
- **Nature**: Binary classification algorithm.
- **Inspired by Biology**: 
  - Mimics the human nervous system.
  - Neurons and synapses form the basis.
  - Learning occurs by adjusting synaptic strengths based on external stimuli.
  - Perceptron simulates a **single neuron**.

---

## **2. Perceptron Model**

### **Architecture**

- Inputs: \( x_1, x_2, ..., x_d \)
- Weights: \( w_1, w_2, ..., w_d \)
- Activation score:  
  \[
  a = w_1x_1 + w_2x_2 + \cdots + w_dx_d
  \]
- Output:
  - \( 1 \) if \( a > \theta \)
  - \( -1 \) if \( a \leq \theta \)

---

## **3. Mathematical Notation**

- Input vector: \( X^T = (x_1, x_2, ..., x_d) \)
- Weight vector: \( W^T = (w_1, w_2, ..., w_d) \)
- Activation: \( a = W^T X \)

### **Bias Term**

- Makes threshold \( \theta = 0 \)
- Introduced as \( b = -\theta \)
- Activation:  
  \[
  a = W^T X + b
  \]
- Output: \( \text{sign}(W^T X + b) \)

---

## **4. Notational Trick (for Bias)**

- Introduce a constant feature \( x_0 = 1 \)
- Let \( w_0 = b \)
- Rewrite activation as:
  \[
  a = \sum\_{i=0}^{d} w_i x_i = W^T X
  \]
- Makes equations more elegant by embedding the bias in weights.

---

## **5. Training Algorithm (PerceptronTrain)**

### **Steps**

1. Initialize all weights \( w_i = 0 \) and bias \( b = 0 \)
2. For a maximum number of iterations:
   - For each training example \( (X, y) \):
     - Compute \( a = W^T X + b \)
     - If misclassified \( (y \cdot a \leq 0) \):
       - Update weights: \( w_i = w_i + y \cdot x_i \)
       - Update bias: \( b = b + y \)

---

## **6. Test Algorithm (PerceptronTest)**

- Given: weights \( W \), bias \( b \), and test input \( X \)
- Compute activation: \( a = W^T X + b \)
- Output: \( \text{sign}(a) \)

---

## **7. Key Features**

- **Online Learning**: Updates occur one example at a time.
- **Error-Driven**: Updates only on misclassification.

---

## **8. Misclassification Detection**

- An object is misclassified if:
  \[
  y \cdot a \leq 0
  \]

---

## **9. Perceptron Update Rule**

### **Intuition**

- **Misclassified Positive**:
  - Increase activation: add current example to weights and bias.
- **Misclassified Negative**:
  - Decrease activation: subtract current example from weights and bias.

### **Mathematical Update**

- \( W = W + yX \)
- \( b = b + y \)

---

## **10. Mathematical Explanation of Update**

- After update on misclassification:
  \[
  a' = a + \sum\_{i=1}^{d} x_i^2 + 1 > a
  \]
- The adjustment increases activation (for positive misclassification) or decreases it (for negative).

---

## **11. Remarks**

### **Activation Adjustment**

- One update may not fix misclassification.
- More aggressive algorithms (e.g., Passive Aggressive Classifier) might be needed.

### **Training Order**

- Order of data matters.
- Random shuffling in each iteration works best.

### **Hyperparameter and Overfitting**

- MaxIter is a hyperparameter.
- Too many iterations → overfitting.
- Too few iterations → underfitting.

---

## **1. Geometric Interpretation of Perceptron**

### **Decision Boundary**

- The perceptron decides class based on:
  \[
  W^T X + b > 0 \quad \text{(classified as +1)}
  \]
  \[
  W^T X + b \leq 0 \quad \text{(classified as -1)}
  \]
- The **decision boundary** is the set of points where:
  \[
  W^T X + b = 0
  \]
  This forms a **hyperplane**.

---

## **2. Hyperplane and Weight Vector**

- A **hyperplane** is a flat, N-1 dimensional surface in N-dimensional space.
- In **2D**, this is a **line**:  
  \[
  w_1 x_1 + w_2 x_2 = 0
  \]
- In **N dimensions**, it defines an **(N–1)-dimensional hyperplane**.
- The **weight vector \( W \)** is **perpendicular** to the hyperplane—it defines its orientation.

---

## **3. Visualizing Misclassification**

### **Before Update**

- If a **positive instance** \( X \) makes an angle **greater than 90°** with \( W \), then:
  \[
  W^T X < 0
  \]
  → It is misclassified as negative.

### **After Update**

- Perceptron update rule:
  \[
  W' = W + X
  \]
- The new weight vector \( W' \) now lies **between \( W \) and \( X \)**.
- The angle between \( W' \) and \( X \) becomes **less than 90°**.
- So \( W'^T X > 0 \), and the instance is correctly classified.

This is a key geometric idea: the perceptron "rotates" the weight vector towards misclassified examples.

---

## **4. Linearly Separable Data**

- A dataset is **linearly separable** if a hyperplane exists that correctly separates all positive and negative instances.
- There can be **many possible hyperplanes** that do this; the solution is **not unique**.

---

## **5. Non-linearly Separable Data**

- If no straight line (or hyperplane) can divide the classes perfectly, the data is **not linearly separable**.
- Example: Red and blue points mixed in a circular fashion can’t be separated by a straight line.

---

## **6. Important Observations**

- If the data **is** linearly separable:
  - The **perceptron algorithm is guaranteed to find** a separating hyperplane.
- The final result depends on:
  - The **order** of training instances (as discussed earlier).
  - The **last few updates**, which have more influence on the final weights.

---

## **7. Averaged Perceptron (Concept Mentioned)**

- One way to get more stable and general results:
  - **Take the average** of all weight vectors seen during training.
  - This is known as the **Averaged Perceptron algorithm** (not detailed here).

---
