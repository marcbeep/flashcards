---

## Zero Probabilities and Laplace Smoothing

### **Example: Naive Bayes Classification**

We want to predict whether someone will **play** or not based on weather conditions.

Given test instance:
**(Outlook = sunny, Temp = cool, Humidity = high, Windy = true)**

We compute the probability of `play=yes` and `play=no`:

#### Calculation:

**P(Play = yes | X)** =
(2/9) √ó (3/9) √ó (3/9) √ó (9/14) ‚âà **0.0053**

**P(Play = no | X)** =
(3/5) √ó (1/5) √ó (4/5) √ó (3/5) √ó (5/14) ‚âà **0.0206**

ü°∫ Since 0.0206 > 0.0053, we predict **Play = no**.

---

### **How These Probabilities Are Calculated**

Each conditional probability like **P(Outlook = sunny | Play = no)** is calculated using:

$$
P(x_i = a | C = c) = \frac{n(a, c)}{N(c)}
$$

- $n(a, c)$: Number of training examples in class **c** with feature value **a**
- $N(c)$: Total number of training examples in class **c**

---

### **Problem: Zero Probabilities**

Imagine a test instance:
**(Outlook = overcast, Temp = cool, Humidity = high, Windy = true)**

Now, assume:

$$
P(Outlook = overcast | Play = no) = 0
$$

Then:

$$
P(Play = no | X) = 0 √ó ‚Ä¶ = 0
$$

No matter what other features say, the result becomes **0** just because of one zero probability.

This is bad for small datasets and **even worse for datasets with many features** (e.g., text classification with 1000s of words).

---

### **Solution: Laplace Smoothing**

To avoid multiplying by **0**, we slightly increase every count‚Äîeven if it‚Äôs currently 0.

#### Formula:

$$
P(x_i = a | C = c) = \frac{n(a, c) + 1}{N(c) + m_i}
$$

Where:

- $n(a, c)$: number of times feature value **a** appears in class **c**
- $N(c)$: total number of examples in class **c**
- $m_i$: number of possible values for feature **x_i**

This ensures:

- No probability is zero
- All probabilities still sum to 1

---

### **Before and After Laplace Smoothing**

Let‚Äôs say we have 5 possible values for a feature, and current counts for class **c** are:

| Value | Count (before) | Probability (before) |
| ----- | -------------- | -------------------- |
| a1    | 3              | 3/9                  |
| a2    | 1              | 1/9                  |
| a3    | 0              | 0/9 ‚Üê problem!       |
| a4    | 2              | 2/9                  |
| a5    | 3              | 3/9                  |

Apply Laplace smoothing:

New total = 9 + 5 = **14**
Each count increases by 1:

| Value | Count (after) | Probability (after)  |
| ----- | ------------- | -------------------- |
| a1    | 4             | 4/14                 |
| a2    | 2             | 2/14                 |
| a3    | 1             | 1/14 ‚Üê no more zero! |
| a4    | 3             | 3/14                 |
| a5    | 4             | 4/14                 |

---

### **Key Takeaway**

- **Zero probabilities** can ruin predictions in Naive Bayes classifiers.
- **Laplace smoothing** fixes this by adding 1 to each count.
- It ensures no probability is zero while keeping total probability valid.
