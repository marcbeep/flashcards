---

### **PageRank Algorithm (by Google)**

**Goal:**
Determine how _important_ a webpage is â€” so that more important pages show up earlier in search results.

---

### **1. Web as a Graph**

Think of the Web as a directed graph:

- **Nodes** = webpages
- **Edges (links)** = hyperlinks from one page to another

Each page can:

- **Receive in-links** (other pages link to it)
- **Send out-links** (it links to other pages)

---

### **2. Intuition Behind PageRank**

PageRank treats links like _votes_:

- A link from Page A to Page B is a vote from A for B.
- But not all votes are equal!
  A vote from an _important_ page is worth _more_.

So, a page is important if:

- Many pages link to it **AND**
- Those linking pages are themselves important.

---

### **3. Basic PageRank Formula**

Letâ€™s define:

- $P(a)$: PageRank score of page $a$
- $O_a$: Number of out-links from page $a$
- $E$: Set of all directed links (edges) in the web graph

The formula for PageRank is:

$$
P(a) = \sum_{(x,a) \in E} \frac{P(x)}{O_x}
$$

This means:

- To get the score of page $a$, sum up the scores of all pages linking to it.
- But each of those scores is divided by how many pages they link to (because each vote is shared).

This creates one equation like this for **every page** â€” together they form a **system of equations**.

---

### **4. Matrix Representation**

Letâ€™s organize everything into matrices:

- Let there be $n$ pages.
- $P$ is a column vector:

  $$
  P = [P(1), P(2), ..., P(n)]^T
  $$

- Define matrix $A$ such that:

  - $A_{ij} = \frac{1}{O_i}$ if there is a link from page $i$ to page $j$
  - Otherwise, $A_{ij} = 0$

The entire system can be written as:

$$
P = A^T P
$$

So, the PageRank vector $P$ is an **eigenvector** of $A^T$, with eigenvalue 1.

---

### **5. Solving with Power Iteration**

To find $P$, we use the **power iteration method**:

1. Start with an initial guess (say, all pages equally important).
2. Repeatedly compute:

   $$
   P^{(i)} = A P^{(i-1)}
   $$

3. Stop when the values stabilize (change is less than a small threshold $\varepsilon$).

---

### **6. Real-World Challenges**

The above method **assumes** certain mathematical conditions (like connectedness and no "dangling nodes").
But the real web graph doesn't satisfy these, so:

ðŸ‘‰ **Later refinements** (not covered here) add fixes like "teleporting" (random jumps) to ensure convergence and robustness.

---
