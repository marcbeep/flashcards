---

## **Probabilistic Classifiers**

### **1. Ordinary vs Probabilistic Classifiers**

- **Ordinary classifier**: A function $f(X)$ assigns an input $X$ to a class $c \in \{c_1, c_2, ..., c_k\}$.

- **Probabilistic classifier**: Instead of a single class, it gives probabilities $p_i = P(c_i | X)$ for each class. These probabilities form a distribution and must sum to 1:

  $$
  p_1 + p_2 + \dots + p_k = 1
  $$

---

### **2. Discriminative vs Generative Models**

- **Discriminative Models**:

  - Model the conditional probability directly: $P_\theta(C | X)$
  - Learn parameters $\theta$ from data that best fit this form.

- **Generative Models**:

  - Model the joint probability: $P_\theta(X, C)$
  - Learn parameters from data, then use Bayes’ rule to get $P(C | X)$

---

### **3. Generative Models in Detail**

- Assume data is generated from some unknown distribution $P(X, c)$.

- If known, we could compute:

  $$
  f^*(X) = \arg\max_{c \in \mathcal{C}} P(X, c)
  $$

  This is the **Bayes Optimal Classifier** – it makes the least error on average.

- In practice, we don't know $P$, so we approximate it with $\hat{P}$ learned from data.

---

### **4. Model Assumptions and Parameter Estimation**

- Assume $P(X, c)$ belongs to a parametric family (e.g., Normal distribution).
- Use **i.i.d. assumption**: training data are independently and identically distributed.
- Estimate parameters using **Maximum Likelihood Estimation (MLE)**.

---

### **5. MLE – Example with One Parameter (Bernoulli)**

#### **Example 1: Biased Coin (Binary Classification)**

- Suppose we flip a coin and observe: T, H, H, H
- Let $\beta$ be the probability of heads (H).
- The likelihood of observing this sequence:

  $$
  P_\beta(THHH) = (1 - \beta) \cdot \beta^3 = \beta^3 - \beta^4
  $$

- To find the best $\beta$, differentiate and set to 0:

  $$
  \frac{d}{d\beta}(\beta^3 - \beta^4) = 3\beta^2 - 4\beta^3 = 0
  $$

  Solve:

  $$
  \beta = \frac{3}{4}
  $$

#### **General Case (h heads, t tails):**

- Likelihood: $\beta^h (1 - \beta)^t$
- Log-likelihood (easier to work with):

  $$
  \log(\beta^h (1 - \beta)^t) = h \log \beta + t \log(1 - \beta)
  $$

- Maximise this to get MLE of $\beta$.

---

### **6. MLE – Example with Multiple Parameters (Multiclass)**

#### **Example 3: K-sided Die**

- Let $\beta_1, \beta_2, ..., \beta_K$ be the probabilities of sides 1 through K.
- Suppose $x_i$ is the number of times side $i$ appeared.
- Likelihood:

  $$
  \prod_{i=1}^{K} \beta_i^{x_i}
  $$

- Log-likelihood:

  $$
  \sum_{i=1}^{K} x_i \log \beta_i
  $$

- Subject to: $\sum_{i=1}^{K} \beta_i = 1$

#### **Use Lagrange Multipliers:**

- Define:

  $$
  \mathcal{L}(\beta, \lambda) = \sum x_i \log \beta_i - \lambda\left( \sum \beta_i - 1 \right)
  $$

- Solve:

  $$
  \frac{\partial \mathcal{L}}{\partial \beta_i} = \frac{x_i}{\beta_i} - \lambda = 0 \Rightarrow \beta_i = \frac{x_i}{\lambda}
  $$

- Enforce the constraint:

  $$
  \sum \beta_i = 1 \Rightarrow \lambda = \sum x_i
  $$

  So:

  $$
  \beta_i = \frac{x_i}{\sum x_i}
  $$

---

### **7. Summary: Probabilistic Classifiers**

- **Generative Models** (model $P(X, C)$, use Bayes rule to classify):

  - Naive Bayes

- **Discriminative Models** (model $P(C | X)$ directly):

  - Logistic Regression
  - Neural Networks (Multilayer Perceptrons)

---
