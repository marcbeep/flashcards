---

## **K-Nearest Neighbours (K-NN)**

### 1. **What is K-NN?**

K-NN is one of the simplest classification algorithms. It works as follows:

- **Training phase:** Just store the training data. There’s no learning or model building.
- **Classification phase:**

  - For a new (test) data point, find the **k** closest points in the training set.
  - Look at their labels.
  - Assign the **most common label** among them to the test point.

If **k = 1**, we just find the closest point and copy its label.

---

### 2. **How Distance is Measured (Similarity/Distance Functions)**

To find “closest” points, we measure how similar or distant two points are. Several methods are used:

#### a. **Numerical Data**

**i. Euclidean Distance (L2 norm):**

$$
\text{Euclidean}(X, Y) = \sqrt{\sum_{i=1}^{d}(x_i - y_i)^2}
$$

- Measures straight-line distance.
- Larger distances = more dissimilar.

**ii. Manhattan Distance (L1 norm):**

$$
\text{Manhattan}(X, Y) = \sum_{i=1}^{d}|x_i - y_i|
$$

- Adds absolute differences in each dimension.
- Like navigating a city grid.

**iii. Cosine Similarity:**

$$
\text{Cosine}(X, Y) = \frac{X \cdot Y}{\|X\|\|Y\|} = \cos(\theta)
$$

- Measures angle between vectors.
- Used often in text analysis.
- Cosine distance = $1 - \text{Cosine similarity}$.

---

#### b. **Set Data**

- **Jaccard Similarity:**

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$

- **Jaccard Distance =** $1 - J(A, B)$

- **Overlap Coefficient:**

$$
\text{Overlap}(A, B) = \frac{|A \cap B|}{\min(|A|, |B|)}
$$

---

#### c. **Binary Data**

- **Hamming Distance:**

$$
\text{Hamming}(X, Y) = \text{number of differing positions}
$$

E.g., $X = (1,0,1), Y = (0,0,1) \Rightarrow \text{Distance} = 1$

---

#### d. **Categorical Data**

- **Basic similarity:**

$$
S(x_i, y_i) =
\begin{cases}
1 & \text{if } x_i = y_i \\
0 & \text{otherwise}
\end{cases}
$$

- **Frequency-based similarity (accounts for rare categories):**

$$
S(x_i, y_i) =
\begin{cases}
1/p_i(x_i)^2 & \text{if } x_i = y_i \\
0 & \text{otherwise}
\end{cases}
$$

where $p_i(x_i)$ = frequency of value $x_i$ in the dataset.

---

### 3. **Choosing the Value of k**

- **k** is a hyperparameter: you decide its value, it's not learned.
- Rule of thumb:

  - Large datasets → larger k.
  - Small datasets → smaller k (but not too small).

- Avoid picking k based on test performance—this leads to overfitting.

#### ✅ Good practice:

- Use a **validation set** (a subset of the training data not used for training) to choose k.
- Or use **cross-validation**:

  - Split training data into multiple folds.
  - Train and validate on different folds.
  - Average the performance to choose the best k.

---

### 4. **Complexity of K-NN**

- **Training time:** Very fast. Just store the data.
- **Classification time:** Slow. You must compare each test point to every training point.

If the dataset is big, this becomes a problem. To speed up:

- Use **Approximate Nearest Neighbour (ANN)** algorithms (e.g., FLANN).

---

### 5. **Limitations and Enhancements**

#### a. **Distance-Weighted Voting**

In regular k-NN, all neighbours have equal vote. Instead:

- Weight each vote based on distance:

$$
w_i = \frac{1}{\text{distance}(X', Y_i)^2}
$$

- Add up weights for each class, choose the class with the highest total.

#### b. **Feature Importance**

- K-NN assumes **all features are equally important**, which is often not true.
- Irrelevant features can hurt performance.

#### c. **Feature Scaling**

Features must be on the **same scale**.

- Use **Gaussian Normalization** (a.k.a. Z-score normalization):

$$
x_{\text{new}} = \frac{x - \mu}{\sigma}
$$

Where:

- $\mu$ is the mean of the feature.
- $\sigma$ is the standard deviation.

This centers data around 0 with standard deviation 1. Essential before applying distance measures.

---

### 6. **Practical Tips Summary**

- Always normalize your data before using k-NN.
- Choose k using validation or cross-validation.
- Use L1 (Manhattan) or L2 (Euclidean) distances; try both.
- If high-dimensional, consider **dimensionality reduction** before k-NN.
- Beware of irrelevant features; they can mislead distance calculations.

---
