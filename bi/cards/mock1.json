[
  {
    "q": "Which statement best characterises the difference between on-policy and off-policy learning in reinforcement learning?\n- A) On-policy learns optimal value of any target policy while following an unrelated behaviour policy.\n- B) On-policy updates its estimates using data generated by the current behaviour policy, whereas off-policy can learn about a separate target policy from data generated by any behaviour policy via importance sampling or bootstrapping.\n- C) On-policy methods are always model-based; off-policy methods are always model-free.\n- D) On-policy is equivalent to Monte-Carlo; off-policy is equivalent to Temporal Difference.",
    "a": "- **Correct answer: B**\n- On-policy: evaluates/improves the *same* policy that generates the data.\n- Off-policy: evaluates/improves a *different* target policy using behaviour-policy experience (often needs importance sampling or bootstrapped estimates).\n- **Why others are wrong**:\n  - A) reverses roles: description actually matches off-policy.\n  - C) Model-based vs model-free is independent of on/off-policy.\n  - D) MC and TD can each be on- or off-policy; correspondence stated is false."
  },
  {
    "q": "In TD(\\(\\lambda\\)) learning, what is the effect of increasing \\(\\lambda\\) towards 1?\n- A) It makes updates rely *only* on the immediate reward.\n- B) It makes the algorithm behave like Monte-Carlo methods, using longer backups and lower bias but higher variance.\n- C) It decreases the variance of the return estimate while keeping the same bias.\n- D) It eliminates the need for eligibility traces.",
    "a": "- **Correct answer: B**\n- A larger \\(\\lambda \\to 1\\) weights longer returns more heavily, approaching the Monte-Carlo target (low bias, high variance).\n- **Why others are wrong**:\n  - A) Small \\(\\lambda\\) (\\(0\\)) uses only one-step reward, not large.\n  - C) Variance *increases* as \\(\\lambda\\) rises.\n  - D) Eligibility traces are needed to implement TD(\\(\\lambda\\)) for any \\(\\lambda \\neq 0,1\\)."
  },
  {
    "q": "Which term in the Q-learning update causes *bootstrapping*?\n\\[\nQ(s,a) \\leftarrow Q(s,a) + \\alpha \\bigl[\\,r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\bigr]\\]\n- A) The learning-rate \\(\\alpha\\)\n- B) The discount factor \\(\\gamma\\)\n- C) The target term \\(\\gamma \\max_{a'} Q(s',a')\\)\n- D) The immediate reward \\(r\\)",
    "a": "- **Correct answer: C**\n- Bootstrapping means updating from an *estimate of future value* rather than a full return. The term \\(\\gamma \\max_{a'} Q(s',a')\\) is that estimate.\n- **Why others are wrong**:\n  - A) \\(\\alpha\\) controls step size but is not the bootstrapped estimate.\n  - B) \\(\\gamma\\) scales future reward but by itself is not the estimate.\n  - D) \\(r\\) is an observed signal, not an estimate."
  },
  {
    "q": "Which of the following best explains why the ReLU activation mitigates the vanishing-gradient problem compared with the sigmoid activation?\n- A) ReLU is differentiable everywhere so its gradient is always 1.\n- B) ReLU outputs are unbounded above and its derivative is constant 1 for positive inputs, preventing exponential shrinkage of gradients in deep layers.\n- C) ReLU saturates at both ends, keeping gradients near zero.\n- D) ReLU enforces weight sharing which stabilises gradients.",
    "a": "- **Correct answer: B**\n- Positive inputs have derivative 1, so repeated multiplication through layers does **not** exponentially decay as with sigmoid derivatives \\(<1\\).\n- **Why others are wrong**:\n  - A) ReLU is non-differentiable at 0 and derivative is 0 for negative inputs.\n  - C) Sigmoid saturates; ReLU only on one side.\n  - D) Weight sharing is unrelated to activation choice."
  },
  {
    "q": "Dropout reduces overfitting in deep neural networks primarily by:\n- A) Decreasing the learning rate during training.\n- B) Stochastically omitting hidden units, which forces the network to learn redundant, robust representations.\n- C) Adding L1 regularisation to all weights.\n- D) Normalising activations to zero mean and unit variance.",
    "a": "- **Correct answer: B**\n- Randomly *dropping* units prevents co-adaptation, acting like an ensemble of subnetworks.\n- **Other options**:\n  - A) Learning-rate schedules are separate.\n  - C) L1 is a different regulariser.\n  - D) Describes batch-norm, not dropout."
  },
  {
    "q": "Consider an \\(\\epsilon\\)-greedy exploration strategy in a non-stationary bandit. Which decay schedule is most appropriate?\n- A) Keep \\(\\epsilon\\) constant at 0.01 throughout.\n- B) Decrease \\(\\epsilon\\) aggressively to near zero after few steps.\n- C) Use a modest constant (e.g. 0.1) or cyclic reset to maintain exploration because the optimal arm may change over time.\n- D) Increase \\(\\epsilon\\) over time to explore more when estimates are accurate.",
    "a": "- **Correct answer: C**\n- Non-stationary rewards demand *continued* exploration; either constant moderate \\(\\epsilon\\) or periodic resets works.\n- **Why wrong**:\n  - A) Too small → premature convergence.\n  - B) Same issue, more severe.\n  - D) Increasing \\(\\epsilon\\) later wastes samples when estimates stabilise."
  },
  {
    "q": "Which component *cannot* be removed from the velocity update of canonical Particle Swarm Optimisation without fundamentally changing its search dynamics?\n\\[\nv_i \\leftarrow w v_i + c_1 r_1 (p_i - x_i) + c_2 r_2 (g - x_i)\n\\]\n- A) Momentum term \\(w v_i\\)\n- B) Personal attraction \\(c_1 r_1 (p_i - x_i)\\)\n- C) Social attraction \\(c_2 r_2 (g - x_i)\\)\n- D) All three terms are individually indispensable to retain both exploration and exploitation behaviour.",
    "a": "- **Correct answer: D**\n- Removing any single term disrupts balance: momentum (inertia) enables exploration, personal term maintains individual memory, social term drives convergence.\n- **Why the other single-choice answers are wrong**: each claims a particular term can be removed, which contradicts PSO theory."
  },
  {
    "q": "In Ant Colony Optimisation for the TSP, pheromone *evaporation* serves mainly to:\n- A) Intensify exploitation of the current best tour.\n- B) Avoid premature convergence by gradually forgetting sub-optimal paths, enabling exploration.\n- C) Increase computational efficiency by pruning edges.\n- D) Ensure pheromone trails remain constant over time.",
    "a": "- **Correct answer: B**\n- Evaporation lowers all trails, so old bad decisions fade; without it, algorithm locks into early tours.\n- **Other options**:\n  - A) Intensification is achieved by additional deposition, not evaporation.\n  - C) Evaporation affects probabilities only, not pruning.\n  - D) Opposite of reality."
  },
  {
    "q": "Which property of zero-sum stochastic games guarantees that any equilibrium (minimax) value is unique across all policies?\n- A) Discount factor \\(\\gamma < 1\\).\n- B) Rewards sum to a constant so state values for optimal play coincide for both agents.\n- C) The state space is finite.\n- D) Perfect information of transitions.",
    "a": "- **Correct answer: B**\n- Zero-sum implies one player's gain is the other's loss; hence the *value* \\(V^*(s)\\) is unique even though optimal policies may not be.\n- **Why others are wrong**:\n  - A) \\(\\gamma\\) ensures convergence but not uniqueness.\n  - C) Finiteness is helpful but non-zero-sum can still have multiple equilibrium values.\n  - D) Information does not enforce uniqueness."
  },
  {
    "q": "Stacking three 3×3 convolution layers with stride 1 (and padding 1) in a CNN achieves what effective receptive field compared with a single 7×7 layer?\n- A) 3×3\n- B) 5×5\n- C) 7×7 while using fewer parameters and adding extra non-linearities.\n- D) 9×9",
    "a": "- **Correct answer: C**\n- Receptive field grows additively: three 3×3 layers cover 7×7 area, but parameter count is 3·(3×3×C^2) vs 1·(7×7×C^2) and add ReLUs.\n- Other options underestimate or overestimate."
  },
  {
    "q": "Which gate in an LSTM cell directly mitigates the vanishing-gradient problem by providing an *additive* path for the gradients?\n- A) Forget gate\n- B) Input gate\n- C) Output gate\n- D) The cell (memory) state with its self-connection",
    "a": "- **Correct answer: D**\n- The constant error carousel (cell state) with self-connection of weight 1 allows gradients to flow unchanged.\n- Gates A-C *modulate* but the self-connected cell provides the path."
  },
  {
    "q": "Early stopping is best interpreted as which form of regularisation?\n- A) L0 norm\n- B) Implicit L2 (shrinkage) because weights remain closer to initial small values when training halts before convergence.\n- C) Data augmentation\n- D) Ensemble averaging",
    "a": "- **Correct answer: B**\n- Shorter training keeps weights small ≈ quadratic penalty; theory shows equivalence to L2 for convex losses.\n- Others unrelated."
  },
  {
    "q": "In actor–critic methods, using the *advantage* \\(A(s,a)=Q(s,a)-V(s)\\) as the critic target primarily:\n- A) Increases bias.\n- B) Reduces the variance of policy-gradient estimates without changing expected gradient.\n- C) Eliminates the need for a value function.\n- D) Forces the actor to be deterministic.",
    "a": "- **Correct answer: B**\n- Subtracting a baseline (here \\(V(s)\\)) keeps expectation but lowers variance.\n- Other statements incorrect."
  },
  {
    "q": "What is the main role of the *target network* in Deep Q-Learning (DQN)?\n- A) To provide additional exploration noise.\n- B) To stabilise learning by supplying a slowly-changing bootstrap target so the moving target problem is reduced.\n- C) To enforce an on-policy update.\n- D) To compute eligibility traces.",
    "a": "- **Correct answer: B**\n- Fixing parameters \\(\\theta^-\\) for several steps minimises oscillations/divergence.\n- Others unrelated."
  },
  {
    "q": "Momentum in stochastic gradient descent helps optimisation by:\n- A) Increasing learning rate when gradients change sign.\n- B) Accumulating an exponentially weighted moving average of past gradients, dampening oscillations across high-curvature directions.\n- C) Ensuring convergence to global minimum.\n- D) Performing line search at every step.",
    "a": "- **Correct answer: B**\n- Momentum term \\(v_t=\\beta v_{t-1}+ (1-\\beta)\\nabla L\\) smooths updates, accelerates along consistent directions.\n- A) describes RMSProp/Adam adaptivity.\n- C) Not guaranteed.\n- D) Irrelevant."
  },
  {
    "q": "An Evolutionarily Stable Strategy (ESS) must satisfy which condition(s) for any mutant strategy \\(y\\)?\n- A) \\(u(x,x) > u(y,x)\\) only.\n- B) Either \\(u(x,x) > u(y,x)\\) or \\(u(x,x)=u(y,x)\\) *and* \\(u(x,y) > u(y,y)\\).\n- C) \\(u(x,y) = u(y,x)\\).\n- D) \\(u(y,y) > u(x,x)\\).",
    "a": "- **Correct answer: B**\n- ESS definition: must resist invasion.\n- Other options incomplete or incorrect."
  },
  {
    "q": "In PSO, decreasing the inertia weight \\(w\\) linearly from 0.9 to 0.4 over iterations typically:\n- A) Increases exploration progressively.\n- B) Encourages early exploration and later exploitation/convergence.\n- C) Eliminates the need for personal-best memory.\n- D) Has no measurable effect.",
    "a": "- **Correct answer: B**\n- High initial \\(w\\) spreads swarm; lower final \\(w\\) tightens search.\n- Others wrong."
  },
  {
    "q": "Upper Confidence Bound (UCB1) for a K-armed bandit selects arm \\(i\\) that maximises\n\\[ Q_i + c\\sqrt{\\frac{\\ln t}{N_i}} \\].\nWhat is the *intuitive* role of the second term?\n- A) Pure exploitation of empirical mean.\n- B) Penalising rarely chosen arms.\n- C) Optimism under uncertainty: favour arms with higher uncertainty (low \\(N_i\\)).\n- D) Decaying learning rate.",
    "a": "- **Correct answer: C**\n- Bonus term shrinks with \\(N_i\\); encourages early exploration.\n- B) opposite sign; actually *rewards* rare arms.\n- Others incorrect."
  },
  {
    "q": "Eligibility traces can be viewed as:\n- A) A Monte-Carlo method with zero variance.\n- B) A mechanism that assigns decaying credit to previously visited state-action pairs, effectively interpolating between TD(0) and Monte-Carlo return lengths.\n- C) An exploration strategy.\n- D) A replacement for the discount factor.",
    "a": "- **Correct answer: B**\n- Trace decay parameter \\(\\lambda\\) controls mix.\n- Others wrong."
  },
  {
    "q": "Which of the following is *not* required to compute the Nash equilibrium of a two-player normal-form zero-sum game using linear programming?\n- A) Each player's payoff matrix.\n- B) Objective of maximising the player's worst-case payoff.\n- C) Probabilistic constraints that strategies sum to 1.\n- D) A discount factor \\(\\gamma<1\\).",
    "a": "- **Correct answer: D**\n- Discount only for *stochastic* or repeated games; static matrix game LP needs no \\(\\gamma\\)."
  },
  {
    "q": "Why does bootstrapping in TD methods potentially introduce bias compared with Monte-Carlo methods?\n- A) Because TD uses real rewards.\n- B) Because the bootstrap target relies on an estimated value of the next state that can be systematically wrong.\n- C) Because TD ignores the discount factor.\n- D) Because TD requires a model of the environment.",
    "a": "- **Correct answer: B**\n- Estimate error propagates, leading to bias.\n- Others incorrect."
  },
  {
    "q": "In genetic algorithms, mutation primarily provides:\n- A) Local exploitation around high-fitness individuals.\n- B) Diversity injection allowing exploration of new regions and prevention of premature convergence.\n- C) Combining building blocks from two parents.\n- D) Guarantee of monotonic fitness improvement each generation.",
    "a": "- **Correct answer: B**\n- Crossover handles C.\n- Others wrong."
  },
  {
    "q": "A Q-learning agent uses softmax action selection with temperature \\(\\tau\\). What happens as \\(\\tau \\to 0^+\\)?\n- A) The policy becomes uniform random.\n- B) The policy approaches greedy with respect to Q-values.\n- C) The update step size shrinks.\n- D) Q-values stop bootstrapping.",
    "a": "- **Correct answer: B**\n- Small \\(\\tau\\) sharply peaks probability on max Q.\n- A) corresponds to large \\(\\tau\\).\n- Others unrelated."
  },
  {
    "q": "In minimax-Q learning for a zero-sum stochastic game, the value update at state \\(s\\) uses\n\\[ V(s) = \\text{val}\\bigl(Q(s,\\cdot,\\cdot)\\bigr) \\]\nwhere *val* denotes:\n- A) The element-wise maximum over joint actions.\n- B) The Nash equilibrium value obtained by solving a linear program that maximises the agent's expected return under the opponent's best counter strategy.\n- C) The mean of Q across actions.\n- D) The minimum of Q over opponent actions only.",
    "a": "- **Correct answer: B**\n- Minimax value is saddle-point of the zero-sum matrix.\n- Others incorrect."
  },
  {
    "q": "Which exploration heuristic explicitly *balances* exploration and exploitation by adding an uncertainty bonus proportional to \\(1/\\sqrt{N_i}\\) to each arm's empirical mean?\n- A) Thompson sampling\n- B) UCB1\n- C) Softmax with high temperature\n- D) \\(\\epsilon\\)-greedy",
    "a": "- **Correct answer: B**\n- Bonus term described is UCB.\n- Others different mechanisms."
  },
  {
    "q": "Which of the following correctly states the *Bellman optimality equation* for the action-value function?\n- A) \\(Q^*(s,a)=\\mathbb{E}[\\,r_{t+1}+\\gamma Q^*(s_{t+1},a_{t+1})\\mid s,a]\\)\n- B) \\(Q^*(s,a)=\\mathbb{E}[\\,r_{t+1}+\\gamma \\max_{a'} Q^*(s_{t+1},a')\\mid s,a]\\)\n- C) \\(Q^*(s,a)=\\mathbb{E}[\\,r_{t+1}\\mid s,a]\\)\n- D) \\(Q^*(s,a)=\\max_{a'}\\mathbb{E}[\\,r_{t+1}+\\gamma Q^*(s_{t+1},a')\\mid s,a]\\)",
    "a": "- **Correct answer: B**\n- Optimal action-value uses max over next actions *inside* expectation.\n- A) On-policy form, not optimal.\n- C) Missing future reward.\n- D) Max outside expectation is wrong ordering."
  },
  {
    "q": "Derive the Bellman *optimality* equation for the state-value function \\(V^*(s)\\) in a finite Markov Decision Process and explain each step.",
    "a": "- **Definition of optimal value**: \\(V^*(s)=\\max_{\\pi} V^{\\pi}(s)\\).\n- **One-step look-ahead**: start with any first action \\(a\\), then follow an optimal policy thereafter.\n- **Expansion**:\n  \\[ V^*(s) = \\max_a \\mathbb{E}\\bigl[ r_{t+1} + \\gamma V^*(s_{t+1}) \\;\\big|\\; s_t=s, a_t=a \\bigr] \\]\n  - Uses *Markov property*: next state distribution depends only on \\(s,a\\).\n- **Expectations** written with transition probabilities:\n  \\[ = \\max_a \\sum_{s'} P(s'|s,a) \\bigl[ R(s,a,s') + \\gamma V^*(s') \\bigr] \\]\n- **Optimality justification**: after first step the remaining decision problem is identical, so we can recurse.\n- **Key points for full credit**:\n  - Show maximisation appears *before* expectation.\n  - Mention requirement of full knowledge of \\(P\\) and \\(R\\) for exact computation.\n  - State that equation forms a system of \\|S\\| nonlinear equations with unique solution."
  },
  {
    "q": "Back-propagation in a multi-layer perceptron: give the complete weight-update formula for layer \\(l\\) with sigmoid activation, and outline the chain-rule derivation.",
    "a": "- **Forward pass**: \\(a_i^l = \\phi(v_i^l), \\; v_i^l = \\sum_j w_{ij}^l a_j^{l-1}\\).\n- **Loss (MSE)**: \\(E = \\tfrac12 \\sum_k (y_k - d_k)^2\\).\n- **Local error term**: \\(\\delta_i^l = \\frac{\\partial E}{\\partial v_i^l} = (a_i^l - d_i) \\phi'(v_i^l)\\) for output layer; for hidden:\n  \\[ \\delta_i^l = \\phi'(v_i^l) \\sum_k \\delta_k^{l+1} w_{ki}^{l+1} \\]\n- **Weight gradient**: \\(\\frac{\\partial E}{\\partial w_{ij}^l} = \\delta_i^l a_j^{l-1}\\).\n- **Update rule (SGD)**:\n  \\[ w_{ij}^l \\leftarrow w_{ij}^l - \\eta \\; \\delta_i^l a_j^{l-1} \\]\n- **Explain chain rule**: gradient of loss wrt weight decomposes into partials along path output → sigmoid → weight.\n- **Perfect answer points**:\n  - Derivation for both output and hidden layers.\n  - Sigmoid derivative: \\(\\phi'(v)=\\phi(v)(1-\\phi(v))\\).\n  - Mention computational graph leads to efficient back-prop."
  },
  {
    "q": "Design an \\(\\epsilon\\)-greedy exploration schedule for Q-learning in a non-stationary environment and justify it theoretically and empirically.",
    "a": "- **Schedule proposal**:\n  - Start with \\(\\epsilon_0 = 0.5\\).\n  - Exponential decay to a floor: \\(\\epsilon_t = \\max(\\epsilon_{\\min},\\, \\epsilon_0 \\cdot e^{-kt})\\) with \\(\\epsilon_{\\min}=0.1\\), \\(k = 10^{-4}\\).\n  - Every \\(T_{reset}=10^4\\) steps, temporarily reset \\(\\epsilon\\leftarrow 0.3\\) for 100 steps (cyclic boost).\n- **Justification**:\n  - Non-stationarity ⇒ need *persistent* exploration; floor avoids \\(\\epsilon\\to0\\).\n  - Exponential decay still biases toward exploitation as estimates mature.\n  - Periodic boosts let agent detect shifts in optimal arm.\n- **Theoretical notes**:\n  - Ensures each action has \\(P>0\\) of being chosen infinitely often ⇒ convergence of Q-learning in changing MDPs with tracking step sizes (\\(\\alpha_t\\) not decaying too fast).\n  - Balances regret: exploration cost vs adaptation.\n- **Empirical evaluation plan**:\n  - Track average reward and moving regret before and after environment changes.\n  - Compare to constant \\(\\epsilon\\) and monotone decay baselines; show faster recovery after changepoints."
  },
  {
    "q": "Outline Ant Colony Optimisation (ACO) for the Travelling Salesperson Problem, giving formulas for probability selection, pheromone update and algorithmic steps.",
    "a": "- **Representation**: complete graph of cities; ants build tours.\n- **Probabilistic transition** from city \\(i\\) to \\(j\\) for ant \\(k\\) at time \\(t\\):\n  \\[ P_{ij}^{(k)}(t)=\\frac{\\bigl[\\tau_{ij}(t)\\bigr]^\\alpha\\, \\bigl[\\eta_{ij}\\bigr]^\\beta}{\\sum_{m\\in J_k} \\bigl[\\tau_{im}(t)\\bigr]^\\alpha\\, \\bigl[\\eta_{im}\\bigr]^\\beta} \\]\n  - \\(\\tau_{ij}\\): pheromone, \\(\\eta_{ij}=1/d_{ij}\\), \\(J_k\\): unvisited.\n- **Local update** (ACS):\n  \\[ \\tau_{ij}\\leftarrow (1-\\phi)\\tau_{ij}+\\phi\\tau_0 \\]\n- **Global update** after all ants complete tours:\n  \\[ \\tau_{ij}\\leftarrow (1-\\rho)\\tau_{ij}+\\rho\\sum_{k=1}^{m}\\Delta\\tau_{ij}^{(k)},\\; \\Delta\\tau_{ij}^{(k)}=\\begin{cases}1/L_k & \\text{if edge used by ant }k\\\\0 & \\text{else}\\end{cases} \\]\n- **Algorithm steps**:\n  - Initialise \\(\\tau_{ij}=\\tau_0\\).\n  - Repeat until stopping criterion:\n    - Place ants, construct solutions via \\(P_{ij}\\).\n    - Apply local updates during construction.\n    - Evaluate tours; identify best.\n    - Apply global pheromone update (often only best ant contributes).\n- **Perfect answer criteria**: include role of \\(\\alpha,\\beta,\\rho,\\phi\\); discuss evaporation (exploration) vs deposition (exploitation); comment on convergence toward shorter tours."
  },
  {
    "q": "Explain Watkins' Q(\\(\\lambda\\)) algorithm (Q-learning with eligibility traces) including update equations, the effect of *replacing traces*, and conditions for convergence.",
    "a": "- **Eligibility trace** for state-action pair: \\(e_t(s,a)\\). Initialise 0.\n- **Trace update** (replacing):\n  \\[ e_t(s,a)=\\begin{cases}1 & \\text{if } s_t=s, a_t=a\\\\ \\gamma\\lambda e_{t-1}(s,a) & \\text{otherwise}\\end{cases} \\]\n- **TD error**:\n  \\[ \\delta_t = r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1},a') - Q(s_t,a_t) \\]\n- **Q update** for *all* \\((s,a)\\):\n  \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\; \\delta_t \\; e_t(s,a) \\]\n- **Replacing traces**: sets current pair's trace to 1 instead of accumulating >1, avoids excessive credit assignment.\n- **Behaviour**: Bridges TD(0) (\\(\\lambda=0\\)) and Monte-Carlo (\\(\\lambda\\to1\\)) while retaining off-policy property due to max term.\n- **Convergence** (tabular, deterministic learning rates): requires\n  - \\(\\alpha_t\\) satisfies Robbins–Monro conditions.\n  - Behaviour policy explores all pairs infinitely often.\n  - \\(\\lambda\\) may need to decay as episodes progress for strict proofs, though in practice constant works.\n- **Benefits**: faster propagation of reward signals, reduced variance compared with pure MC, retains bootstrapping like Q-learning."
  }
]
