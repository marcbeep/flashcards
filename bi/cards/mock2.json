[
  {
    "q": "Which statement best characterises the difference between on-policy and off-policy learning in reinforcement learning?\n- A) On-policy learns optimal value of any target policy while following an unrelated behaviour policy.\n- B) On-policy updates its estimates using data generated by the current behaviour policy, whereas off-policy can learn about a separate target policy from data generated by any behaviour policy via importance sampling or bootstrapping.\n- C) On-policy methods are always model-based; off-policy methods are always model-free.\n- D) On-policy is equivalent to Monte-Carlo; off-policy is equivalent to Temporal Difference.",
    "a": "B - Correct because it accurately describes the key distinction: on-policy methods learn from the current policy's behavior, while off-policy methods can learn from any behavior policy using techniques like importance sampling. A is incorrect because it confuses the definitions of on-policy and off-policy. C is wrong because both types can be model-based or model-free. D is incorrect because both Monte-Carlo and TD methods can be used in either on-policy or off-policy settings."
  },
  {
    "q": "What is the primary function of the Bellman optimality equation in reinforcement learning?\n- A) It describes how agents should behave randomly during exploration.\n- B) It defines the total reward collected in one episode.\n- C) It provides a recursive relationship to compute the optimal value function.\n- D) It estimates the policy gradient in policy-based methods.",
    "a": "C - Correct because the Bellman optimality equation is fundamental to RL as it provides a recursive way to compute optimal value functions. A is incorrect because it describes exploration strategies, not the Bellman equation. B is wrong because it describes episode returns, not the recursive relationship. D is incorrect because it confuses value-based methods with policy-based methods."
  },
  {
    "q": "How does Q-learning update its Q-values?\n- A) By bootstrapping from the maximum estimated future reward regardless of the current policy.\n- B) By averaging rewards over the entire episode.\n- C) By using Monte-Carlo rollouts to estimate values.\n- D) By only considering immediate rewards.",
    "a": "A - Correct because Q-learning is an off-policy algorithm that updates Q-values using the maximum future reward, independent of the current policy. B is incorrect because it describes Monte-Carlo methods. C is wrong because Q-learning uses bootstrapping, not Monte-Carlo rollouts. D is incorrect because Q-learning considers both immediate and future rewards."
  },
  {
    "q": "Which function helps mitigate the vanishing gradient problem in deep neural networks?\n- A) Tanh\n- B) ReLU\n- C) Softmax\n- D) Sigmoid",
    "a": "B - Correct because ReLU (Rectified Linear Unit) helps prevent vanishing gradients by having a constant gradient of 1 for positive inputs. A and D are incorrect because both Tanh and Sigmoid can suffer from vanishing gradients due to their saturating nature. C is wrong because Softmax is used for multi-class classification, not for addressing gradient issues."
  },
  {
    "q": "Why is dropout used in training neural networks?\n- A) To prevent overfitting by randomly deactivating neurons during training.\n- B) To convert a feedforward network into a recurrent one.\n- C) To reduce computation time during inference.\n- D) To increase the number of neurons in the network.",
    "a": "A - Correct because dropout is a regularization technique that randomly deactivates neurons during training to prevent overfitting. B is incorrect because dropout doesn't change network architecture. C is wrong because dropout is only used during training, not inference. D is incorrect because dropout actually reduces active neurons during training."
  },
  {
    "q": "Which technique allows neural networks to learn by adjusting weights using gradient information?\n- A) Dropout\n- B) Backpropagation\n- C) Batch Normalization\n- D) Activation Clipping",
    "a": "B - Correct because backpropagation is the fundamental algorithm that computes gradients and updates weights. A is incorrect because dropout is a regularization technique. C is wrong because batch normalization is for normalizing layer inputs. D is incorrect because activation clipping is for preventing exploding gradients."
  },
  {
    "q": "Which optimization method improves standard gradient descent by incorporating past gradients?\n- A) Momentum\n- B) Overfitting\n- C) Regularization\n- D) Stochastic Noise Injection",
    "a": "A - Correct because momentum uses past gradients to accelerate convergence and reduce oscillations. B is incorrect because overfitting is a problem, not an optimization method. C is wrong because regularization is for preventing overfitting. D is incorrect because noise injection is a different technique altogether."
  },
  {
    "q": "What do eligibility traces enable in reinforcement learning algorithms?\n- A) Faster and more efficient updates by assigning credit over multiple time steps.\n- B) Model-free planning.\n- C) Increased exploration using randomness.\n- D) Removal of temporal difference errors.",
    "a": "A - Correct because eligibility traces help assign credit to past states and actions, making learning more efficient. B is incorrect because eligibility traces don't enable planning. C is wrong because they're not related to exploration. D is incorrect because they don't remove TD errors."
  },
  {
    "q": "Which operator in genetic algorithms introduces diversity by changing individual genes?\n- A) Mutation\n- B) Cloning\n- C) Crossover\n- D) Selection",
    "a": "A - Correct because mutation randomly changes genes to introduce diversity. B is incorrect because cloning creates exact copies. C is wrong because crossover combines existing genes. D is incorrect because selection chooses existing solutions."
  },
  {
    "q": "What is the main principle of Particle Swarm Optimization (PSO)?\n- A) Particles adjust their position based on their own and neighbors' best positions.\n- B) Particles reproduce and mutate like in genetic algorithms.\n- C) Each particle explores randomly with no memory.\n- D) The algorithm only relies on global best solutions.",
    "a": "A - Correct because PSO particles learn from both personal and social experience. B is incorrect because PSO doesn't use reproduction or mutation. C is wrong because particles do maintain memory of best positions. D is incorrect because PSO uses both personal and global best positions."
  },
  {
    "q": "How does the epsilon-greedy algorithm balance exploration and exploitation?\n- A) By ignoring the Q-table periodically.\n- B) By alternating randomly between greedy and random actions.\n- C) By setting \\( \\epsilon = 0.5 \\) always.\n- D) By taking a random action with probability \\( \\epsilon \\), and the best-known action with probability \\( 1 - \\epsilon \\).",
    "a": "D - Correct because it precisely describes the epsilon-greedy mechanism. A is incorrect because it doesn't ignore the Q-table. B is wrong because it's not about alternating but about probability-based selection. C is incorrect because epsilon can be any value between 0 and 1."
  },
  {
    "q": "Which learning strategy is used in Q-learning?\n- A) Supervised\n- B) On-policy\n- C) Off-policy\n- D) Model-based",
    "a": "C - Correct because Q-learning is an off-policy algorithm that can learn optimal policies from any behavior policy. A is incorrect because Q-learning is not supervised learning. B is wrong because it's not on-policy. D is incorrect because Q-learning is model-free."
  },
  {
    "q": "What do convolutional layers in CNNs primarily do?\n- A) Flatten the image into a vector.\n- B) Apply non-linear transformations.\n- C) Perform dimensionality reduction via pooling.\n- D) Extract features by applying filters over local regions of input data.",
    "a": "D - Correct because convolutional layers use filters to extract local features. A is incorrect because flattening is done in fully connected layers. B is wrong because non-linear transformations are done by activation functions. C is incorrect because pooling layers handle dimensionality reduction."
  },
  {
    "q": "Which type of neural network is suitable for sequence data like time series?\n- A) Autoencoders\n- B) Feedforward Neural Networks\n- C) Recurrent Neural Networks (RNNs)\n- D) Convolutional Neural Networks (CNNs)",
    "a": "C - Correct because RNNs are designed to handle sequential data by maintaining internal state. A is incorrect because autoencoders are for dimensionality reduction. B is wrong because feedforward networks can't handle sequences. D is incorrect because CNNs are better for spatial data."
  },
  {
    "q": "How do LSTM units help in RNNs?\n- A) By acting as an activation function.\n- B) By preserving long-term dependencies through gated memory cells.\n- C) By removing all historical input information.\n- D) By randomly resetting weights during training.",
    "a": "B - Correct because LSTM's gated architecture helps maintain long-term dependencies. A is incorrect because LSTM is not an activation function. C is wrong because LSTM preserves important historical information. D is incorrect because LSTM doesn't randomly reset weights."
  },
  {
    "q": "In a zero-sum game, what is the relationship between players' payoffs?\n- A) One player's gain is exactly the other player's loss.\n- B) Players cooperate to maximize joint reward.\n- C) Payoffs are independent of each other.\n- D) Each player gains the same amount.",
    "a": "A - Correct because zero-sum games are defined by the sum of payoffs being zero. B is incorrect because zero-sum games are competitive, not cooperative. C is wrong because payoffs are directly related. D is incorrect because gains and losses must sum to zero."
  },
  {
    "q": "What does the Nash Equilibrium describe in game theory?\n- A) A scenario with maximum joint reward.\n- B) A state where no player can benefit from unilaterally changing their strategy.\n- C) A cooperative agreement between players.\n- D) A state where players continuously change their strategy.",
    "a": "B - Correct because Nash Equilibrium is defined as a state where no player can improve their outcome by changing strategy alone. A is incorrect because it describes Pareto optimality. C is wrong because it describes cooperative games. D is incorrect because it describes dynamic games."
  },
  {
    "q": "Which algorithm is inspired by the behavior of ant colonies?\n- A) Genetic Algorithms\n- B) Particle Swarm Optimization (PSO)\n- C) Simulated Annealing\n- D) Ant Colony Optimization (ACO)",
    "a": "D - Correct because ACO is directly inspired by ant colony behavior. A is incorrect because genetic algorithms are inspired by evolution. B is wrong because PSO is inspired by bird flocking. C is incorrect because simulated annealing is inspired by metallurgy."
  },
  {
    "q": "What mechanism is used in ACO to communicate between agents?\n- A) Reward signals\n- B) Genetic crossover\n- C) Velocity updates\n- D) Pheromone trails",
    "a": "D - Correct because ants communicate through pheromone trails. A is incorrect because reward signals are used in reinforcement learning. B is wrong because genetic crossover is used in genetic algorithms. C is incorrect because velocity updates are used in PSO."
  },
  {
    "q": "In backpropagation, how are weight updates computed?\n- A) By computing forward activations only.\n- B) By using random perturbations.\n- C) By maximizing neuron activations.\n- D) By propagating the gradient of the loss function backward through the layers.",
    "a": "D - Correct because backpropagation computes gradients by propagating errors backward. A is incorrect because forward pass alone doesn't compute updates. B is wrong because random perturbations are not used. C is incorrect because maximizing activations is not the goal."
  },
  {
    "q": "Which method is most likely to prevent overfitting in a deep network?\n- A) Increasing model size\n- B) Early stopping\n- C) Using higher learning rates\n- D) Adding more training epochs",
    "a": "B - Correct because early stopping prevents overfitting by stopping training when validation performance degrades. A is incorrect because larger models are more prone to overfitting. C is wrong because higher learning rates can lead to instability. D is incorrect because more epochs can lead to overfitting."
  },
  {
    "q": "Which equation is foundational in computing the value of a state in RL under a policy \\( \\pi \\)?\n- A) Bellman expectation equation\n- B) Cross-entropy\n- C) Policy gradient formula\n- D) Loss function",
    "a": "A - Correct because the Bellman expectation equation is fundamental for computing state values. B is incorrect because cross-entropy is a loss function. C is wrong because policy gradient is for policy optimization. D is incorrect because loss functions are for training, not value computation."
  },
  {
    "q": "Which strategy in MAB problems explores new actions randomly with small probability?\n- A) SARSA\n- B) Policy gradient\n- C) \\(\\epsilon\\)-greedy\n- D) Q-iteration",
    "a": "C - Correct because epsilon-greedy uses epsilon probability for random exploration. A is incorrect because SARSA is an RL algorithm. B is wrong because policy gradient is for continuous action spaces. D is incorrect because Q-iteration is for planning."
  },
  {
    "q": "What is the main goal of evolutionary algorithms?\n- A) To evolve populations toward better solutions using mechanisms inspired by natural evolution.\n- B) To perform matrix decomposition for feature extraction.\n- C) To train supervised models using labeled data.\n- D) To backpropagate error gradients.",
    "a": "A - Correct because evolutionary algorithms use natural selection principles to optimize solutions. B is incorrect because it describes dimensionality reduction. C is wrong because it describes supervised learning. D is incorrect because it describes neural network training."
  },
  {
    "q": "Which component in PSO updates a particle's position?\n- A) Reinforcement signal.\n- B) Elite selection.\n- C) Random mutation rate.\n- D) Its velocity based on personal and global best.",
    "a": "D - Correct because PSO particles update positions using velocity influenced by personal and global best positions. A is incorrect because reinforcement signals are used in RL. B is wrong because elite selection is used in genetic algorithms. C is incorrect because mutation is used in genetic algorithms."
  },
  {
    "q": "Explain the difference between on-policy and off-policy learning with an example.",
    "a": "- **On-policy learning**:\n  - Learns the value of the policy being used to generate behavior\n  - Example: SARSA updates Q-values using the actual next action taken\n  - Formula: \\(Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha[r_t + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)]\\)\n\n- **Off-policy learning**:\n  - Learns the value of a different policy than the one used for behavior\n  - Example: Q-learning updates using the maximum Q-value of next state\n  - Formula: \\(Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha[r_t + \\gamma \\max_a Q(s_{t+1},a) - Q(s_t,a_t)]\\)\n\n- Key difference: Q-learning can learn optimal policy while following any behavior policy, while SARSA learns the value of the behavior policy itself."
  },
  {
    "q": "Describe how backpropagation works in a feedforward neural network. Include the role of the chain rule.",
    "a": "- **Forward Pass**:\n  - Compute activations layer by layer\n  - For layer \\(l\\): \\(\\mathbf{a}^l = f(\\mathbf{W}^l\\mathbf{a}^{l-1} + \\mathbf{b}^l)\\)\n\n- **Backward Pass**:\n  - Compute gradients using chain rule\n  - Error at output layer: \\(\\delta^L = \\nabla_a C \\odot f'(\\mathbf{z}^L)\\)\n  - For hidden layers: \\(\\delta^l = ((\\mathbf{W}^{l+1})^\\top \\delta^{l+1}) \\odot f'(\\mathbf{z}^l)\\)\n\n- **Weight Updates**:\n  - \\(\\Delta w_{ij}^l = -\\eta \\delta_i^l a_j^{l-1}\\)\n  - \\(\\Delta b_i^l = -\\eta \\delta_i^l\\)\n\n- Chain rule enables gradient flow through multiple layers by multiplying local gradients"
  },
  {
    "q": "Derive the Bellman optimality equation for \\( Q^* \\).",
    "a": "- Start with definition of optimal Q-value:\n\\[ Q^*(s,a) = \\mathbb{E}[R_t + \\gamma \\max_{a'} Q^*(S_{t+1},a') | S_t=s, A_t=a] \\]\n\n- Expand expectation over next state and reward:\n\\[ Q^*(s,a) = \\sum_{s'} P(s'|s,a) \\sum_r P(r|s,a,s') [r + \\gamma \\max_{a'} Q^*(s',a')] \\]\n\n- Simplify using expected reward \\(R(s,a,s')\\):\n\\[ Q^*(s,a) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')] \\]\n\n- This recursive equation defines the optimal action-value function"
  },
  {
    "q": "Outline the key components of a genetic algorithm and their roles in optimization.",
    "a": "- **Population**:\n  - Set of candidate solutions (chromosomes)\n  - Each chromosome represents a potential solution\n\n- **Selection**:\n  - Choose parents based on fitness\n  - Common methods:\n    - Tournament selection\n    - Roulette wheel selection\n    - Rank-based selection\n\n- **Crossover**:\n  - Combine parent solutions\n  - Example: Single-point crossover\n  - Creates diversity in population\n\n- **Mutation**:\n  - Random changes to chromosomes\n  - Maintains genetic diversity\n  - Prevents premature convergence\n\n- **Fitness Function**:\n  - Evaluates solution quality\n  - Guides selection process\n  - Determines survival probability"
  },
  {
    "q": "Explain how LSTM networks handle the vanishing gradient problem in RNNs.",
    "a": "- **Core Components**:\n  - Forget gate: \\(f_t = \\sigma(\\mathbf{W}_f [h_{t-1}, x_t] + \\mathbf{b}_f)\\)\n  - Input gate: \\(i_t = \\sigma(\\mathbf{W}_i [h_{t-1}, x_t] + \\mathbf{b}_i)\\)\n  - Output gate: \\(o_t = \\sigma(\\mathbf{W}_o [h_{t-1}, x_t] + \\mathbf{b}_o)\\)\n\n- **Cell State Update**:\n  - \\(\\tilde{C}_t = \\tanh(\\mathbf{W}_c [h_{t-1}, x_t] + \\mathbf{b}_c)\\)\n  - \\(C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t\\)\n\n- **Hidden State**:\n  - \\(h_t = o_t \\odot \\tanh(C_t)\\)\n\n- **Gradient Flow**:\n  - Cell state provides direct path for gradients\n  - Gates control information flow\n  - Forget gate prevents gradient explosion\n  - Input gate controls new information\n  - Output gate controls information exposure"
  }
]
