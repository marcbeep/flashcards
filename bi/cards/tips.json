[
  {
    "q": "Explain the key differences between on-policy and off-policy learning in reinforcement learning. Include examples of algorithms for each type.",
    "a": "- **On-policy learning**:\n  - Learns about the policy it is currently following\n  - Uses data generated by the current policy\n  - Example: SARSA\n  - Update uses actual next action: Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]\n\n- **Off-policy learning**:\n  - Learns about a different policy than the one being followed\n  - Can learn from data generated by any policy\n  - Example: Q-learning\n  - Update uses max over next actions: Q(s,a) ← Q(s,a) + α[r + γmax_a'Q(s',a') - Q(s,a)]\n\n- **Key distinction**: On-policy evaluates/improves the same policy generating data, while off-policy can learn optimal policy while following exploratory behavior."
  },
  {
    "q": "Describe how Q-learning uses bootstrapping and explain why this is important for learning optimal policies.",
    "a": "- **Bootstrapping in Q-learning**:\n  - Updates Q-values using estimated future rewards\n  - Uses term γmax_a'Q(s',a') as estimate of future value\n  - Doesn't wait for complete episode like Monte Carlo\n\n- **Why it's important**:\n  - Enables online learning (updates at each step)\n  - More efficient than waiting for episode completion\n  - Allows learning optimal policy while following exploratory behavior\n  - Helps propagate value information quickly through state space\n\n- **Formula**: Q(s,a) ← Q(s,a) + α[r + γmax_a'Q(s',a') - Q(s,a)]\n  - r: immediate reward\n  - γmax_a'Q(s',a'): bootstrapped estimate of future value"
  },
  {
    "q": "Explain how ReLU activation functions help mitigate the vanishing gradient problem in deep neural networks.",
    "a": "- **Vanishing gradient problem**:\n  - Gradients become very small when backpropagating through many layers\n  - Occurs with sigmoid/tanh due to derivatives < 1\n  - Makes learning in deep networks difficult\n\n- **How ReLU helps**:\n  - Derivative is 1 for positive inputs, 0 for negative\n  - No gradient shrinkage for positive values\n  - Allows gradients to flow freely through layers\n  - Enables training of deeper networks\n\n- **Comparison**:\n  - Sigmoid/tanh: derivatives < 1 cause exponential decay\n  - ReLU: constant gradient of 1 prevents decay for positive inputs"
  },
  {
    "q": "Describe how dropout works to prevent overfitting in neural networks and explain its key benefits.",
    "a": "- **How dropout works**:\n  - Randomly disables neurons during training\n  - Each neuron has probability p of being dropped\n  - Forces network to learn redundant representations\n  - During inference, all neurons used but outputs scaled by p\n\n- **Key benefits**:\n  - Prevents co-adaptation of neurons\n  - Acts like training an ensemble of subnetworks\n  - Improves generalization\n  - Simple to implement and computationally efficient\n\n- **Why it works**:\n  - Makes network more robust to missing inputs\n  - Prevents over-reliance on specific neurons\n  - Encourages distributed representations"
  },
  {
    "q": "Explain the concept of eligibility traces in TD(λ) learning and how they help balance between TD(0) and Monte Carlo methods.",
    "a": "- **Eligibility traces**:\n  - Keep track of recently visited states/actions\n  - Decay over time with factor γλ\n  - Allow credit assignment to past states\n\n- **TD(λ) update**:\n  - Combines TD(0) and Monte Carlo approaches\n  - λ controls how far back credit is assigned\n  - λ = 0: only current state updated (TD(0))\n  - λ = 1: all states in episode updated (Monte Carlo)\n\n- **Benefits**:\n  - Faster learning than TD(0)\n  - Lower variance than Monte Carlo\n  - Flexible trade-off between bias and variance\n  - Efficient credit assignment across multiple steps"
  },
  {
    "q": "Describe the Bellman optimality equation and explain its importance in reinforcement learning.",
    "a": "- **Bellman optimality equation**:\n  - V*(s) = max_a E[r + γV*(s') | s,a]\n  - Q*(s,a) = E[r + γmax_a'Q*(s',a') | s,a]\n  - Expresses optimal value in terms of optimal future values\n\n- **Importance**:\n  - Foundation of many RL algorithms\n  - Guarantees existence of optimal policy\n  - Provides recursive relationship for computing optimal values\n  - Used in value iteration, policy iteration, Q-learning\n\n- **Key properties**:\n  - Unique solution for finite MDPs\n  - Can be solved iteratively\n  - Forms basis for both model-based and model-free methods"
  },
  {
    "q": "Explain how genetic algorithms work, including the roles of selection, crossover, and mutation.",
    "a": "- **Genetic Algorithm components**:\n  - Population of candidate solutions\n  - Fitness function to evaluate solutions\n  - Selection: choose parents based on fitness\n  - Crossover: combine traits from two parents\n  - Mutation: random changes to explore new solutions\n\n- **Process**:\n  1. Initialize random population\n  2. Evaluate fitness of each solution\n  3. Select parents for next generation\n  4. Create offspring through crossover\n  5. Apply mutations\n  6. Repeat until stopping criterion\n\n- **Key aspects**:\n  - Selection pressure balances exploration/exploitation\n  - Crossover combines good building blocks\n  - Mutation maintains diversity and prevents local optima"
  },
  {
    "q": "Describe how Particle Swarm Optimization (PSO) works and explain the role of each term in the velocity update equation.",
    "a": "- **PSO components**:\n  - Particles move through solution space\n  - Each particle remembers personal best\n  - Swarm remembers global best\n  - Velocity update combines three terms\n\n- **Velocity update**:\n  v_i ← wv_i + c₁r₁(p_i - x_i) + c₂r₂(g - x_i)\n  - wv_i: momentum term (inertia)\n  - c₁r₁(p_i - x_i): personal attraction\n  - c₂r₂(g - x_i): social attraction\n\n- **Role of each term**:\n  - Momentum: maintains current direction\n  - Personal: moves toward particle's best\n  - Social: moves toward swarm's best\n  - Together they balance exploration and exploitation"
  },
  {
    "q": "Explain the concept of Nash equilibrium in game theory and how it relates to zero-sum games.",
    "a": "- **Nash equilibrium**:\n  - Stable strategy profile where no player can benefit by changing strategy alone\n  - Each player's strategy is best response to others' strategies\n  - May involve mixed strategies (randomization)\n\n- **In zero-sum games**:\n  - One player's gain equals other's loss\n  - Nash equilibrium represents minimax solution\n  - Value is unique even if multiple optimal strategies exist\n  - Can be found using linear programming\n\n- **Properties**:\n  - Always exists in finite games\n  - May not be unique\n  - Represents rational behavior when opponents are rational"
  },
  {
    "q": "Describe how Convolutional Neural Networks (CNNs) extract hierarchical features from images.",
    "a": "- **CNN architecture**:\n  - Convolutional layers: apply filters to detect features\n  - Pooling layers: reduce spatial dimensions\n  - Fully connected layers: final classification\n\n- **Feature hierarchy**:\n  - Early layers: edges, colors, basic patterns\n  - Middle layers: shapes, textures, parts\n  - Deep layers: complex objects, full scenes\n\n- **Why it works**:\n  - Local connectivity: each neuron sees small region\n  - Weight sharing: same filter applied everywhere\n  - Translation invariance: features detected regardless of position\n  - Hierarchical composition: complex features built from simple ones"
  }
]
