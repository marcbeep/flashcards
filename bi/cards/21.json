[
  {
    "q": "What is a dominant strategy and dominant strategy equilibrium?",
    "a": "- A dominant strategy is a player's best response to any strategy the other players might choose.\n- A dominant strategy equilibrium occurs when all players choose their dominant strategy."
  },
  {
    "q": "Explain the Prisoners’ Dilemma and its outcomes.",
    "a": "- Two players choose to Confess (Defect) or Hold out (Cooperate).\n- Payoffs:\n  - (Confess, Confess): (1,1)\n  - (Hold out, Hold out): (3,3)\n  - Mixed actions: One gets 5, the other 0\n- Dominant strategy: Always Confess\n- Paradox: Both do worse with (1,1) than if they cooperated (3,3)"
  },
  {
    "q": "What is the opportunity for learning in the Prisoners’ Dilemma?",
    "a": "- Through repeated interaction, agents can learn to cooperate\n- Multi-agent learning can lead to Pareto optimal outcomes like (3,3)"
  },
  {
    "q": "Describe the Battle of the Sexes game and its equilibria.",
    "a": "- Coordination game with asymmetric preferences:\n  - Alice prefers Ballet (2,1)\n  - Bob prefers Football (1,2)\n- Two pure Nash equilibria:\n  - (Ballet, Ballet)\n  - (Football, Football)\n- Mixed Nash equilibrium:\n  - Alice: (2/3 Ballet, 1/3 Football)\n  - Bob: (2/3 Football, 1/3 Ballet)\n- Mixed strategy leads to possible miscoordination and lower payoff (2/3 each)"
  },
  {
    "q": "What is fictitious play in multi-agent learning?",
    "a": "- Each agent tracks the frequency of others’ past actions\n- Forms a belief (model) of the others’ strategies\n- Plays a best response to this model\n- Variants:\n  - Recency-weighted updates\n  - Softmax/smoothed responses\n  - Gradual adjustment toward best response"
  },
  {
    "q": "What happens if all agents use fictitious play?",
    "a": "- Strict Nash equilibria become stable (absorbing)\n- Often leads to cycling behavior in strategies\n- Empirical action distributions may still converge to Nash"
  },
  {
    "q": "What is a Markov or Stochastic Game?",
    "a": "- Game with states that evolve over time based on joint actions\n- Components:\n  - \\( n \\): number of agents\n  - \\( S \\): set of states\n  - \\( R \\): rewards\n  - \\( P \\): transition probabilities\n  - \\( \\gamma \\): discount factor\n- Each state is like a stage game"
  },
  {
    "q": "Why is learning important in Markov Games?",
    "a": "- Computing Nash Equilibria is hard\n- Agents may not know:\n  - Their own or others' payoffs\n  - Transition dynamics\n  - Other agents’ strategies\n- Learning helps adapt and improve over time"
  },
  {
    "q": "What learning methods are used in stochastic games?",
    "a": "- Independent Learning: ignore others\n- Joint-Action Learning: model other agents\n- Minimax-Q Learning: for zero-sum games\n- Nash-Q Learning: find Nash equilibria\n- Correlated Equilibrium (CE) Q-Learning: uses shared signals for coordination"
  },
  {
    "q": "What are zero-sum stochastic games and their properties?",
    "a": "- One player's gain is another's loss (sum of payoffs is constant)\n- Properties:\n  - All equilibria have the same value\n  - Bellman-style value equation applies\n  - Value iteration with minimax yields Nash equilibrium"
  },
  {
    "q": "Describe the spectrum of Multi-Agent Learning (MAL) approaches.",
    "a": "- Ranges from:\n  - Independent learners (simpler, less coordinated)\n  - To joint action learners (complex, high coordination)\n- Most modern methods lie in between these extremes"
  }
]
