[
  {
    "q": "What is the goal of reinforcement learning (RL)?",
    "a": "- To learn a policy that maximizes cumulative reward.\n- Solves problems modeled as Markov Decision Processes (MDPs)."
  },
  {
    "q": "What are the key components of a Markov Decision Process (MDP)?",
    "a": "- States \\( s \\)\n- Actions \\( a \\)\n- Transition probabilities\n- Rewards \\( r \\)\n- Discount factor \\( \\gamma \\)"
  },
  {
    "q": "What is the Markov property?",
    "a": "- The future depends only on the present state, not the past.\n- Formally: \\( P(s_{t+1} | s_t, a_t) = P(s_{t+1} | s_0, ..., s_t, a_t) \\)"
  },
  {
    "q": "Define the state-value function \\( V^{\\pi}(s) \\).",
    "a": "- Expected return starting from state \\( s \\), following policy \\( \\pi \\):\n\\[\nV^{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ \\sum\\limits_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_0 = s \\right]\n\\]"
  },
  {
    "q": "Define the action-value function \\( Q^{\\pi}(s, a) \\).",
    "a": "- Expected return starting from state \\( s \\), taking action \\( a \\), then following policy \\( \\pi \\):\n\\[\nQ^{\\pi}(s, a) = \\mathbb{E}_{\\pi} \\left[ \\sum\\limits_{t=0}^{\\infty} \\gamma^t r_{t+1} \\mid s_0 = s, a_0 = a \\right]\n\\]"
  },
  {
    "q": "What is the Bellman equation for \\( V^{\\pi}(s) \\)?",
    "a": "- Recursive form:\n\\[\nV^{\\pi}(s) = \\mathbb{E}_{\\pi} [r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) \\mid s_t = s]\n\\]"
  },
  {
    "q": "What is the Bellman optimality equation for \\( V^*(s) \\)?",
    "a": "- Expresses the best possible value from state \\( s \\):\n\\[\nV^*(s) = \\max_a \\mathbb{E} [r_{t+1} + \\gamma V^*(s_{t+1}) \\mid s_t = s, a_t = a]\n\\]"
  },
  {
    "q": "What is the Bellman optimality equation for \\( Q^*(s, a) \\)?",
    "a": "- Expresses the best possible value from state-action pair \\( (s, a) \\):\n\\[\nQ^*(s,a) = \\mathbb{E} [r_{t+1} + \\gamma \\max_{a'} Q^*(s_{t+1}, a') \\mid s_t = s, a_t = a]\n\\]"
  },
  {
    "q": "How do optimal policies relate to value functions?",
    "a": "- Optimal policies share the same \\( V^* \\) and \\( Q^* \\).\n- A policy \\( \\pi^* \\) is optimal if:\n  - \\( V^{\\pi^*}(s) = V^*(s) \\) for all \\( s \\)\n  - \\( Q^{\\pi^*}(s, a) = Q^*(s, a) \\) for all \\( s, a \\)"
  },
  {
    "q": "Why is \\( V^*(s) \\) useful for planning?",
    "a": "- You can act optimally by choosing:\n\\[\na^* = \\arg\\max_a \\mathbb{E}[r + \\gamma V^*(s')]\n\\]\n- Called one-step-ahead search."
  },
  {
    "q": "Why is \\( Q^*(s, a) \\) even more powerful than \\( V^*(s) \\)?",
    "a": "- You can act optimally **without any search**:\n\\[\na^* = \\arg\\max_a Q^*(s, a)\n\\]\n- Just pick the action with highest Q-value."
  },
  {
    "q": "Give a simple example of value functions in a golf game.",
    "a": "- State: Ball location\n- Actions: `putt`, `driver`\n- Reward: -1 per stroke until hole\n- \\( Q^*(s, \\text{putt}) \\) gives value for putting from \\( s \\)\n- \\( Q^*(s, \\text{driver}) \\) gives value for driving, then acting optimally"
  }
]
