---
## ğŸ” Reinforcement Learning (RL)

### Traditional ML vs RL

- In traditional machine learning (ML), learning happens from labeled data.
- In reinforcement learning, an agent learns by **interacting with an environment**, receiving **rewards** based on its actions. The goal is to learn a policy to maximize the cumulative reward over time.
---

### ğŸ° Multi-Armed Bandits

- A simplified RL problem where an agent must choose between multiple options ("arms").
- **Exploration vs Exploitation**:

  - **Exploration**: trying different actions to discover their rewards.
  - **Exploitation**: choosing the action that has given the best reward so far.

- Strategies:

  - **Greedy**: always choose the best-known action.
  - **Îµ-greedy**: usually pick the best-known action, but with probability Îµ choose a random one.
  - **Random**: choose any action with equal chance.

---

### ğŸ”¢ Value Functions

- **State-Value Function (VÏ€(s))**: Expected total reward starting from state **s**, following policy **Ï€**.
- **Action-Value Function (QÏ€(s, a))**: Expected total reward starting from state **s**, taking action **a**, and then following policy **Ï€**.

---

### ğŸ”„ Bellman Equation

- A recursive way to express value functions.
- For state value:

  $$
  V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^\pi(s')]
  $$

  - **Ï€(a|s)**: probability of taking action a in state s
  - **P(s'|s,a)**: transition probability to state sâ€² from s with action a
  - **R(s,a,sâ€²)**: reward for moving from s to sâ€² via a
  - **Î³**: discount factor (between 0 and 1)

---

### ğŸ§® Solving Bellman Equations

- **Dynamic Programming (DP)**: Requires a model of the environment. Uses iterative updates.
- **Temporal Difference (TD)**: Learns from experience without a full model. Combines ideas from DP and Monte Carlo.
- **Monte Carlo Tree Search (MCTS)**: Used especially in game-playing agents like AlphaGo. Builds a search tree using simulations.

---

### ğŸ“˜ Q-learning

- A model-free RL algorithm to learn **Q-values**.
- Update rule:

  $$
  Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
  $$

  - **Î±**: learning rate
  - **Î³**: discount factor
  - **r**: reward
  - **sâ€²**: next state

- Faces the exploration/exploitation dilemma just like the bandit problem.

---

### ğŸ¤– Deep Q-Learning

- Uses a **neural network** to approximate Q(s, a) when the state/action space is too large.
- **Loss function**:

  $$
  L = \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)^2
  $$

  - Trains the neural network to minimize this loss.

#### ğŸ§  Exam Tip:

**Q: What's the main advantage of Deep Q-Learning over traditional Q-learning?**
**A: It can handle high-dimensional input spaces.**
(Correct answer: **C**)

---

## ğŸ§  Deep Learning (DL)

### âš¡ Basic Concepts

- **Neurons**: Basic units in a neural network that apply a function to inputs.
- **Neural Network**: Layers of neurons connected with weights (edges).

---

### ğŸ—ï¸ Network Structure

- **Input Layer**: Takes the data (e.g., image pixels).
- **Hidden Layers**: Intermediate layers that learn features.
- **Output Layer**: Gives the final result (e.g., classification).

> A "deep" network has **multiple hidden layers**.

---

### ğŸ“ˆ Activation Functions

These introduce non-linearity.

- **ReLU (Rectified Linear Unit)**: $f(x) = \max(0, x)$
- **Sigmoid**: $f(x) = \frac{1}{1 + e^{-x}}$

---

### ğŸ§± Specialized Layers

- **Convolutional Layers**: Useful for images; detect patterns like edges.
- **Pooling Layers**: Reduce the size of feature maps (e.g., max pooling).

---

### ğŸ“‰ Loss Functions

Used to measure how well the model is doing.

- **MSE (Mean Squared Error)**:

  $$
  \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
  $$

  - **y_i**: true value
  - **Å·_i**: predicted value

---

### ğŸ” Backpropagation

- The process of training the network by updating weights:

  1. **Forward Propagation**: Compute output.
  2. **Compute Loss**.
  3. **Backward Propagation**: Use gradients to update weights (via **gradient descent**).

---

### ğŸ› ï¸ Optimizers

- Algorithms like **SGD (Stochastic Gradient Descent)** or **Adam** are used to adjust weights and minimize the loss.

---

## ğŸ“š Exam Practice Examples

- **Explain Q-learning**: It's a method in RL where agents learn action values (Q-values) to maximize rewards using a simple update rule.
- **List 2 activation functions**: ReLU, Sigmoid

---
