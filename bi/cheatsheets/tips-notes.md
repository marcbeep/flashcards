# RL

# On Policy vs Off Policy

- Understand the distinction between **on-policy** and **off-policy** learning. On-policy methods update the policy based on actions taken under the current policy, whereas off-policy methods learn from data generated by any policy. (Reference lecture 9)

---

## üß† **What is the distinction between on-policy and off-policy learning?**

### ‚ûï Core Idea:

- **On-policy learning**: The agent **learns about the policy it is currently following**.
- **Off-policy learning**: The agent **learns about a different policy** than the one it's currently following (usually the optimal one).

---

## üìú Definitions

### üî∑ On-Policy Learning

- The agent **follows** a policy œÄ (called the **behavior policy**), and **learns to improve that same policy**.

- The policy being **improved** is the **same** as the policy being **used to generate data**.

- üß™ Example algorithm: **SARSA**

  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
  $$

  - $s$: current state
  - $a$: action taken under current policy
  - $r$: reward received
  - $s'$: next state
  - $a'$: next action chosen using current (possibly exploratory) policy œÄ
  - $\alpha$: learning rate
  - $\gamma$: discount factor

---

### üî∂ Off-Policy Learning

- The agent **follows one policy** to collect data (the **behavior policy**) but **learns about another policy** (the **target policy**).

- Often used to **learn the optimal policy** while still exploring.

- üß™ Example algorithm: **Q-learning**

  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
  $$

  - Difference: uses the **maximum** Q-value of the next state $s'$, assuming **greedy target policy** (optimal behavior), even if the agent took a random action.

---

## üçä Simple Metaphorical Example

### üçΩÔ∏è Restaurant Example:

You're trying to learn **what restaurant gives the best food (policy)**.

#### On-Policy (SARSA-style):

- You follow your **current preferences**, which sometimes include trying random restaurants.
- You **judge** how good your experience is **based on where you actually go** next.

  > You ate at a random caf√© today because you wanted variety. You update your opinion of caf√©s based on that experience.

#### Off-Policy (Q-learning-style):

- You still explore different restaurants (random choices), but you always **update your belief about the best restaurant**, assuming next time you'll pick the best one.

  > Even if you ate at a random caf√© today, you update your belief **as if** next time you'll go to your favorite steakhouse.

---

## üìä Summary Table

| Feature               | On-Policy                      | Off-Policy                      |
| --------------------- | ------------------------------ | ------------------------------- |
| Learns about          | The **same** policy it follows | A **different** (target) policy |
| Policy used in update | The **actual** next action     | The **greedy** or target action |
| Example algorithm     | SARSA                          | Q-learning                      |
| Stability             | More stable, may be suboptimal | More optimal, can be unstable   |
| Used in               | Safer exploration tasks        | When optimality is desired      |

---

## üîë Key Takeaways

- On-policy methods (like SARSA) **track what they actually do**.
- Off-policy methods (like Q-learning) **pretend they'll act optimally next time**, even if they don't.
- The **difference lies in how the learning target is computed**: using real action (SARSA) vs. best action (Q-learning).

# Q Learning & it's Bootstrapping techniques

- Learn about **Q-learning** and its bootstrapping mechanism, which updates Q-values based on estimated future rewards without waiting for the completion of the episode. (Reference lecture 8)

---

## üß† What is Q-learning?

Q-learning is a **model-free**, **off-policy** reinforcement learning algorithm that learns the **optimal action-value function** $Q^*(s, a)$. This function estimates the **expected total reward** the agent will get by:

- Taking action $a$ in state $s$,
- Then following the **best possible actions** forever after.

---

## üîÅ What is Bootstrapping?

**Bootstrapping** means:

> Updating a value estimate using **another estimate**, instead of waiting for the actual result.

So, rather than waiting for the total future return (like Monte Carlo methods do), Q-learning **uses its current guess of the future** to update what it knows now.

---

## üìê Q-learning Update Rule

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

Where:

- $s$: current state
- $a$: action taken from state $s$
- $r$: reward received after taking action $a$
- $s'$: next state
- $a'$: any possible action in state $s'$
- $\alpha$: learning rate (how fast we learn)
- $\gamma$: discount factor (how much we care about future rewards)

---

## üîç How Bootstrapping Appears

Look at this term:

$$
r + \gamma \max_{a'} Q(s', a')
$$

This is the **bootstrapped target** ‚Äî it's **not based on full future experience**, just the **current estimate** of how good the next state is, assuming we act optimally.

> So we **update today's guess** $Q(s, a)$ based on a **guess about tomorrow** $\max Q(s', a')$.

---

## üçî Metaphorical Example ‚Äì Choosing Restaurants

Imagine you're exploring restaurants and trying to learn which one gives the best long-term value.

- You go to **Burger Shack** today.
- You plan to go to **Pizza Place** next.
- You already have a **rating** for Pizza Place based on past experience.

Now:

- Instead of waiting to finish all your meals to evaluate the whole journey,
- You **use your current rating** for Pizza Place (future state) to update how you feel about Burger Shack (current state).

That‚Äôs **bootstrapping**!

---

## ‚öôÔ∏è Key Properties of Q-learning

| Feature          | Explanation                                                                                         |
| ---------------- | --------------------------------------------------------------------------------------------------- |
| **Model-free**   | No need to know transition probabilities or rewards ahead of time                                   |
| **Off-policy**   | It learns as if you always choose the best action, even if you explore                              |
| **Bootstrapped** | Updates happen at every step, using current value estimates                                         |
| **Converges**    | With proper conditions (learning rate decay, full exploration), it converges to optimal $Q^*(s, a)$ |

---

## üìù What You Should Know (Exam Points)

- ‚úÖ **State the Q-learning update rule**
- ‚úÖ **Label all variables** and explain what they mean
- ‚úÖ **Define bootstrapping** in your own words:

  - "Using estimated future value to update current estimates."

- ‚úÖ **Explain the difference** from Monte Carlo methods:

  - Q-learning updates step-by-step (online), Monte Carlo waits for the episode to end.

- ‚úÖ **Know that Q-learning is off-policy** and updates towards the greedy policy even if your behavior includes random actions (like in $\varepsilon$-greedy).

---

# Eligibility Traces and TD

- **Eligibility traces** and **Temporal Difference (TD) learning** are techniques used in RL to efficiently update values based on partial information. (Reference lecture 10)

---

## üß† What is Temporal Difference (TD) Learning?

**TD Learning** is a **prediction-based** method in reinforcement learning that combines:

- **Monte Carlo methods** (learning from real experience)
- **Dynamic Programming** (bootstrapping from estimates)

### üîÅ Key Idea:

> TD updates **value estimates** based on **incomplete episodes**, using the **estimated value of the next state**.

---

### ‚úÖ TD(0) Update Rule (the simplest case)

$$
V(s_t) \leftarrow V(s_t) + \alpha \left[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right]
$$

Where:

- $V(s_t)$: current estimate of value of state $s_t$
- $r_{t+1}$: reward received after moving to the next state
- $\gamma$: discount factor
- $\alpha$: learning rate
- $V(s_{t+1})$: estimate of the next state‚Äôs value

### üîç This is **bootstrapping**: using **existing estimates** (like $V(s_{t+1})$) to improve current ones.

---

## üß† What are Eligibility Traces?

**Eligibility traces** are like **‚Äúmemory trails‚Äù** that keep track of **how recently and how often** each state (or action) was visited.

> They help **assign credit** to all recent steps, not just the last one ‚Äî like giving credit to everyone who helped in a relay race, not just the last runner.

---

### ‚úÖ Eligibility Trace Equation

Each state (or state-action pair) has a trace value $e(s)$, updated as:

$$
e_t(s) =
\begin{cases}
1 & \text{if } s = s_t \\
\gamma \lambda e_{t-1}(s) & \text{otherwise}
\end{cases}
$$

Where:

- $\lambda \in [0, 1]$: **trace-decay parameter**
- $\gamma$: discount factor
- Recent states get higher traces; older ones fade.

---

## üîÅ Combining TD + Eligibility Traces ‚Üí **TD(Œª)**

Now, the value update is:

$$
V(s) \leftarrow V(s) + \alpha \cdot \delta_t \cdot e_t(s)
$$

Where:

- $\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$: **TD error**
- You apply this TD error **across all states**, weighted by their trace $e_t(s)$

---

## üìè What is $\lambda$? Why is it important?

- $\lambda$ controls how **far back** credit is given:

  - $\lambda = 0$ ‚Üí only current state is updated (like TD(0))
  - $\lambda = 1$ ‚Üí all states in the episode get updated (like Monte Carlo)

So:

> $\lambda$ gives you a **spectrum** between TD(0) and Monte Carlo.

---

## üßÉ Simple Metaphor ‚Äî Lemonade Stand

You‚Äôre managing a **lemonade stand** with 3 workers:

- Worker A: squeezed lemons
- Worker B: mixed the drink
- Worker C: handed it to the customer

You get a **good review**.

- TD(0): you only thank Worker C (the last step).
- TD(1): you thank **all** workers equally.
- TD(Œª): you **thank recent workers more**, and earlier ones less ‚Äî depending on $\lambda$.

This is what **eligibility traces** do: they **decay the credit** as you go further back in time.

---

## üîë Key Takeaways (What You Should Know)

| Concept               | What to Remember                                                              |
| --------------------- | ----------------------------------------------------------------------------- |
| **TD(0)**             | Updates from current reward + estimated next value                            |
| **Eligibility trace** | Decaying memory of state visits                                               |
| **TD(Œª)**             | Combines TD(0) and Monte Carlo via a parameter $\lambda$                      |
| **Trace update**      | Recent states get higher credit; older ones fade via $\lambda$                |
| **Use case**          | Speeds up learning by spreading credit more efficiently across multiple steps |

---

## ‚úÖ Summary Formula Set

- **TD error**:

  $$
  \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
  $$

- **Eligibility trace update**:

  $$
  e_t(s) =
  \begin{cases}
  1 & \text{if } s = s_t \\
  \gamma \lambda e_{t-1}(s) & \text{otherwise}
  \end{cases}
  $$

- **TD(Œª) update**:

  $$
  V(s) \leftarrow V(s) + \alpha \cdot \delta_t \cdot e_t(s)
  $$

---

# Bellman Equation

- The **Bellman equation** is central in reinforcement learning. Understand both the optimality equation and how the equation changes under a given policy. (Reference lecture 7)

---

## üß† What is the Bellman Equation?

The **Bellman equation** expresses the value of a state as the **expected return** from that state ‚Äî in other words:

> "The value of a state equals the immediate reward plus the discounted value of the next state."

There are **two main versions**:

1. Under a **specific policy** ‚Üí **Bellman expectation equation**
2. For the **best possible policy** ‚Üí **Bellman optimality equation**

---

## 1Ô∏è‚É£ Bellman **Expectation** Equation (under a given policy \\( \pi \\))

This tells us the **value of a state** when following a particular policy.

### üî∑ State-Value Function

$$
V^{\pi}(s) = \mathbb{E}_{\pi} \left[ r_{t+1} + \gamma V^{\pi}(s_{t+1}) \mid s_t = s \right]
$$

Where:

- $V^{\pi}(s)$: value of state $s$ under policy $\pi$
- $r_{t+1}$: reward after taking action
- $s_{t+1}$: next state
- $\gamma \in [0,1]$: discount factor
- $\mathbb{E}_{\pi}$: expectation when actions are chosen using policy $\pi$

#### üß† Intuition:

> Value now = expected **reward now** + **future value**, if we follow policy $\pi$

---

### üî∑ Action-Value Function

$$
Q^{\pi}(s,a) = \mathbb{E}_{\pi} \left[ r_{t+1} + \gamma Q^{\pi}(s_{t+1}, a_{t+1}) \mid s_t = s, a_t = a \right]
$$

Here, you're evaluating **state-action pairs**, not just states.

---

## 2Ô∏è‚É£ Bellman **Optimality** Equation (for the best policy \\( \pi^\* \\))

This version assumes you act **optimally** at each step ‚Äî it helps **find** the best policy.

### üî∂ Optimal State-Value Function

$$
V^*(s) = \max_a \mathbb{E} \left[ r_{t+1} + \gamma V^*(s_{t+1}) \mid s_t = s, a_t = a \right]
$$

- $V^*(s)$: best possible value of state $s$
- The **max** chooses the action that gives the **highest expected return**

---

### üî∂ Optimal Action-Value Function

$$
Q^*(s,a) = \mathbb{E} \left[ r_{t+1} + \gamma \max_{a'} Q^*(s_{t+1}, a') \mid s_t = s, a_t = a \right]
$$

- $Q^*(s,a)$: best possible total reward if you start in state $s$, take action $a$, then act optimally
- The **inner max** finds the best next action in state $s_{t+1}$

---

## üìè Comparison Table

| Equation Type       | Formula (state value)              | Policy Type                           |                                          |             |
| ------------------- | ---------------------------------- | ------------------------------------- | ---------------------------------------- | ----------- |
| Bellman Expectation | ( V^{\pi}(s) = \sum_a \pi(a        | s) \sum\_{s'} P(s'                    | s,a) \[R(s,a,s') + \gamma V^{\pi}(s')] ) | Given $\pi$ |
| Bellman Optimality  | ( V^\*(s) = \max_a \sum\_{s'} P(s' | s,a) \[R(s,a,s') + \gamma V^\*(s')] ) | Optimal policy                           |             |

---

## üçî Simple Metaphor ‚Äî Planning a Meal

Imagine you're planning dinner each day.

- The **value** of today‚Äôs plan depends on:

  - The food you eat tonight üçù
  - How satisfied you'll be tomorrow if your plan continues üçï ‚Üí üçú ‚Üí üç©

### üéØ Bellman Expectation Equation:

> ‚ÄúAssuming I **keep eating the same type of food**, how good will this be long-term?‚Äù

### ü•á Bellman Optimality Equation:

> ‚ÄúIf I **make the best food choice at every meal**, what‚Äôs the best long-term eating strategy?‚Äù

---

## üßÆ Numerical Example (Tiny Gridworld)

Imagine a 2x2 grid world. You‚Äôre in state $s$, and you can move **right** or **down**.

- Right gives reward +1, leads to state $s'$
- Down gives reward +0.5, leads to a different state

### Bellman Optimality for state $s$:

$$
V^*(s) = \max \left( 1 + \gamma V^*(s_{\text{right}}),\; 0.5 + \gamma V^*(s_{\text{down}}) \right)
$$

The **max** picks whichever direction gives the higher total value ‚Äî you‚Äôre planning to **act optimally** at each step.

---

## ‚úÖ What You Should Know (Exam Points)

| Topic                            | What to Remember                                                                                        |
| -------------------------------- | ------------------------------------------------------------------------------------------------------- |
| **Bellman expectation equation** | Used to evaluate a **given policy**                                                                     |
| **Bellman optimality equation**  | Used to **find the best policy**                                                                        |
| **Bootstrap**                    | Both equations involve **recursive self-reference**: value of state = reward + discounted value of next |
| **Value functions**              | $V(s)$: state value, $Q(s,a)$: action value                                                             |
| **Why it matters**               | Forms the **core of most RL algorithms**: Q-learning, value iteration, policy iteration, etc.           |

---

# Deep Learning and Neural Networks

# Relu, sigmoid, hyperbolic tangent

- Be familiar with **activation functions** such as **ReLU**, **sigmoid**, and **hyperbolic tangent**. Focus on how **ReLU** helps mitigate the **vanishing gradient problem** by allowing for non-zero gradients in deep networks. (Reference lecture 12)

---

## üß† What are Activation Functions?

In a neural network, each neuron receives inputs, computes a **weighted sum**, and passes it through an **activation function** to produce its output.

> The activation function decides **whether a neuron ‚Äúfires‚Äù** and how strongly.

It adds **non-linearity**, allowing the network to learn complex patterns.

---

## üîß Common Activation Functions

### 1. **Sigmoid** (a.k.a. logistic function)

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

- Output range: $(0, 1)$
- Looks like an S-curve
- Smooth and differentiable

‚úÖ Good for: probabilities
‚ùå Problem: **vanishing gradient** for large $|x|$ (explained below)

---

### 2. **Hyperbolic Tangent (tanh)**

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

- Output range: $(-1, 1)$
- Also S-shaped but centered at 0

‚úÖ Stronger signal than sigmoid
‚ùå Still suffers from **vanishing gradients** at large $|x|$

---

### 3. **ReLU (Rectified Linear Unit)**

$$
\text{ReLU}(x) = \max(0, x)
$$

- Output:

  - 0 if $x \leq 0$
  - $x$ if $x > 0$

‚úÖ Simple and fast
‚úÖ **Does not saturate** for positive values
‚úÖ Helps fix **vanishing gradient** problem
‚ùå Can ‚Äúdie‚Äù if neurons output 0 permanently (Dead ReLU)

---

## üö® What is the Vanishing Gradient Problem?

In deep networks with many layers:

- Gradients (derivatives) are **chained via backpropagation**
- Sigmoid and tanh have **derivatives < 1**, so:

  $$
  \text{Gradient} = \frac{dy}{dx} \cdot \frac{dx}{dz} \cdot \frac{dz}{dw} \cdot \dots \to 0
  $$

- Result: **early layers receive tiny updates**
- Network learns **very slowly** or **not at all**

---

## üî• How ReLU Helps

### ‚úÖ ReLU's derivative:

$$
\frac{d}{dx} \text{ReLU}(x) =
\begin{cases}
0 & \text{if } x \leq 0 \\
1 & \text{if } x > 0
\end{cases}
$$

- For $x > 0$, gradient is **1**
- No shrinking ‚Üí gradients stay strong through layers
- Enables **deep networks to learn** effectively

---

## üßÉ Metaphor ‚Äì Leaky Pipes

- Think of gradients like **water flowing through pipes** (layers).
- **Sigmoid/tanh** = old narrow pipes ‚Äî water **slows down and fades** after many layers.
- **ReLU** = wide open valves (for $x > 0$) ‚Äî water **flows freely**, no decay.

> That‚Äôs why deep networks **prefer ReLU** ‚Äî signals don‚Äôt vanish.

---

## üìä Summary Comparison

| Function | Output Range  | Derivative           | Pros                   | Cons                   |               |                     |
| -------- | ------------- | -------------------- | ---------------------- | ---------------------- | ------------- | ------------------- |
| Sigmoid  | $(0,1)$       | Small for big (      | x                      | )                      | Probabilities | Vanishing gradients |
| Tanh     | $(-1,1)$      | Still small at edges | Centered output        | Still vanishes         |               |                     |
| ReLU     | $[0, \infty)$ | 1 for $x > 0$        | Fast, avoids vanishing | Can die for $x \leq 0$ |               |                     |

---

## ‚úÖ What You Should Know (Exam Points)

| Topic                        | What to Remember                                                            |
| ---------------------------- | --------------------------------------------------------------------------- |
| **Activation function role** | Adds non-linearity; allows network to learn complex patterns                |
| **Sigmoid/tanh issue**       | Gradients shrink across layers ‚Üí vanishing gradients                        |
| **ReLU fix**                 | Derivative = 1 for positive values ‚Üí gradients stay strong                  |
| **ReLU advantage**           | Enables **deep learning** by preserving gradient strength                   |
| **Tradeoffs**                | ReLU can die (always output 0); variations like **Leaky ReLU** address this |

---

## Bonus: Code Snippets

```python
import torch.nn as nn

sigmoid = nn.Sigmoid()
tanh = nn.Tanh()
relu = nn.ReLU()
```

---

Great question! Let‚Äôs look at **Leaky ReLU** ‚Äî a simple yet clever fix for one of ReLU's biggest problems.

---

## üí° What is Leaky ReLU?

**Leaky ReLU** is a variant of ReLU that **allows a small, non-zero gradient** when the input is negative.

### üßæ Formula:

$$
\text{LeakyReLU}(x) =
\begin{cases}
x & \text{if } x \geq 0 \\\\
\alpha x & \text{if } x < 0
\end{cases}
$$

Where:

- $x$ is the input to the neuron
- $\alpha$ is a small positive constant (e.g. $\alpha = 0.01$)

---

## üö® Why Leaky ReLU?

### Problem with ReLU:

- When $x < 0$, **ReLU outputs 0**, and its **gradient is 0**
- If too many neurons output 0, they **stop learning** ‚Üí called **‚Äúdying ReLU‚Äù**

### Leaky ReLU fix:

- Instead of zero, output a **small negative slope** for $x < 0$
- This keeps the neuron **‚Äúalive‚Äù**, even if its input is negative

---

## üîç Derivative of Leaky ReLU

$$
\frac{d}{dx} \text{LeakyReLU}(x) =
\begin{cases}
1 & \text{if } x \geq 0 \\\\
\alpha & \text{if } x < 0
\end{cases}
$$

So, unlike standard ReLU, the gradient **never becomes zero**, which helps the network keep updating the weights.

---

## üßÉ Metaphor ‚Äì Leaky Faucet

- **ReLU**: If the handle is turned left (negative), the faucet shuts completely ‚Äî no water (no gradient).
- **Leaky ReLU**: Even if the handle is turned left, it **drips slowly** ‚Äî still a little water (gradient).

This small leak helps prevent neurons from drying up (i.e., **‚Äúdying‚Äù**).

---

## üî¨ Variants

- **Parametric ReLU (PReLU)**: Learns the slope $\alpha$ during training instead of setting it manually.
- **Randomized Leaky ReLU (RReLU)**: Randomly samples $\alpha$ from a range during training.

---

## ‚úÖ Summary (What You Should Know)

| Feature            | Leaky ReLU                                                            |
| ------------------ | --------------------------------------------------------------------- |
| Output for $x < 0$ | $\alpha x$ instead of 0                                               |
| Slope $\alpha$     | Typically 0.01 (but tunable)                                          |
| Gradient           | Never exactly zero                                                    |
| Benefit            | Avoids ‚Äúdying ReLU‚Äù ‚Äî neurons keep learning even with negative inputs |
| Trade-off          | Adds a small bias for negative values (less sparse activation)        |

---

## üîß Code Example (PyTorch)

```python
import torch.nn as nn

# Default alpha = 0.01
leaky_relu = nn.LeakyReLU(negative_slope=0.01)

output = leaky_relu(torch.tensor([-2.0, 0.0, 2.0]))
```

---

# Dropout and Early Stopping

- **Overfitting** in neural networks can be addressed with techniques like **dropout** and **early stopping**, which prevent the model from becoming too complex and overfitting to the training data. (Reference lecture 16)

---

## üìâ What is Overfitting?

> Overfitting happens when a neural network **memorizes the training data** too well, including **noise or outliers**, instead of learning general patterns.

As a result:

- It performs **very well on training data** ‚úÖ
- But **poorly on new, unseen data** ‚ùå

---

## üéì Simple Example

Imagine you're learning math for a test:

- You **memorize every answer** from last year‚Äôs exam ‚Üí ‚úÖ perfect training performance
- But the actual test has **new questions** ‚Üí ‚ùå poor generalisation

That‚Äôs overfitting: the model is too ‚Äúsmart‚Äù on the training set but doesn‚Äôt truly **understand** the patterns.

---

## üìä In Neural Networks

Overfitting happens especially when:

- The model is **too large** (too many layers or neurons)
- Training goes on for **too many epochs**
- There‚Äôs **not enough data** to learn general patterns

---

## üõ† Techniques to Prevent Overfitting

### 1Ô∏è‚É£ **Dropout**

> Dropout randomly "turns off" neurons during training.

### üí° How it works:

- In each training step, a random subset of neurons is **ignored (dropped out)**.
- Their outputs are set to 0.
- This forces the network to **learn redundant, distributed representations**, making it more **robust**.

### üßÆ Formula (conceptual):

If neuron $i$ is dropped:

$$
\text{output}_i = 0
$$

If not dropped:

$$
\text{output}_i = \text{standard output}
$$

During **inference**, all neurons are used, but their outputs are **scaled down** (typically multiplied by dropout probability) to match training behavior.

### ‚úÖ Benefits:

- Acts like training many **thinned subnetworks**
- Reduces **co-adaptation**: neurons don‚Äôt rely on each other too much
- Leads to better **generalisation**

---

### 2Ô∏è‚É£ **Early Stopping**

> Early stopping monitors model performance on **validation data** and **stops training** when performance starts to worsen.

### üîÑ Why it works:

- During training, validation loss often **decreases at first**, then starts increasing as the model **overfits**.
- Early stopping **halts training** at the point where the model is still generalising well.

### ü™õ How it's done:

- Keep a copy of the model with **lowest validation loss** seen so far.
- Stop if validation loss doesn‚Äôt improve for **N consecutive epochs** (called "patience").

---

## üßÉ Metaphor ‚Äì Cooking Pasta üçù

- **Overcooked pasta** is like an overfit model ‚Äî too soft, not good!
- **Dropout** is like stirring regularly so noodles don‚Äôt stick ‚Äî prevents local over-reliance.
- **Early stopping** is like taking the pasta off heat when it‚Äôs **just right** ‚Äî not too early, not too late.

---

## ‚úÖ What You Should Know (Exam Points)

| Concept                     | Explanation                                                           |
| --------------------------- | --------------------------------------------------------------------- |
| **Overfitting**             | Model fits training data too closely, fails to generalise             |
| **Dropout**                 | Randomly disables neurons during training to encourage robustness     |
| **Early stopping**          | Monitors validation loss and halts training before overfitting begins |
| **Symptoms of overfitting** | Training loss ‚Üì but validation loss ‚Üë                                 |
| **Prevention goals**        | Improve performance on unseen/test data (generalisation)              |

---

## üîß Code Example (PyTorch)

```python
import torch.nn as nn

# Dropout layer
dropout = nn.Dropout(p=0.5)  # 50% chance to drop

# Early stopping is typically implemented via a training loop
```

---

# Backpropogation

- **Backpropagation** is the fundamental algorithm for training neural networks. Be prepared to explain the flow of error gradients backward through the network to update weights. (Reference lecture 14)
  Absolutely! Let‚Äôs explain **backpropagation** ‚Äî the core algorithm that allows neural networks to **learn from data**. We‚Äôll walk through what it is, how it works, and how gradients flow backward through layers to update weights ‚Äî with reference to **Lecture 14**.

---

## üß† What is Backpropagation?

> **Backpropagation** is the algorithm that calculates how much each **weight** in a neural network **contributes to the total error**, and uses that to update the weights.

It‚Äôs the **engine of learning** in deep neural networks ‚Äî enabling them to **reduce the loss** step by step.

---

## ‚öôÔ∏è What Happens in a Neural Network?

### Two main phases during training:

1. **Forward Pass**

   - Inputs move through the network
   - Neurons compute outputs layer by layer
   - The final output is compared to the target ‚Üí gives us **error**

2. **Backward Pass (Backpropagation)**

   - The error is **propagated backwards** through the network
   - Each weight is updated **based on its contribution** to the error

---

## üìê Core Formula (Gradient Descent with Backprop)

$$
w_{ij}^{(l)} \leftarrow w_{ij}^{(l)} - \eta \cdot \frac{\partial E}{\partial w_{ij}^{(l)}}
$$

Where:

- $w_{ij}^{(l)}$: weight from neuron $j$ in layer $l-1$ to neuron $i$ in layer $l$
- $\eta$: learning rate
- $E$: total error (e.g. Mean Squared Error)
- $\frac{\partial E}{\partial w_{ij}^{(l)}}$: how much changing this weight changes the error (i.e. **gradient**)

---

## üîÅ How Do We Compute Gradients?

We use the **Chain Rule** of calculus to **backtrack** how the error depends on each weight.

For each neuron:

1. Compute the **error** at that neuron.
2. Compute how that error depends on its **input weights**.
3. Multiply those to get the **gradient**.

---

## üßÆ Backpropagation for One Neuron (from Lecture 14)

Let‚Äôs say:

- Output: $o = f(q)$, where $q = \sum_j w_j x_j$ is the input sum
- Error function:

  $$
  E = \frac{1}{2}(o - d)^2
  $$

We want:

$$
\frac{\partial E}{\partial w_j}
$$

Using chain rule:

$$
\frac{\partial E}{\partial w_j} = (o - d) \cdot o(1 - o) \cdot x_j
$$

Then the weight update is:

$$
w_j \leftarrow w_j - \eta \cdot \frac{\partial E}{\partial w_j}
$$

---

## üîô How Error Flows Back

Each layer uses the gradient from the layer **after it** to compute its own.

- Output layer uses the **difference** between prediction and target
- Hidden layers use gradients from the **next layer** weighted by the connections

This is like:

> ‚ÄúHow much error did I cause, and how can I adjust to reduce it?‚Äù

---

## üçù Metaphor ‚Äì Fixing a Spaghetti Recipe

You serve spaghetti and get feedback: ‚ÄúToo salty!‚Äù

- **Output layer (final dish)**: gets the complaint
- **Hidden layers (ingredients/process)**: backtrace where too much salt came from

  - Too much in sauce? Or in noodles?
  - Each part adjusts based on its **contribution to the final error**

Backpropagation is this feedback system ‚Äî it tells each layer what to fix!

---

## ‚úÖ What You Should Know (Exam Points)

| Concept              | Explanation                                                          |
| -------------------- | -------------------------------------------------------------------- |
| **Backpropagation**  | Computes gradients of the error w\.r.t. each weight using chain rule |
| **Forward pass**     | Computes output and loss                                             |
| **Backward pass**    | Propagates error backwards to compute gradients                      |
| **Gradient formula** | Based on error, activation function derivative, and input            |
| **Used for**         | Training all types of neural nets (MLPs, CNNs, RNNs, etc.)           |
| **Key component**    | Enables gradient descent optimisation                                |

---

## üîß Code Insight (PyTorch)

Backprop is handled automatically, but this is what happens under the hood:

```python
loss.backward()  # Computes gradients
optimizer.step() # Updates weights using gradients
```

---

# Gradient Descent

- Understand **gradient descent** and its variants like **momentum** for optimization in neural networks. Learn how the **learning rate** controls the size of weight updates during training. (Reference lecture 16)

---

## üß† What is Gradient Descent?

> **Gradient Descent** is an algorithm used to **minimize the loss** of a model by **updating the weights in the direction that reduces error**.

It‚Äôs the core optimizer for training neural networks.

---

## üîß Basic Gradient Descent Formula

$$
w \leftarrow w - \eta \cdot \frac{\partial E}{\partial w}
$$

Where:

- $w$: weight (parameter being learned)
- $E$: error or loss
- $\frac{\partial E}{\partial w}$: gradient of the loss with respect to the weight
- $\eta$: **learning rate** ‚Äî how big the step is

---

## üßÉ Intuition ‚Äî Hiking Down a Mountain

Imagine you're on a foggy hill and trying to reach the bottom:

- You **look at the slope** under your feet (gradient)
- You **step downhill** (subtract the gradient)
- The size of your step = **learning rate**

Repeat this until you reach a valley (minimum loss)!

---

## üéØ The Learning Rate $\eta$

> Controls **how big the weight updates are**.

### üîç Effects:

- ‚úÖ Small $\eta$: safer, more precise ‚Üí but **slower**
- ‚ùå Large $\eta$: faster ‚Üí but can **overshoot** or **diverge**

You want it **just right**, or use **adaptive strategies**.

---

## üîÅ Variants of Gradient Descent

### 1Ô∏è‚É£ **Batch Gradient Descent**

- Uses **all training data** to compute gradient
- Very accurate but **slow** and **memory-intensive**

### 2Ô∏è‚É£ **Stochastic Gradient Descent (SGD)**

- Uses **one data point** at a time
- Faster, noisier, more frequent updates
- Helps escape shallow minima

### 3Ô∏è‚É£ **Mini-Batch Gradient Descent**

- Uses a **small batch** of data (e.g. 32 samples)
- Best of both worlds ‚Äî fast + stable

---

## üí® Momentum

> Momentum adds a memory of the **past gradients** to **smooth out** learning.

### ‚úÖ Formula:

$$
v_t = \beta v_{t-1} + (1 - \beta) \cdot \nabla E(w) \\
w \leftarrow w - \eta \cdot v_t
$$

Where:

- $v_t$: velocity (moving average of gradients)
- $\beta$: momentum coefficient (e.g. 0.9)
- $\nabla E(w)$: current gradient

### üß† Why it's useful:

- **Speeds up** learning in directions with consistent gradients
- **Dampens oscillations** in noisy directions
- Helps navigate **ravines or valleys** in the loss surface

---

## üßÉ Metaphor ‚Äì Rolling Ball on Terrain

- **Basic gradient descent**: push the ball based on the slope **right now**
- **With momentum**: the ball picks up speed in good directions and **doesn‚Äôt stop at every bump**

---

## üß† Summary Table

| Variant    | Key Idea            | Pros            | Cons                             |
| ---------- | ------------------- | --------------- | -------------------------------- |
| Basic GD   | Follow the gradient | Simple          | Slow, sensitive to learning rate |
| SGD        | One sample per step | Fast updates    | Noisy                            |
| Mini-batch | Small groups        | Balanced        | Needs tuning batch size          |
| Momentum   | Adds velocity       | Smooths updates | Adds parameter $\beta$           |

---

## ‚úÖ What You Should Know (Exam Points)

| Concept                  | What to Remember                                            |
| ------------------------ | ----------------------------------------------------------- |
| **Gradient descent**     | Core weight update rule for minimizing loss                 |
| **Learning rate $\eta$** | Controls step size ‚Äî too high = unstable; too low = slow    |
| **Momentum**             | Adds previous gradient history to accelerate learning       |
| **Variants**             | SGD, mini-batch, momentum all modify how gradients are used |
| **Optimization goal**    | Minimize the loss function efficiently and reliably         |

---

## üîß PyTorch Example (with Momentum)

```python
import torch.optim as optim

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
```

---

# Evolutionary Algorithms and Swarm Intelligence

- Study **genetic algorithms** and **evolutionary strategies**. Understand how **mutation** and **crossover** operators work to explore new areas of the solution space and improve the population over generations. (Reference lecture 24)

---

## üß¨ What Are Genetic Algorithms and Evolutionary Strategies?

Both are part of **Evolutionary Computation**, a set of methods inspired by **biological evolution** ‚Äî the way animals evolve over generations by natural selection.

Think of it like this:

> You're trying to find the best design for a paper airplane. Instead of manually testing every idea, you create a "population" of different designs, test them, keep the best ones, and combine them to make better ones. Over time, the airplanes ‚Äúevolve‚Äù to fly farther.

---

## üîÑ Basic Idea

1. **Population**: A group of candidate solutions (like different paper airplane designs).
2. **Fitness**: A score that tells how good a solution is (e.g., how far the airplane flies).
3. **Selection**: Better solutions are more likely to pass their traits to the next generation.
4. **Crossover (Recombination)**: Combines traits from two parent solutions to make new ones (like children getting traits from both parents).
5. **Mutation**: Introduces small random changes (like a DNA mutation), helping to explore new ideas.

Over many generations, the population gets better at solving the problem.

---

## üß™ Genetic Algorithms (GAs)

**Genetic Algorithms** use **selection**, **crossover**, and **mutation** to evolve solutions. Let‚Äôs explain each step with an example.

### üß¨ Step-by-Step Example:

You want to find the best 5-digit binary string (like `10101`) that gives the highest score (fitness).

### 1. **Initial Population**:

Randomly create 4 binary strings:

- `10101`
- `00111`
- `11000`
- `00010`

### 2. **Evaluate Fitness**:

Let‚Äôs say the score = number of 1s:

- `10101` ‚Üí 3
- `00111` ‚Üí 3
- `11000` ‚Üí 2
- `00010` ‚Üí 1

### 3. **Selection**:

Pick the best ones to become parents (more 1s = better fitness). So `10101`, `00111`, `11000` might be selected.

### 4. **Crossover**:

Mix parents to make children.

- Parent A: `10101`
- Parent B: `00111`

Pick a random crossover point (say after 2 digits):

- A ‚Üí `10 | 101`
- B ‚Üí `00 | 111`

Child ‚Üí `10` + `111` = `10111`

This is like mixing DNA.

### 5. **Mutation**:

Randomly flip one bit in a child:

- Before: `10111`
- After mutation: `11111` (3rd bit flipped from 0 to 1)

Mutation helps explore solutions that crossover alone might miss.

Repeat this process ‚Üí population gets better over time.

---

## ‚öôÔ∏è Evolutionary Strategies (ES)

**Similar to GAs**, but focus more on **mutation and selection** rather than crossover. Also, solutions are often **real-valued numbers** (not binary strings).

### Simple Example:

You're trying to optimize a number that gets the **highest square value** (e.g., maximize $x^2$).

Start with a population:

- `x = 1.2`, `-0.5`, `2.1`, `3.0`

Fitness = square:

- $1.44$, $0.25$, $4.41$, $9.00$

Keep the best ‚Üí say `x = 3.0`

Then **mutate** it slightly to explore nearby values:

- Try `x = 2.9`, `3.1`, `2.8`, etc.

Pick the one with the highest square ‚Üí repeat.

**Mutation is the main driver** in ES. There's less focus on combining solutions, and more on **tuning** a good one by trying variations.

---

## üîß Mutation and Crossover: Intuitive Summary

- **Crossover** is like mixing genes: You breed two good solutions to create a potentially better one.

  - Like combining features of two successful products.

- **Mutation** is like random tweaks: You try small changes to escape local optima.

  - Like trying a new feature by accident that turns out to be great.

---

## üß† Why They Work (Based on Lecture 24):

- **Selection** mimics survival of the fittest.
- **Mutation** introduces diversity and avoids getting stuck in bad solutions.
- **Crossover** mixes successful traits.
- Over time, **the population evolves** toward better solutions ‚Äî just like in nature.

---

# ACO and PSO

Great! Let‚Äôs break down **Swarm Intelligence** and the two main algorithms you need to understand: **Ant Colony Optimization (ACO)** and **Particle Swarm Optimization (PSO)** ‚Äî using clear and simple examples from your lectures (26, 27, 28).

---

## üêú What Is Swarm Intelligence?

Swarm Intelligence is about **how groups of simple agents** (like ants or birds) working together can solve complex problems ‚Äî **without any leader**.

Imagine a swarm of ants finding the shortest path to food, or birds flying in formation. Each agent follows **simple local rules**, but the group shows **intelligent global behavior**.

---

## üß≠ 1. **Ant Colony Optimization (ACO)**

Inspired by how **ants find the shortest path** to food using **pheromone trails**.

### üêú Real Ant Behavior

- Ants wander randomly until they find food.
- They return to the nest, leaving **pheromones** on the path.
- Other ants follow **stronger trails**.
- **Shorter paths** get reinforced faster (more trips = more pheromone).
- Over time, the colony converges on the **shortest route**.

### üí° How ACO Works in Optimization

You can imagine solving the **Traveling Salesman Problem (TSP)** ‚Äî find the shortest path visiting all cities once.

### Steps:

1. **Artificial Ants** explore different paths (like different city orders).
2. **Pheromone Update** happens in two ways:

   - **Evaporation**: Old pheromones fade ‚Üí avoids getting stuck on bad paths.
   - **Deposition**: Good paths (shorter ones) get more pheromone.

### üîÑ Path Choice Rule:

Ants choose next cities based on:

- **Pheromone strength** $\tau_{ij}$ ‚Üí how attractive the path is.
- **Heuristic info** $\eta_{ij}$ ‚Üí e.g., inverse of distance.
- **Tuned by exponents** $\alpha$ and $\beta$

**Formula (probability of choosing path from i to j):**

$$
P_{ij}(t) = \frac{[\tau_{ij}(t)]^\alpha \cdot [\eta_{ij}]^\beta}{\sum_{k \in J} [\tau_{ik}(t)]^\alpha \cdot [\eta_{ik}]^\beta}
$$

This balances **following pheromones** (exploitation) and **trying new routes** (exploration).

---

## üê¶ 2. **Particle Swarm Optimization (PSO)**

Inspired by how birds (or fish) **move in groups**, adjusting their paths based on their own experience and others‚Äô.

### üê¶ Real Bird Flocking Behavior

- Birds align with neighbors (cohesion).
- Avoid collisions (separation).
- Move toward group‚Äôs average direction (alignment).

### üí° How PSO Works in Optimization

Each **particle** is a potential solution (like a bird searching for food in a field).

### Each particle remembers:

- **Personal best position** $p_{\text{best}}$
- **Global best position** $g_{\text{best}}$

### üîÑ Velocity Update Rule:

$$
v_i^{t+1} = w v_i^t + \phi_1 U_1 (p_i - x_i^t) + \phi_2 U_2 (g - x_i^t)
$$

Where:

- $v_i$: current velocity
- $x_i$: current position
- $p_i$: personal best
- $g$: global best
- $U_1, U_2$: random values between 0 and 1
- $w$: inertia (momentum)
- $\phi_1, \phi_2$: strength of personal and social pull

Then update position:

$$
x_i^{t+1} = x_i^t + v_i^{t+1}
$$

### Intuition:

- Move a bit like before (inertia)
- Move toward your own best (learning from self)
- Move toward the group‚Äôs best (learning from others)

---

## üìä ACO vs PSO Summary

| Feature               | ACO                         | PSO                                     |
| --------------------- | --------------------------- | --------------------------------------- |
| Inspired by           | Ants finding food           | Birds flocking / fish schooling         |
| Key idea              | Pheromone trails guide path | Velocities move particles               |
| Memory of past        | Pheromone traces on paths   | Personal best and global best positions |
| Exploration mechanism | Random choice, evaporation  | Random factors in velocity              |
| Problem types         | Discrete paths (e.g., TSP)  | Continuous and discrete optimization    |

---

## üß† Why They Work

- Both rely on **simple agents + local rules**.
- Use feedback:

  - **ACO** uses **stigmergy** (indirect communication via environment).
  - **PSO** uses **direct communication** (shared knowledge of bests).

- They **self-organize** to find good solutions ‚Äî even for problems that are hard to solve by hand!

---

# Optimisation and Game Theory

# Zero Sum Games

---

## üé≤ What is a **Zero-Sum Game**?

A **zero-sum game** is a situation where:

> **One player‚Äôs gain is exactly another player‚Äôs loss.**

The total ‚Äúscore‚Äù in the game stays constant ‚Äî if one player wins 5 points, the other loses 5 points. So, the **sum is zero**.

### üß† Simple Example: Rock-Paper-Scissors (RPS)

- If you win ‚Üí you get +1, the other player gets -1.
- If it's a tie ‚Üí both get 0.
- Whatever one gains, the other loses ‚Üí total = 0.

|              | Rock | Paper | Scissors |
| ------------ | ---- | ----- | -------- |
| **Rock**     | 0,0  | -1,1  | 1,-1     |
| **Paper**    | 1,-1 | 0,0   | -1,1     |
| **Scissors** | -1,1 | 1,-1  | 0,0      |

---

## üß† What is a **Nash Equilibrium**?

A **Nash Equilibrium** is a stable situation where:

> **No player can do better by changing their strategy alone**, if the other player keeps their strategy the same.

### In Rock-Paper-Scissors:

If both players **randomly choose** each move with equal probability (‚Öì each), then:

- Neither player has a reason to switch strategies.
- If you try always picking Rock, the other player can beat you by always picking Paper.
- So, the **random mix is stable** ‚Äî that‚Äôs a **Nash Equilibrium**!

---

## üîÅ Why Nash Equilibrium Matters in Zero-Sum Games

In **zero-sum games**, the Nash equilibrium tells us:

- The best strategy for each player when the opponent is also playing their best.
- It guarantees **no regrets** ‚Äî even if you don‚Äôt win, you couldn‚Äôt have done better by changing your plan (assuming the other player sticks to theirs).

---

## üéØ Visual Analogy

Imagine a **seesaw**:

- If one side goes up (+), the other must go down (‚àí).
- If both players sit at just the right spots (balanced), neither can shift their position to gain height without the other adjusting.

That balance point = **Nash Equilibrium** in a zero-sum game.

---

## üîí Key Properties (from your notes):

- Always **exists** in finite games.
- May involve **mixed strategies** (random choice, not always deterministic).
- **Stable**: No single player has an incentive to change alone.

---

## ‚úÖ Summary

| Concept              | Simple Definition                                                              |
| -------------------- | ------------------------------------------------------------------------------ |
| **Zero-Sum Game**    | One player's gain = another player's loss                                      |
| **Nash Equilibrium** | A strategy where neither player benefits from changing their move alone        |
| **Example**          | Rock-Paper-Scissors: best strategy = play each move with equal probability (‚Öì) |

---

Absolutely! Let‚Äôs break down **Normal-Form Games** and how to **compute and interpret equilibrium strategies**, using simple examples and ideas from **Lecture 21**.

---

## üé≤ What is a **Normal-Form Game**?

A **Normal-Form Game** is a way of representing a game where:

- There are **two or more players**.
- Each player picks a **strategy at the same time** (no turns).
- The outcomes (called **payoffs**) are shown in a **matrix** (like a table).

---

### üìã Simple Example: "Battle of the Sexes"

Imagine Alice and Bob want to go out together, but they prefer different events:

- Alice loves **ballet**, Bob loves **football**.
- They both prefer to be **together** than apart.

|                 | Bob: Ballet | Bob: Football |
| --------------- | ----------- | ------------- |
| Alice: Ballet   | (2, 1)      | (0, 0)        |
| Alice: Football | (0, 0)      | (1, 2)        |

- First number = Alice‚Äôs payoff
- Second number = Bob‚Äôs payoff

---

## üß† What‚Äôs the Goal?

Each player wants to **maximize their own payoff**.

But they must choose **without knowing** what the other will do ‚Äî so they need to **think strategically**.

---

## ‚öñÔ∏è What is a **Nash Equilibrium** in Normal-Form Games?

It‚Äôs a **stable pair of strategies** where:

> No player can do better by switching their choice **alone**.

### In Battle of the Sexes:

- Two **pure strategy Nash equilibria**:

  1. Both choose Ballet ‚Üí (2, 1)
  2. Both choose Football ‚Üí (1, 2)

- Each person is happy **given what the other chose**.

There‚Äôs also a **mixed strategy equilibrium** where:

- Alice plays Ballet with 2/3 probability, Football with 1/3.
- Bob plays Football with 2/3 probability, Ballet with 1/3.

This randomizes choices to **balance fairness**, but can lead to **miscoordination**.

---

## üßÆ How to Compute Equilibrium Strategies (Simply)

1. **Check Best Responses**:

   - For each of the other player's actions, find your best move.

2. **Look for matches**:

   - A cell where both players are playing **best responses to each other** = Nash equilibrium.

3. **For Mixed Strategies** (random choices):

   - Find probabilities that make each player **indifferent** between their options.
   - This means: ‚ÄúI don‚Äôt care what I pick, because either way I expect the same payoff.‚Äù

   From your lecture:

   > For Alice to be indifferent:
   > $2y_B = y_F \Rightarrow y_B = \frac{1}{3}, y_F = \frac{2}{3}$

---

## üéØ Visual Analogy

Imagine each player standing at one end of a see-saw with two stable resting points (Ballet/Ballet and Football/Football). As long as they coordinate, they‚Äôre balanced. But if one moves alone, the see-saw tilts and they both fall off balance.

---

## ‚úÖ Summary

| Concept                 | Simple Explanation                                                   |
| ----------------------- | -------------------------------------------------------------------- |
| **Normal-Form Game**    | Table showing payoffs for each combination of strategies             |
| **Players Act**         | Simultaneously (no turns)                                            |
| **Payoff Matrix**       | Shows what each player gets for every action pair                    |
| **Nash Equilibrium**    | A set of strategies where neither wants to change alone              |
| **Mixed Strategy**      | Each player chooses actions randomly with certain probabilities      |
| **Battle of the Sexes** | Example of a game with two pure equilibria and one mixed equilibrium |

---

# Normal Form Games

---

## üé≤ What is a **Normal-Form Game**?

A **Normal-Form Game** is a way of representing a game where:

- There are **two or more players**.
- Each player picks a **strategy at the same time** (no turns).
- The outcomes (called **payoffs**) are shown in a **matrix** (like a table).

---

### üìã Simple Example: "Battle of the Sexes"

Imagine Alice and Bob want to go out together, but they prefer different events:

- Alice loves **ballet**, Bob loves **football**.
- They both prefer to be **together** than apart.

|                 | Bob: Ballet | Bob: Football |
| --------------- | ----------- | ------------- |
| Alice: Ballet   | (2, 1)      | (0, 0)        |
| Alice: Football | (0, 0)      | (1, 2)        |

- First number = Alice‚Äôs payoff
- Second number = Bob‚Äôs payoff

---

## üß† What‚Äôs the Goal?

Each player wants to **maximize their own payoff**.

But they must choose **without knowing** what the other will do ‚Äî so they need to **think strategically**.

---

## ‚öñÔ∏è What is a **Nash Equilibrium** in Normal-Form Games?

It‚Äôs a **stable pair of strategies** where:

> No player can do better by switching their choice **alone**.

### In Battle of the Sexes:

- Two **pure strategy Nash equilibria**:

  1. Both choose Ballet ‚Üí (2, 1)
  2. Both choose Football ‚Üí (1, 2)

- Each person is happy **given what the other chose**.

There‚Äôs also a **mixed strategy equilibrium** where:

- Alice plays Ballet with 2/3 probability, Football with 1/3.
- Bob plays Football with 2/3 probability, Ballet with 1/3.

This randomizes choices to **balance fairness**, but can lead to **miscoordination**.

---

## üßÆ How to Compute Equilibrium Strategies (Simply)

1. **Check Best Responses**:

   - For each of the other player's actions, find your best move.

2. **Look for matches**:

   - A cell where both players are playing **best responses to each other** = Nash equilibrium.

3. **For Mixed Strategies** (random choices):

   - Find probabilities that make each player **indifferent** between their options.
   - This means: ‚ÄúI don‚Äôt care what I pick, because either way I expect the same payoff.‚Äù

   From your lecture:

   > For Alice to be indifferent:
   > $2y_B = y_F \Rightarrow y_B = \frac{1}{3}, y_F = \frac{2}{3}$

---

## üéØ Visual Analogy

Imagine each player standing at one end of a see-saw with two stable resting points (Ballet/Ballet and Football/Football). As long as they coordinate, they‚Äôre balanced. But if one moves alone, the see-saw tilts and they both fall off balance.

---

## ‚úÖ Summary

| Concept                 | Simple Explanation                                                   |
| ----------------------- | -------------------------------------------------------------------- |
| **Normal-Form Game**    | Table showing payoffs for each combination of strategies             |
| **Players Act**         | Simultaneously (no turns)                                            |
| **Payoff Matrix**       | Shows what each player gets for every action pair                    |
| **Nash Equilibrium**    | A set of strategies where neither wants to change alone              |
| **Mixed Strategy**      | Each player chooses actions randomly with certain probabilities      |
| **Battle of the Sexes** | Example of a game with two pure equilibria and one mixed equilibrium |

---

# Multi Armed Bandit

---

## üé∞ What is the **Multi-Armed Bandit Problem**?

Imagine you're in a casino with **10 slot machines** (called "arms"). Each one gives **different unknown rewards** when you play it.

Your goal is to:

> **Maximize your total reward over time**.

But there‚Äôs a problem:

- You **don‚Äôt know** which machine is best at first.
- So you must choose:

  - **Explore**: Try new machines to learn about them.
  - **Exploit**: Play the one you **think** is best so far.

This trade-off is called the **exploration vs. exploitation dilemma**.

---

## ü§ñ Enter the **$\epsilon$-Greedy Algorithm**

It‚Äôs a **simple but powerful strategy** to handle this trade-off.

### üí° How it works:

At each step:

- With probability **$\epsilon$** ‚Üí **Explore**:

  - Pick a **random** machine (even if it seems bad).

- With probability **$1 - \epsilon$** ‚Üí **Exploit**:

  - Pick the machine with the **highest estimated reward** so far.

### Example:

Let‚Äôs say $\epsilon = 0.1$ (10%):

- 90% of the time ‚Üí choose the best-known machine.
- 10% of the time ‚Üí pick a random machine to explore.

This way, you:

- **Mostly play smart**, based on what you‚Äôve learned.
- **Occasionally try something new**, in case there's a better option.

---

## üß† Why It Works

- If you **never explore**, you might get stuck with a machine that **seemed** best early on.
- If you **only explore**, you waste chances to get big rewards.
- $\epsilon$-greedy balances both.

---

## üßÆ How to Estimate a Machine‚Äôs Reward

For each machine $a$, keep track of:

$$
Q_t(a) = \frac{r_1 + r_2 + \dots + r_k}{k}
$$

Where:

- $r_1, ..., r_k$ are the rewards received from machine $a$
- $k$ is how many times you've played it

You **update this average** every time you play that machine.

---

## üß™ Summary: $\epsilon$-Greedy in Action

| Step              | Description                                                   |
| ----------------- | ------------------------------------------------------------- |
| Choose $\epsilon$ | Small number like 0.1 or 0.01 (controls how much you explore) |
| At each round     | With probability $\epsilon$, explore; otherwise, exploit      |
| Update values     | Use average reward or incremental updates (see Lecture 5)     |

---

## üì¶ Real-Life Analogy

Think of trying restaurants in a new city:

- You mostly go to the one you like best (exploit).
- But now and then, you try a different one (explore).
- Over time, you might discover **an even better place**!

---

# Q Learning Exploring

- **Q-learning** is an off-policy RL algorithm that can learn optimal actions in unknown environments. Understand how **exploration** in Q-learning is handled using **epsilon-greedy** or other exploration techniques like **softmax**. (Reference lecture 8)

## üîç How Does Q-learning Explore?

Since Q-learning needs to **try different actions** to learn good values, it uses **exploration strategies**.

### üîÅ 1. **$\epsilon$-Greedy Exploration** (from Lecture 5):

- With probability $\epsilon$, pick a **random action** (explore).
- With probability $1 - \epsilon$, pick the **best action so far** (exploit).

This helps Q-learning **gather data** about new actions while still using what it knows.

---

### üî• 2. **Softmax Exploration**

Instead of random or best, **Softmax** assigns a **probability to each action** based on how good its Q-value is.

#### Formula:

$$
P(a) = \frac{e^{Q(a)/\tau}}{\sum_b e^{Q(b)/\tau}}
$$

Where:

- $Q(a)$ = estimated value of action $a$
- $\tau$ = **temperature**:

  - **High $\tau$** = more random (explore more)
  - **Low $\tau$** = pick best actions more often (exploit)

### üéØ Example:

- If two actions have Q-values 10 and 9, both get chosen ‚Äî but 10 is more likely.
- You don‚Äôt **ignore** the less-good action, just pick it **less often**.

---

## ü§ñ Summary: Q-learning + Exploration

| Concept               | Description                                                            |
| --------------------- | ---------------------------------------------------------------------- |
| **Q-learning**        | Learns best actions by updating Q-values using rewards and estimates   |
| **Off-policy**        | Learns optimal strategy even if it behaves differently during training |
| **Exploration need**  | Must try different actions to learn their rewards                      |
| **$\epsilon$-greedy** | Simple: explore randomly with small chance                             |
| **Softmax**           | Smarter: explore based on how good each action seems                   |
| **Goal**              | Find the best action in each state over time                           |

---

## üçî Real-Life Analogy

Imagine ordering lunch from a food app:

- $\epsilon$-greedy = always order your favorite, but sometimes try something new at random.
- Softmax = usually order what you like best, but also try slightly worse-rated dishes **based on how close they are in rating**.

---

# Bio Inspired Optimisation

---

## üß¨ Bioinspired Optimization ‚Äì Key Ideas

These algorithms **mimic natural processes** (like evolution, insect behavior, or flocking) to **search for good solutions** in hard problems, especially where traditional methods struggle.

---

## üêú **Ant Colony Optimization (ACO)**

- **Inspired by**: Ants finding shortest paths using pheromones.
- **Used for**: Combinatorial problems (e.g., **TSP**, routing).
- **How it works**:

  - Ants build solutions probabilistically.
  - Good solutions ‚Üí more pheromone.
  - **Pheromone trails** + **heuristics** guide search.

---

## üê¶ **Particle Swarm Optimization (PSO)**

- **Inspired by**: Bird flocking / fish schooling.
- **Used for**: Continuous optimization (e.g., **function minimization**).
- **How it works**:

  - Particles move through space using:

    - Personal best
    - Global best
    - Momentum + randomness

  - Balances **exploration and exploitation**.

---

## üß™ **Genetic Algorithms (GA)**

- **Inspired by**: Natural selection and genetics.
- **Used for**: Diverse problems (e.g., scheduling, design).
- **How it works**:

  - Population of solutions.
  - Apply **selection**, **crossover**, and **mutation**.
  - Fittest survive and evolve over generations.

---

## üß† What To Remember

| Algorithm | Inspiration   | Key Mechanism        | Good For               |
| --------- | ------------- | -------------------- | ---------------------- |
| ACO       | Ant foraging  | Pheromone trails     | Discrete path problems |
| PSO       | Bird flocking | Velocity updates     | Continuous functions   |
| GA        | Evolution     | Crossover + mutation | General optimization   |

---

# Convolutional and Recurrent Neural Networks

# CNNs

- **Convolutional Neural Networks (CNNs)** are powerful tools for image recognition and other tasks that involve grid-like data. Understand how convolutional layers work to extract hierarchical features and how these features help in classification tasks. (Reference lecture 15)

---

CNNs are **special neural networks** designed for **grid-like data**, especially images (2D grids of pixels).

---

## üß± Core Building Block: Convolutional Layer

### üîç What it does:

- Applies **filters (kernels)** that **slide over the image**.
- At each position, the filter computes a **dot product** with the image patch ‚Üí creates a **feature map**.
- **Each filter detects a specific feature**, like an edge, corner, or texture.

### üß† Intuition:

> Think of filters as **pattern detectors**. Early layers detect **simple features** (e.g., edges), deeper layers detect **complex patterns** (e.g., faces).

---

## üîÑ Hierarchical Feature Extraction

- **Layer 1**: Edges and colors
- **Layer 2**: Shapes and textures
- **Layer 3+**: Object parts and full objects

This **hierarchy** helps the CNN **understand what‚Äôs in the image** step by step.

---

## üß† Why It Works for Classification

- CNN extracts **increasingly abstract features** from raw pixels.
- At the end, a **fully connected layer** uses these features to **predict the class** (e.g., cat vs. dog).

---

## ‚úÖ What To Remember

| Concept                   | Explanation                                                        |
| ------------------------- | ------------------------------------------------------------------ |
| **Convolutional Layer**   | Slides filters over input to detect features                       |
| **Feature Maps**          | Show where features (like edges) occur                             |
| **Hierarchical Features** | Early = simple, deeper = complex                                   |
| **Useful For**            | Images, audio, and other grid-structured data                      |
| **Why powerful**          | Reuse of weights (filters), local patterns, translation invariance |

---

# RNNs

---

## üîÅ Recurrent Neural Networks (RNNs)

RNNs are **neural networks for sequence data** ‚Äî where the **order of inputs matters**, like:

- **Time series** (e.g., stock prices)
- **Text** (e.g., predicting the next word)
- **Audio** (e.g., speech recognition)

---

## üîÑ How RNNs Work

- RNNs process **one step at a time**, passing a **hidden state** forward.
- This gives them **memory** of previous steps.

> Example: Understanding a sentence word by word while remembering the earlier words.

---

## ‚ö†Ô∏è Problem: Vanishing Gradients

- When training long sequences, the **gradient becomes very small** as it flows backward in time.
- This makes it hard for RNNs to **learn long-range dependencies** (e.g., remembering a word from 10 steps ago).

---

## üí° Solution: **LSTM (Long Short-Term Memory)**

**LSTM units** are special RNN cells that **solve the vanishing gradient problem**.

### üß† Key idea:

> LSTM uses **gates** to control how information flows, what to **remember**, and what to **forget**.

---

## üõ†Ô∏è LSTM Structure

Each LSTM cell has:

- **Forget Gate**: What part of the memory to erase
- **Input Gate**: What new info to add
- **Output Gate**: What to output to the next step

It maintains a **cell state** (long-term memory) that **preserves gradients**, so the network can learn **dependencies across long sequences**.

---

## ‚úÖ What To Remember

| Concept                | Explanation                                    |
| ---------------------- | ---------------------------------------------- |
| **RNN**                | Neural net for sequences with memory of past   |
| **Vanishing gradient** | Happens when training long sequences           |
| **LSTM**               | Special RNN unit with gates to control memory  |
| **Solves**             | Allows learning long-term dependencies         |
| **Used for**           | Text, time series, speech, anything sequential |

---

Let me know if you'd like a visual breakdown of an LSTM cell!
