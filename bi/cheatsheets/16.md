Here's a combined, clear summary of the lecture slides on **Deep Learning**, with explanations and relevant missing context added where useful:

---

## **Convolutional Neural Networks (ConvNets) – Core Concepts**

### **1. Convolution Operations**

- A **convolution** slides a filter (small matrix) over an image (large matrix) to extract features like edges.
- Each filter detects specific patterns, e.g., vertical or horizontal edges.

### **2. Key Parameters**

- **Stride**: how far the filter moves at each step. Larger strides shrink the output.

  - Output size formula: **(N - F)/S + 1**

    - N = input size, F = filter size, S = stride.

- **Padding**: adds extra pixels (typically zeros) around the input to control output size.

  - Padding = (F-1)/2 keeps the output the same size as input when stride = 1.

### **3. Convolutional Layers**

- A **Conv layer** applies multiple filters. Each produces one output channel.
- Layers often use ReLU (Rectified Linear Unit) for non-linearity.

---

## **ConvNet Architectures – Case Studies**

### **LeNet-5 (LeCun et al., 1998)**

- Designed for digit classification (e.g., MNIST).
- Structure: **CONV → POOL → CONV → POOL → FC**
- Used 5x5 filters, 2x2 pooling.

---

### **AlexNet (Krizhevsky et al., 2012)**

- Input: 227x227x3 images.
- First Layer: 96 filters of size 11x11, stride 4 → Output: 55x55x96
- Parameters in CONV1: (11×11×3)×96 = 34,848 ≈ 35K
- Pooling: 3x3 filters, stride 2 → Output size = (55-3)/2 + 1 = 27 → 27x27x96
- Full pipeline:

  ```
  INPUT → CONV1 → POOL1 → NORM1 → CONV2 → POOL2 → NORM2
        → CONV3 → CONV4 → CONV5 → POOL3 → FC6 → FC7 → FC8
  ```

- Innovations:

  - First to use **ReLU**
  - **Dropout** to prevent overfitting (0.5)
  - **Data augmentation**
  - **L2 regularization** (weight decay = 5e-4)
  - Optimizer: **SGD + Momentum 0.9**

---

### **VGG-16 (Simonyan & Zisserman, 2014)**

- 16 layers total; only **3x3 conv filters**, stride 1, pad 1.
- **2-3 CONV layers** per pooling layer.
- Uses **2x2 max pooling**, stride 2.
- Stacking three 3x3 filters = 7x7 receptive field but fewer parameters.
- Deeper than AlexNet, improves accuracy:

  - Top-5 error: 7.3% vs 11.7% (ZFNet)

---

### **GoogLeNet (Szegedy et al., 2014)**

- Introduced the **Inception module** – combines different filter sizes in parallel.
- Efficient: 22 layers, **only 5 million parameters** (12x less than AlexNet).
- No fully connected (FC) layers.
- Winner of ILSVRC 2014: Top-5 error = **6.7%**

---

### **ResNet (He et al., 2015)**

- Very deep (e.g., 152 layers for ImageNet).
- Introduced **residual connections**:

  - Instead of learning **output = f(input)**, learn **residual = f(input) – input**.
  - Helps gradients flow better during training.

- State-of-the-art performance at the time.

---

## **Transformers**

- Encoder-decoder architecture, originally for NLP.
- Introduced in: _"Attention is All You Need"_ (Vaswani et al., 2017)

---

## **Training Deep Networks**

### **Strategies to Avoid Underfitting / Overfitting**

- **Underfitting (high bias)**:

  - Use deeper/wider networks
  - Train longer

- **Overfitting (high variance)**:

  - Use more data
  - Apply **regularization**, e.g., L2 norm
  - Use **dropout**, early stopping

### **Key Techniques**

- **Dropout**: randomly "turn off" neurons during training to prevent co-adaptation.
- **Early stopping**: stop training when validation loss stops improving.

---

## **Optimizers**

### **1. SGD with Momentum**

- Combines current gradient with previous step:

  ```
  v_t = β*v_{t-1} + (1-β)*∇L(w)
  w = w - α*v_t
  ```

  - β = momentum factor (usually 0.9)
  - α = learning rate

### **2. AdaGrad**

- Adapts learning rate per parameter based on past gradients.
- Parameters that receive small updates earlier get bigger updates later.
- Drawback: learning rate shrinks too much over time.

### **3. RMSProp**

- Like AdaGrad, but uses a **moving average** of past squared gradients.
- Helps avoid shrinking the learning rate too much.

### **4. Adam (Adaptive Moment Estimation)**

- Combines **Momentum + RMSProp**
- Tracks both:

  - **m_t**: moving average of gradients
  - **v_t**: moving average of squared gradients

- Bias correction is applied to both.
- Common settings:

  - Learning rate = 0.001
  - β₁ = 0.9, β₂ = 0.999, ε = 10⁻⁸

---

## **Deep Learning Software & Tools**

- **Libraries**:

  - TensorFlow / Keras
  - PyTorch
  - Caffe / Caffe2
  - Theano

- **Model Zoos**: Pretrained models available online:

  - Keras: [https://github.com/albertomontesg/keras-model-zoo](https://github.com/albertomontesg/keras-model-zoo)
  - TensorFlow: [https://github.com/tensorflow/models](https://github.com/tensorflow/models)
  - Caffe: [https://github.com/BVLC/caffe/wiki/Model-Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)

---

## **Transfer Learning**

- Use pretrained models (e.g., on ImageNet) for a new task.

- Options:

  - **Freeze** earlier layers (keep pretrained weights)
  - **Fine-tune** later layers on new data

- Examples:

  - MNIST, SVHN digit datasets
  - TensorFlow image retraining tutorial:
    [https://www.tensorflow.org/hub/tutorials/image_retraining](https://www.tensorflow.org/hub/tutorials/image_retraining)

---

Let me know if you’d like visual summaries or examples for specific architectures like ResNet or AlexNet.
