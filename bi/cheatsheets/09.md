---
# üß† Temporal Difference (TD) Learning ‚Äî Core Content
---

## 1. **What is Temporal Difference Learning?**

TD learning is a method in Reinforcement Learning (RL) used to estimate the value of states **while learning** from incomplete episodes. It combines ideas from:

- **Monte Carlo (MC)**: learning from raw experience
- **Dynamic Programming (DP)**: updating estimates using other learned estimates

‚û°Ô∏è It **learns predictions based on other predictions**, without waiting for the final outcome.

---

## 2. **TD Prediction (Policy Evaluation)**

### Goal:

Given a policy $\pi$, estimate the **state-value function** $V^\pi(s)$, which tells you how good a state is if you follow policy $\pi$.

### TD(0) Update Rule:

$$
V(s) \leftarrow V(s) + \alpha \left[ r + \gamma V(s') - V(s) \right]
$$

Where:

- $s$: current state
- $s'$: next state
- $r$: reward received after transition
- $\alpha$: learning rate
- $\gamma$: discount factor (how much future rewards matter)

üß© **Intuition**: You're adjusting your current guess $V(s)$ towards the observed reward plus your guess of the next state's value.

---

## 3. **TD vs Other Methods**

| Method        | Bootstraps (uses estimates)? | Samples (uses actual outcomes)? |
| ------------- | ---------------------------- | ------------------------------- |
| Monte Carlo   | ‚ùå No                        | ‚úÖ Yes                          |
| Dynamic Prog. | ‚úÖ Yes                       | ‚ùå No                           |
| TD Learning   | ‚úÖ Yes                       | ‚úÖ Yes                          |

- **TD** is a hybrid: it updates based on real experience (**like MC**) and uses current estimates (**like DP**).

---

## 4. **Advantages of TD Learning**

- **No environment model needed**: You don't need to know how the world works.
- **Learn online and incrementally**:

  - No need to wait for episode to finish.
  - Works well with partial episodes or incomplete data.

- **Efficient in memory and computation**.

---

## 5. **TD Control Methods**

TD Prediction only estimates values under a fixed policy. TD **Control** updates both the value estimates and the policy, aiming to **find the optimal policy**.

Two main methods:

---

### A. **Sarsa (On-policy TD Control)**

Update rule:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]
$$

- Learns from the **actual action** taken in the next state.
- Follows the policy it's trying to improve (on-policy).

üìå **Behavior**: Conservative. Learns from what you actually did.

---

### B. **Q-learning (Off-policy TD Control)**

Update rule:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

- Learns from the **best possible action** in the next state.
- Learns optimal policy **even while following a different one** (off-policy).

üìå **Behavior**: Aggressive. Learns from what you _should have_ done.

---

## 6. **On-policy vs Off-policy**

| Type       | Learns From     | Learns About   |
| ---------- | --------------- | -------------- |
| On-policy  | Current policy  | Current policy |
| Off-policy | Behavior policy | Optimal policy |

- **Sarsa** is on-policy: learns from and about the same policy.
- **Q-learning** is off-policy: behaves one way, learns about another.

---

## 7. **Convergence to Optimality**

- **Sarsa**:

  - Converges if:

    - Every state-action pair is visited infinitely often
    - Policy becomes greedy over time

- **Q-learning**:

  - Easier convergence
  - Needs:

    - Infinite visits
    - Learning rate $\alpha$ decreases over time (but not too fast)

‚úÖ Even if assumptions aren‚Äôt fully met, both methods perform well in practice.

---

## 8. **Summary of TD Learning**

- TD methods allow **value prediction and control** without full models.
- They **bootstrap and sample** ‚Äî blending the strengths of DP and MC.
- **TD(0)** is the simplest form, using one-step lookahead.
- Control methods:

  - **Sarsa** for on-policy learning.
  - **Q-learning** for off-policy learning.

---
