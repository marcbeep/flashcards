---
## ðŸ§  **Reinforcement Learning (RL)**

### 1. **Introduction & Motivation**

- Inspired by how nature solves problems.
- RL is used in games, decision-making tasks, and robotics.
- Multi-armed bandits introduced as a simple RL setting.
---

### 2. **Key Concepts of RL**

#### a. **Agent-Environment Interface**

- Discrete time steps: $t = 0, 1, 2, \dots$
- At each time $t$, the agent:

  - Observes state $s_t$
  - Chooses action $a_t$
  - Receives reward $r_t$
  - Transitions to next state $s_{t+1}$

#### b. **Policy**

- A policy $\pi$ maps states to action probabilities.
- Example: $\pi(s, \text{UP}) = 0.8, \pi(s, \text{DOWN}) = 0.2$
- The agent updates its policy over time to maximize rewards.

---

### 3. **Goals and Rewards**

#### a. **What is a Goal in RL?**

- Defined via rewards (scalar signals).
- The **reward hypothesis**: all goals can be seen as maximizing cumulative reward.
- Rewards should:

  - Be measurable
  - Occur frequently enough
  - Be outside the agentâ€™s direct control

#### b. **Returns**

- **Episodic tasks**: Break into distinct episodes (e.g., games).

  - Return: $R_t = r_{t+1} + r_{t+2} + \dots + r_T$

- **Continuing tasks**: Go on indefinitely.

  - Use discounted return:
    $R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots$

  - $\gamma \in [0, 1]$: discount factor (closer to 1 = more farsighted)

---

### 4. **Example: Pole Balancing**

- **Episodic view**: +1 reward per step before failure â†’ return = steps survived
- **Continuing view**: 0 reward normally, -1 on failure â†’ return = $-\gamma^k$
- Objective in both: Avoid failure for as long as possible

---

### 5. **Notation**

- Time steps usually start at 0
- Same symbols used for state, reward, and return across episodes
- General return formula:
  $R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}$
  (Works for both episodic and continuing tasks)

---

### 6. **Markov Property & MDPs**

#### a. **Markov Property**

- A state is Markov if it contains all relevant info from the history.
- Future state & reward depend only on current state and action, not full history.

#### b. **Markov Decision Processes (MDPs)**

- If a task satisfies the Markov property, it can be modeled as an MDP.
- **Finite MDP** includes:

  - Finite set of states and actions
  - Transition probabilities: $P(s'|s, a)$
  - Reward expectations: $R(s, a, s') = E[r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']$

#### c. **Example of an MDP**

- Defined by:

  - Set of states
  - Set of actions
  - Transition function
  - Reward function

---
