---

## 🧠 **1. Roots of Reinforcement Learning**

- **Fields of Origin**:
  - Mathematical Psychology (1910s)
  - Control Theory (1950s) – Richard Bellman (Dynamic Programming, Bellman Equations)
- **Multidisciplinary Influences**:
  - Artificial Intelligence
  - Neuroscience
  - Deep Neural Networks
  - Psychology
  - Control Theory
  - Operations Research

---

## 🧩 **2. Key Concepts of RL**

### 🔄 What is RL?

- Goal-oriented learning from interaction
- Learns how to act to **maximize a reward**
- Learns during, from, and about interactions with the environment

### 🚀 Agent Characteristics

- Temporally situated
- Continual learning and planning
- Acts on a stochastic, uncertain environment

---

## 🌍 **3. Types of Environments**

| Type                                          | Description                           | Example                                              |
| --------------------------------------------- | ------------------------------------- | ---------------------------------------------------- |
| **Deterministic vs. Stochastic**              | Next state is (not) fully predictable | Crossword (deterministic), Taxi driving (stochastic) |
| **Fully Observable vs. Partially Observable** | Agent sees full/partial state         | Chess (fully), Taxi driving (partially)              |
| **Episodic vs. Sequential**                   | Has episodes or not                   | Maze running (episodic), Taxi driving (sequential)   |
| **Dynamic vs. Static**                        | Changes during decision-making        | Poker (static), Taxi driving (dynamic)               |
| **Discrete vs. Continuous**                   | States/actions are distinct or smooth | Poker (discrete), Taxi driving (continuous)          |
| **Single vs. Multi-Agent**                    | One or multiple agents                | Crossword (single), Taxi (multi-agent)               |

**Real-world is**: partially observable, stochastic, sequential, dynamic, continuous, multi-agent.

---

## 📐 **4. Elements of Reinforcement Learning**

| Element            | Description                                       |
| ------------------ | ------------------------------------------------- |
| **Policy**         | What to do – agent’s behavior                     |
| **Reward**         | What is good (immediate feedback)                 |
| **Value Function** | What is good in the long run (predicts reward)    |
| **Model**          | What follows what – predicts environment behavior |

---

## 🧪 **5. RL vs Supervised Learning**

| Aspect          | Supervised Learning  | Reinforcement Learning     |
| --------------- | -------------------- | -------------------------- |
| Output          | Desired output given | Reward/feedback only       |
| Goal            | Match output         | Maximize cumulative reward |
| Training Signal | Labels               | Rewards                    |

---

## 🕹️ **6. Tic-Tac-Toe Example (Simple RL)**

- **Value Table**: Map each state to value (e.g., 1 = win, 0 = loss)
- **Learning Rule**:
  \[
  V(s) \leftarrow V(s) + \alpha [V(s') - V(s)]
  \]
  (move value towards next state's value)
- **Greedy vs. Exploration**: Balance choosing best known move vs. trying new ones

### 🧠 Improvement Ideas

- Use symmetries
- Pre-train via self-play
- Learn from random moves
- Use value function approximators

---

## 🧠 **7. Characteristics of RL Learning**

- No direct instruction on what actions to take
- Learns by **trial-and-error**
- **Delayed rewards** make learning harder
- Must **explore and exploit**
- Learns in **uncertain environments**

---

## 🧬 **8. Applications of RL**

- **Games**: AlphaGo, TD-Gammon (backgammon), Tic-Tac-Toe
- **Robotics**: navigation, walking, grasping
- **Industry**: Elevator control, inventory management, dynamic channel assignment
- **Competitions**: RoboCup Soccer (Stone & Veloso)

---

## 📚 **9. Practice Task**

> Identify the four RL elements (policy, reward, value, model) in various tasks:

- Tic-Tac-Toe
- Backgammon
- Poker
- Chess
- Crossword puzzle
- K-armed bandit

---
