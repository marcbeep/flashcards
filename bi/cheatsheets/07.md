# Prev Sections Omitted

## **3. Reinforcement Learning Core Concepts**

### **3.1. Value Functions**

These help the agent decide what is good in the long run.

- **State-Value Function (Vπ)**: Expected return from state _s_ under policy π.

  $$
  V^{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} \mid s_0 = s \right]
  $$

- **Action-Value Function (Qπ)**: Expected return from taking action _a_ in state _s_ and following policy π thereafter.

  $$
  Q^{\pi}(s,a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_{t+1} \mid s_0 = s, a_0 = a \right]
  $$

---

### **3.2. Bellman Equations**

- **For a Policy π**:

  - Expresses recursive relationship:

    $$
    V^{\pi}(s) = \mathbb{E}_{\pi} [r_{t+1} + \gamma V^{\pi}(s_{t+1}) \mid s_t = s]
    $$

- **Intuition**: Averages over all possible next states weighted by their transition probabilities.

---

## **4. Optimality in RL**

### **4.1. Optimal Policies**

- A policy π\* is **optimal** if it achieves the highest value in every state.
- **All optimal policies share the same value functions**:

  - Optimal State-Value:

    $$
    V^*(s) = \max_{\pi} V^{\pi}(s)
    $$

  - Optimal Action-Value:

    $$
    Q^*(s,a) = \max_{\pi} Q^{\pi}(s,a)
    $$

---

### **4.2. Bellman Optimality Equations**

- For **V**\*:

  $$
  V^*(s) = \max_a \mathbb{E} [r_{t+1} + \gamma V^*(s_{t+1}) \mid s_t = s, a_t = a]
  $$

- For **Q**\*:

  $$
  Q^*(s,a) = \mathbb{E} [r_{t+1} + \gamma \max_{a'} Q^*(s_{t+1}, a') \mid s_t = s, a_t = a]
  $$

- These are systems of **nonlinear equations** with unique solutions for V\* and Q\*.

---

## **5. Example: Golf**

- **State**: Ball location
- **Actions**: `putt` (accurate), `driver` (longer distance, less accurate)
- **Reward**: -1 per stroke until the ball is in the hole
- Demonstrates how different actions and strategies affect the value of states and actions.

---

## **6. Why Value Functions Matter**

- **V\***: Helps find optimal policy using **one-step-ahead search**.
- **Q\***: Enables selection of the best action directly:

  $$
  a^* = \arg\max_a Q^*(s, a)
  $$

---
