---

## **Backpropagation for One Neuron**

We want to compute how the error changes with respect to each weight in the neuron. This allows us to update the weights to minimize the error.

- **Error Function**:
  $E = \frac{1}{2}(o - d)^2$
  where:

  - $o$: output of the neuron
  - $d$: desired output (target)

- **Neuron Output**:
  $o = f(q)$
  where:

  - $f$ is the sigmoid function
  - $q = \sum_j w_j x_j$: weighted sum of inputs

### Derivatives (Using Chain Rule):

To find how $E$ changes with weight $w_j$:

$$
\frac{\partial E}{\partial w_j} = \frac{\partial E}{\partial o} \cdot \frac{\partial o}{\partial q} \cdot \frac{\partial q}{\partial w_j}
$$

Each component:

- $\frac{\partial E}{\partial o} = o - d$
- $\frac{\partial o}{\partial q} = f(q)(1 - f(q)) = o(1 - o)$ (derivative of sigmoid)
- $\frac{\partial q}{\partial w_j} = x_j$

So:

$$
\frac{\partial E}{\partial w_j} = (o - d) \cdot o(1 - o) \cdot x_j
$$

### Weight Update Rule:

Use gradient descent:

$$
\Delta w_j = -\eta \cdot \frac{\partial E}{\partial w_j} = -\eta (o - d) o (1 - o) x_j
$$

$$
w_j \leftarrow w_j + \Delta w_j
$$

Where $\eta$ is the learning rate (step size).

---

## **Backpropagation Algorithm Overview**

**Steps:**

1. Initialize weights randomly.
2. Repeat until mean squared error (MSE) is low enough or time runs out:

   - For each input pattern:

     - Compute activations for hidden and output layers.
     - Calculate the error.
     - Adjust weights:

       - Output layer weights
       - Hidden layer weights

---

## **Deep Learning Overview**

- Deep learning uses **deep architectures** (many layers) to learn **useful feature representations** from data.
- Often uses **non-linear transformations** to extract high-level features.
- Two types:

  - **Supervised** (e.g., CNNs: Convolutional Neural Networks)
  - **Unsupervised** (e.g., Autoencoders)

Today’s focus: **Convolutional Neural Networks (CNNs)**

---

## **Convolution and Filtering**

Convolution is used to process signals and images by applying filters. Filters are small grids (kernels) that slide over the data to extract patterns.

### Applications:

- **Noise Removal**: Suppress or remove unwanted noise.
- **Edge Detection**: Highlight edges in images.
- **Pattern Detection**: Match templates or features.
- **Deconvolution**: Reverse effects of filters applied earlier.

---

## **1D Convolution**

### Continuous Case:

Output:

$$
h(t) = (f * g)(t) = \int f(\tau) \cdot g(t - \tau) d\tau
$$

### Discrete Case:

Given:

- Input signal $f$ (length m)
- Filter $g$ (length n)
- Output signal $h$

Each output value is a **weighted sum** of a segment of the input:

$$
h[i] = \sum_{k} f[i - k] \cdot g[k]
$$

---

## **2D Convolution (Image Processing)**

Images are treated as 2D arrays. A 2D filter (kernel) is slid across the image.

**Steps:**

1. Multiply each filter element with corresponding image pixel.
2. Sum the results.
3. Assign this sum to the output pixel.

### Formula:

$$
I'(i,j) = \sum_k \sum_l I(i-k, j-l) \cdot H(k, l)
$$

- $I$: input image
- $H$: filter
- $I'$: output image

---

## **Stride**

Stride defines how many pixels the filter moves at each step:

- Stride = 1 → Filter moves one pixel at a time.
- Larger strides → smaller output.

**Output size formula**:

$$
\text{Output size} = \frac{(N - F)}{\text{stride}} + 1
$$

- $N$: input size
- $F$: filter size

E.g., for $N=7, F=3$:

- Stride 1 → Output size = 5
- Stride 2 → Output size = 3
- Stride 3 → Output size = \~2

---

## **Padding**

To avoid shrinking the output size:

- Add zeros around the input border ("zero-padding").

Common padding:
If filter size $F$, use padding $(F - 1) / 2$

- Keeps input and output the same size.

E.g.:

- $F = 3$: pad = 1
- $F = 5$: pad = 2

---
