## 1. Introduction to Probability

**Probability** is the mathematical study of uncertainty — it describes how likely events are to happen.

- **Random Variable**:

  - A variable \( X \) that takes different values representing possible outcomes of a process.
  - We denote the probability of \( X \) taking value \( x \) as \( p(x) = \mathbb{P}(X = x) \).

- **Sample Space (\( \mathcal{S} \))**:

  - The set of all possible outcomes of an experiment.
  - Examples:
    - Tossing a coin once: \( \mathcal{S} = \{H, T\} \).
    - Tossing a coin twice: \( \mathcal{S} = \{HH, HT, TH, TT\} \).

- **Probability Mass Function (PMF) or Density Function (PDF)**:
  - **Discrete** case: PMF assigns probability \( p(x) \) to each outcome \( x \) (e.g., coin toss).
  - **Continuous** case: PDF assigns probability _density_ over intervals.

**Key Properties**:

- \( p(x) \geq 0 \) for all \( x \).
- \( \sum_x p(x) = 1 \) (for discrete), or \( \int p(x) dx = 1 \) (for continuous).

---

## 2. Basic Probability Concepts

### 2.1 Random Experiment

An action or process that leads to one of several outcomes, even under identical conditions.

Examples:

- Tossing a coin.
- Rolling a dice.
- Measuring rainfall.

---

## 3. Key Probability Rules

Understanding relationships between events is critical. These rules formalize how probabilities combine.

---

### 3.1 **Joint Probability**

- **Definition**: Probability that **two events happen together**.
- Example:

  - Probability that a randomly selected fruit is an **orange** from the **red basket**.

- **Mathematically**:  
  \[
  \mathbb{P}(X = x, Y = y)
  \]
  where \( X \) and \( Y \) are random variables.

**Illustration** (from the lecture example):
| | F = orange | F = apple | Total |
|---------|------------|-----------|-------|
| B = red | 30 | 10 | 40 |
| B = blue| 15 | 45 | 60 |
| Total | 45 | 55 | 100 |

Example:
\[
\mathbb{P}(B = r, F = o) = \frac{30}{100} = 0.3
\]

---

### 3.2 **Sum Rule** (Marginal Probability)

- **Definition**: To find the probability of an event regardless of another variable, **sum over** the other variable.

- **Formula**:
  \[
  \mathbb{P}(X = x) = \sum_y \mathbb{P}(X = x, Y = y)
  \]
- **Example**:
  - What is the probability of picking an orange? (regardless of basket)
    \[
    \mathbb{P}(F = o) = \frac{45}{100} = 0.45
    \]

---

### 3.3 **Conditional Probability**

- **Definition**: The probability that one event happens given that we know another event happened.

- **Formula**:
  \[
  \mathbb{P}(Y = y | X = x) = \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(X = x)}
  \]

- **Example**:
  - Probability that the basket is red given the fruit is orange:
    \[
    \mathbb{P}(B = r | F = o) = \frac{30}{45} = \frac{2}{3}
    \]

---

### 3.4 **Product Rule**

- **Definition**: Relates joint probability to conditional and marginal probability.

- **Formula**:
  \[
  \mathbb{P}(X = x, Y = y) = \mathbb{P}(Y = y | X = x) \mathbb{P}(X = x)
  \]

- **Example**:
  - We can calculate the joint probability if we know conditional and marginal probabilities.

---

### 3.5 **Bayes’ Rule**

- **Definition**: A way to "invert" conditional probabilities.
- **Motivation**: Given \( \mathbb{P}(B|A) \) and \( \mathbb{P}(A) \), how can we find \( \mathbb{P}(A|B) \)?

- **Formula**:
  \[
  \mathbb{P}(A|B) = \frac{\mathbb{P}(B|A) \mathbb{P}(A)}{\mathbb{P}(B)}
  \]

**Where**:

- \( \mathbb{P}(A) \) = prior probability (belief before evidence)
- \( \mathbb{P}(B|A) \) = likelihood (how likely evidence is under assumption)
- \( \mathbb{P}(B) \) = marginal probability of evidence
- \( \mathbb{P}(A|B) \) = posterior probability (updated belief after evidence)

**Intuition**:

- We update our belief about \( A \) after observing \( B \).
