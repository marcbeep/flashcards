---
### üß† Lecture 10: Reinforcement Learning (RL)
---

### üîÅ Reward Shaping

**What it is:**
Reward shaping means adding extra (artificial) rewards to help guide the agent toward learning a good policy faster. This is based on expert or domain knowledge.

**Why it's useful:**

- Speeds up learning.
- Can guide the agent more efficiently while still preserving the best (optimal) strategy, if done correctly.

**Q-learning update rule (basic):**

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
$$

Where:

- $s_t$: current state
- $a_t$: action taken
- $r_{t+1}$: reward received after action
- $\gamma$: discount factor (how much future rewards are considered)
- $\alpha$: learning rate

**With potential-based shaping:**

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + F(s, s') + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t) \right]
$$

- $F(s, s')$: potential-based function, used to shape the reward based on states
- This method helps by adding extra insight into which states are more promising.

---

### üé≠ Actor-Critic Methods

In these methods, learning is split into two parts:

- **Actor**: Learns the policy (decides what action to take).
- **Critic**: Estimates the value function (judges how good a state or action is).

The value function is now represented using parameters (e.g., weights in a neural network). These are updated as learning progresses.

- Advantage = How much better an action is compared to the average at that state.

---

### üéØ Policy Gradient Methods

Instead of estimating value functions, these methods **directly learn the policy**.

- Objective function:

  $$
  J(\theta) = \text{Expected return based on parameters } \theta
  $$

- Update rule using gradient ascent:

  $$
  \theta_{t+1} = \theta_t + \Delta \theta_t, \quad \Delta \theta_t = G_t \nabla_\theta \log \pi(a_t | s_t)
  $$

  Where:

  - $\pi(a_t | s_t)$: probability of taking action $a_t$ in state $s_t$
  - $G_t$: return (sum of rewards)
  - This approach works well when using neural networks.

---

### ü™ú Multi-Step Temporal Difference (TD)

Instead of updating after every step, **multi-step TD** backs up rewards over several steps:

For 2-step TD:

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ r_{t+1} + \gamma r_{t+2} + \gamma^2 \max_a Q(s_{t+2}, a) - Q(s_t, a_t) \right]
$$

This allows the agent to learn from longer-term outcomes.

---

### üßæ Eligibility Traces

- Combines multiple-step TD learning into a single method.
- Tracks how recently each state-action pair was visited.
- Recently visited pairs get more credit when rewards arrive.
- Eligibility fades over time (decays), hence "trace."

---

### üß† Model-Based TD & Planning

- **Planning** means using a model of the environment to improve the policy.
- **Model-based TD** uses both learning and planning:

  - Learns a model of how the environment works.
  - Uses this model to simulate experiences and plan.

- **Dynamic Programming** is a classic planning method.
- **Monte Carlo (MC)** and **standard TD** are model-free.

---

### üßÆ Function Approximation

Why needed:

- Real-world problems have **too many states/actions** to store exact values in a table.

Solution:

- Use function approximators to estimate value functions:

  - Neural networks
  - Gaussian processes
  - Other regression models

This allows generalization to unseen states.

---

### ‚úÖ Wrap-up

This lecture covered many methods and ideas in reinforcement learning:

- **Reward shaping** to speed up learning.
- **Actor-Critic** splits learning into policy and value learning.
- **Policy gradient** directly optimizes the policy.
- **Multi-step TD** and **eligibility traces** improve learning from long-term outcomes.
- **Function approximation** makes learning scalable.
- **Model-based methods** combine learning and planning.

---
