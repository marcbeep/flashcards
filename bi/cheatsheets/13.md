---

## **1. Evaluation of Neural Networks**

### Error Functions

- **For regression**:
  Error is measured using the formula:
  **E = Σ (dᵢ − yᵢ)²**

  - _dᵢ_ = desired (target) output
  - _yᵢ_ = actual output

- **For classification**:
  Accuracy = (number of correctly classified samples) ÷ (total samples)

---

## **2. Perceptrons**

### Basic Perceptron

- **Net input**: net = Σ (wᵢ \* xᵢ)
- **Output**:

  - +1 if net > threshold
  - -1 otherwise

This models a decision boundary using a linear function.

### Including Bias

- Represent bias as a fixed input _x₀ = 1_ with a weight _w₀ = -threshold_
- Now the threshold is absorbed into the weights:
  net = w₀ \* x₀ + w₁ \* x₁ + ... + wₙ \* xₙ

---

## **3. Perceptron Computation**

- The decision boundary is a hyperplane:
  **w₀ + w₁x₁ + ... + wₙxₙ = 0**
- If result > 0 → output is 1
  If result ≤ 0 → output is -1
- This only works if the data is **linearly separable**.

---

## **4. Perceptron Training**

- If a point is misclassified, update the weights:

  - For a point _x_ with class _y_, and current weight _w_, update as:
    **w ← w + ηy·x**
  - η is the learning rate (controls how big each update is)

This formula works for both classes (+1 and -1) by using _y_ directly in the update.

### Algorithm

1. Start with random weights
2. While misclassified points exist:

   - Pick one misclassified point
   - Update weights using the rule above

### Notes on Learning Rate

- Too **large**: training becomes unstable
- Too **small**: training is slow
- If tweaking η doesn’t help, the data may not be linearly separable

---

## **5. Visualizing and Example**

In 2D, the decision boundary is a line. You can find intercepts by setting x or y to 0 and solving.

### Learning Example (Step-by-Step)

1. Start with weights (2, 1, -2)
2. Use a misclassified point (-2, -1)

   - Add the vector (-1) \* x = (-1, 2, 1)
   - New weights: (1, 3, -1)

3. Next point: (0, 2)

   - Add (1, 0, 2)
   - New weights: (2, 3, 1)

4. All points correctly classified → stop

**Note:** Often more iterations are needed.

---

## **6. Generalization**

- Perfect training classification does **not** guarantee good performance on new data.
- Simpler or smoother decision boundaries often generalize better.

---

## **7. Multiclass Discrimination**

### One-vs-All

- Use one perceptron per class.
- Each perceptron outputs:

  - 1 if input belongs to its class
  - 0 otherwise

- The class is the one with the **highest** output.

### Visual Example

- Input x₁, x₂, ..., xₙ connected to multiple perceptron outputs o₁, o₂, o₃, o₄

---

## **8. Example Code (PyTorch)**

```python
from torch.nn import Linear, Sigmoid, Module

class MLP(Module):
    def __init__(self, n_inputs):
        super(MLP, self).__init__()
        self.layer = Linear(n_inputs, 1)
        self.activation = Sigmoid()

    def forward(self, X):
        X = self.layer(X)
        X = self.activation(X)
        return X
```

---

## **9. Multilayer Networks and Backpropagation**

### Problem with Single-Layer Networks

- Can’t handle **non-linearly separable** problems

### Solution

- Use **multiple layers** and a **differentiable activation function** (like sigmoid)
- This enables **gradient descent** learning

---

## **10. Backpropagation**

### Idea

- Adjust weights to minimize error between network output and desired output

### Error Function

- Use Mean Squared Error (MSE):
  **MSE = (1/N) Σ (yᵢ − dᵢ)²**

### Gradient Descent

- Repeatedly adjust weights in the direction that reduces the error:
  **w ← w − η \* ∂Error/∂w**

### Chain Rule

- Needed to compute gradients across layers
- For a composed function f(g(x)), the derivative is:
  **df/dx = (df/dg) \* (dg/dx)**

---

## **11. Sigmoid Neuron and Derivative**

### Sigmoid Function

- **σ(net) = 1 / (1 + e^(−net))**
- Smooth and differentiable
- Derivative:
  **σ’(net) = σ(net) \* (1 − σ(net))**

Used in backpropagation because it’s easy to compute and differentiable.

---

## **12. Feedforward and Backpropagation Steps**

1. **Feedforward**:

   - Pass inputs forward through network

2. **Backpropagate**:

   - Compute error at output
   - Propagate it backward using gradients
   - Adjust weights using gradient descent

---

## **Summary**

- Evaluating neural networks involves computing output errors.
- Perceptrons are linear classifiers trained with simple weight updates.
- Multiclass problems are handled with multiple perceptrons.
- Nonlinear problems require multilayer networks and backpropagation.
- Backpropagation uses gradient descent and the chain rule to compute and apply weight updates.
