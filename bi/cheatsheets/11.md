---

## Deep Learning

### What is Machine Learning?

- Machine Learning (ML) is a subset of Artificial Intelligence where machines learn from data to perform tasks without being explicitly programmed.
- It involves building models that can analyze data, learn patterns, and make predictions or decisions.

**Tom Mitchell’s definition:**

> A computer program learns from experience **E** with respect to tasks **T** and performance measure **P**, if its performance on **T**, measured by **P**, improves with **E**.

### Types of Learning in ML

1. **Supervised Learning** – Uses labelled data.

   - Example: Given images of apples (input) and labels "apple", the model learns to recognize apples.

2. **Unsupervised Learning** – Uses unlabelled data.

   - Example: The model groups similar images together without knowing what they are.

3. **Semi-supervised Learning** – Mix of labelled and unlabelled data.

4. **Reinforcement Learning** – Learns by interacting with an environment and receiving rewards or penalties.

---

### What is Deep Learning?

- Deep Learning (DL) is a part of ML that uses **deep architectures** (many layers of neurons) to learn **high-level features** from data.
- It mimics how the brain processes information through layered structures.

### Types of Deep Learning Architectures

1. **Convolutional Neural Networks (CNNs)** – Used for image classification and vision tasks.
2. **Autoencoders** – Compress data into lower dimensions and then reconstruct it (used for denoising or dimensionality reduction).
3. **Deep Belief Networks (DBNs)** – Layered networks that learn to represent data and generate new data.
4. **Recurrent Neural Networks (RNNs)** – Good for sequential data like time series or speech, using feedback connections as “memory”.

---

### Applications of Deep Learning

- **Computer Vision**: Object recognition, scene segmentation, 3D face reconstruction.
- **Self-driving Cars**: Real-time perception and decision-making.
- **Robotics**: Learning hand-eye coordination and control from data.
- **Other**: Natural language processing, medical diagnosis, game playing (e.g., AlphaGo).

---

### History and Breakthroughs

- **Early Days**:

  - 1943: McCulloch & Pitts – First mathematical model of a neuron.
  - 1949: Hebb – Learning rule (neurons that fire together, wire together).
  - 1958: Rosenblatt – Perceptron, first learning algorithm.
  - 1960s: ADALINE (Widrow & Hoff), early gradient descent models.

- **Setbacks**:

  - 1969: Minsky & Papert showed limitations of single-layer networks.

- **Renewed Interest**:

  - 1980s: Hopfield networks, Kohonen maps, Boltzmann machines.
  - 1986: Backpropagation algorithm (Rumelhart, Hinton, Williams) revived interest in multi-layer networks.

- **Modern Era**:

  - Advancements in computing and large datasets allowed deep networks to be trained effectively.
  - Key figures: LeCun (CNNs), Hinton (DBNs), Bengio, Ng (modern DL techniques).

---

### Neural Networks: From Biology to Artificial Systems

- **Biological Neurons**:

  - Receive signals through dendrites, process in the cell body, and send signals via axons.
  - Signal strength and timing carry information (spike frequency and phase).

- **Artificial Neural Networks (ANNs)**:

  - Built using nodes (neurons) and weights (synapses).
  - Each artificial neuron receives inputs, applies weights, computes an activation, and sends output to other neurons.
  - Simple in structure but capable of very complex behavior.

- **Information Flow**:

  - Inputs → Hidden layers (processing units) → Outputs
  - Recurrent networks allow feedback, enabling memory of previous inputs.

---

### Summary

- **Deep Learning** uses multi-layered neural networks to model complex patterns in data.
- Applications range across vision, robotics, and autonomous systems.
- Inspired by the brain, artificial neural networks have evolved through decades of research.
- The major breakthrough enabling current DL success is the ability to train deep networks effectively using algorithms like backpropagation.

---
