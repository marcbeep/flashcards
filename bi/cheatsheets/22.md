---

## Multi-Agent Reinforcement Learning (MARL)

### Independent Learners

- Each agent learns _as if it were alone_, ignoring others.
- Other agents’ behavior appears like noise in the environment.

**Pros:**

- Simple to implement using single-agent RL techniques.
- Easily scales to many agents.

**Cons:**

- No convergence guarantee.
- No coordination mechanism.

#### Example: Q-Learners in the “Battle of the Sexes”

|       | B    | F    |
| ----- | ---- | ---- |
| **B** | 2, 1 | 0, 0 |
| **F** | 0, 0 | 1, 2 |

- Two Q-learners learn based only on their own rewards.
- They converge to one of the two pure-strategy Nash equilibria (either (B,F) or (F,B)), depending on random initial conditions.
- Over time, each player improves their strategy, but without coordination.

### Joint Action Learners (JAL)

- Agents _observe what others do_.
- Estimate others’ policies and play optimally against those.
- Inspired by **fictitious play** (assume others use stationary strategies).

**Pros:**

- Enables coordination.

**Cons:**

- Requires observation of others.
- Complexity grows fast with more agents.

---

### Minimax Q-Learning

**For zero-sum games** (e.g., Rock-Paper-Scissors):

- One player’s gain is the other’s loss.
- Each agent tries to **maximize its minimum possible reward** (worst-case thinking).

Let:

- π₁, π₂, π₃ be probabilities of choosing Rock, Paper, Scissors.

Expected payoff depends on what the opponent plays.
E.g., if opponent plays Rock, agent’s payoff = π₂ − π₃
(only Paper beats Rock, Scissors loses).

**Goal:**
Maximize this minimum payoff across all possible opponent actions:

```
max π min (π₂ - π₃, -π₁ + π₃, π₁ - π₂)
```

This ensures the agent isn't easily exploited.

**Solution:**

- Solve with **linear programming** to find π that satisfies constraints.
- For Rock-Paper-Scissors, optimal π = (1/3, 1/3, 1/3), V = 0.

#### In Markov Games (multi-state):

- Redefine value function V(s) as expected reward from state s.
- Q(s, a, o) = expected reward if agent does action a, opponent does o.

**Update Rule:**

```
Q(s, a, o) ← (1 - α) Q(s, a, o) + α [r + γ V(s')]
```

Where:

- α = learning rate
- γ = discount factor
- V(s') is computed using a minimax over Q-values in next state.

**Benefits:**

- Converges under certain conditions.
- More robust than naive Q-learning.

**Limitation:**

- Only works for **zero-sum** games.

---

### Nash-Q Learning (for General-Sum Games)

- Extends minimax-Q to **non-zero-sum** games.
- Agents learn Q-values for all joint actions.

Update Rule:

```
Qᵢ(s, a₁, ..., aₙ) ← (1 - α) Qᵢ(s, a₁, ..., aₙ) + α [r + γ NashPayoffᵢ(s')]
```

**Challenges:**

- Need to compute **Nash equilibrium** at every step.
- Computationally hard.
- Multiple equilibria → selection problem (which one to use?).

---

### Gradient Ascent Approaches

Rather than learning Q-values, directly **adjust policy** using gradient of expected reward.

**Basic formula:**

```
Δπᵢ ← ∇Vᵢ(πᵢ)
πᵢ ← πᵢ + Δπᵢ
```

Examples:

- **IGA**: Infinitesimal Gradient Ascent.
- **GIGA**: Generalized IGA.

**Limitation:** Only works well for **2-player matrix games** (static, not dynamic).

---

### WoLF (“Win or Learn Fast”)

Improves gradient methods using **adaptive learning rate**:

- Learn **slowly** when winning (avoid overreacting).
- Learn **quickly** when losing (adapt fast).

Helps stabilize learning and improve convergence.

Variants:

- **WoLF-IGA, GIGA-WoLF, WoLF-PHC (Policy Hill Climbing)**

---

### Fully Cooperative Tasks

In these tasks, all agents **share the same reward**. The aim is to maximise total team performance.

**Q-learning update:**

```
Q(x, u) ← Q(x, u) + α [r + γ max Q(x', u') - Q(x, u)]
```

Where:

- x = current state
- u = joint action (all agents’ actions)
- r = shared reward
- α = learning rate
- γ = discount factor

**Problem:** Without coordination, agents might break ties differently and choose conflicting actions.

#### Example:

Two robots must choose to go Left or Right to avoid a crash while staying in formation.
Best outcomes: (Left, Left) or (Right, Right) → reward 10
Bad coordination: one goes left, one goes right → reward 0 or negative

**Coordination is critical.**

#### Solutions:

- **Team Q-learning**: assumes unique best joint action. Works only if that's true.
- **Distributed Q-learning**: each agent learns local Q-function. Works in deterministic environments only.

---

### Summary of MARL Approaches

**Main Families:**

- **Independent Learners**: simple but uncoordinated.
- **Joint Action Approaches**: coordinate by observing others.
- **Minimax-Q / Nash-Q**: handle competitive & general-sum games using game theory.
- **Gradient-based Methods**: directly tweak policies.
- **WoLF**: adjusts learning rates dynamically.

**Use Cases:**

- Robot teams
- Online auctions
- Self-managing systems
- Video games
- Military simulations

---
