---
## Artificial Neural Networks (ANNs)

### What Are ANNs?

- Inspired by biological neural networks (NNs), where learning happens by changing connections (synapses) between neurons.
- In ANNs, learning is modeled by adjusting weights of connections between artificial neurons.
---

## Artificial Neuron Model

An artificial neuron receives inputs and produces an output based on a function.

### Structure

- Inputs: $x_1, x_2, ..., x_n$

- Weights: $w_{i,1}, w_{i,2}, ..., w_{i,n}$ for neuron $i$

- Net input:

  $$
  \text{net}_i(t) = \sum_{j=1}^n w_{i,j}(t) \cdot x_j(t)
  $$

  This means: multiply each input by its weight, then sum everything.

- Output:

  $$
  y_i(t) = f_i(\text{net}_i(t))
  $$

  where $f_i$ is the activation function that transforms the net input into output.

---

## Activation Functions

These determine what output a neuron produces given its net input.

### 1. Threshold Function (used in Perceptrons)

$$
f_i(\text{net}_i(t)) =
\begin{cases}
1 & \text{if } \text{net}_i(t) \geq 0 \\
0 & \text{otherwise}
\end{cases}
$$

- Output is binary (0 or 1).
- Used in simple classification tasks.

### 2. Linear Function

- Output is a direct linear combination of inputs.
- Not useful alone for complex tasks since it's not nonlinear.

### 3. Sigmoid (Logistic) Function

$$
f_i(\text{net}_i(t)) = \frac{1}{1 + e^{-\frac{\text{net}_i(t) - \theta}{\tau}}}
$$

- Squashes input to range between 0 and 1.

- Parameters:

  - $\theta$: shifts the curve horizontally (like a threshold).
  - $\tau$: controls the steepness (slope) of the curve.

- Smooth and differentiable, making it suitable for learning algorithms like backpropagation.

---

## Neural Network Structure

### Feed-Forward Neural Network

- **Input Layer**: takes input vector, no computation.
- **Hidden Layer**: processes data using neurons with activation functions.
- **Output Layer**: produces final result (output vector).

Information flows from input → hidden → output.

---

## Learning in ANNs: Setting Weights

### Supervised Learning

- Use **training data**: pairs of input vector $x$ and desired output $y$.
- The goal: Adjust weights so the network's output approximates $y$ for each input $x$.

### Generalization vs Memorization

- The model should generalize well (work on new data) rather than just memorizing training data.

### Analogy: Fitting Functions

- Fitting a curve to points:

  - Too simple (e.g. line): underfitting
  - Too complex (e.g. 9th degree polynomial): overfitting

Same with ANNs:

- **Too few neurons**: not enough capacity → underfitting.
- **Too many neurons**: too much capacity → overfitting.

> There's no exact formula for choosing the perfect network size — only heuristics (rules of thumb).

---
