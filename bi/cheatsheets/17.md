---

## Deep Learning in Reinforcement Learning

### Why Deep Learning in RL?

- Traditional RL methods (like Q-tables) fail with large state spaces.
- For example, a small 4x4 grid = 16 states → manageable.
- But an 8x8 image = 64 pixels → 2⁶⁴ combinations → impractical.
- Larger images (like Atari games: 210x160x3 RGB) make state spaces huge.
- **Solution:** Use a **function approximator** (like a neural network) to estimate the Q-values, instead of a table.

---

### Deep Q-Learning (DQN)

#### What is it?

- Combines **Q-learning** (a type of RL algorithm) with a **deep neural network**.
- The neural network estimates the **Q-function**:

  $$
  Q(s, a; \theta) \approx Q^*(s, a)
  $$

  - $s$: state
  - $a$: action
  - $\theta$: neural network parameters (weights)

#### Q-Learning Formula Recap:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

- $\alpha$: learning rate
- $\gamma$: discount factor (future reward importance)
- $r$: reward
- $s'$: next state
- $a'$: next action

---

#### Loss Function in DQN

To train the network:

- Use **Mean Squared Error (MSE)** between the target and predicted Q-value:

  $$
  L(\theta) = \mathbb{E} \left[ \left( y - Q(s, a; \theta) \right)^2 \right]
  $$

  where the target $y$ is:

  $$
  y = r + \gamma \max_{a'} Q(s', a'; \theta^-)
  $$

- $\theta^-$: parameters of a target network (a periodically updated copy of the main network)

---

### Case Study: Playing Atari Games

#### Setup

- **State:** last 4 game frames → stacked as a tensor (shape: 84x84x4 after preprocessing)
- **Action:** game controls (e.g., Left, Right, Up, Down)
- **Reward:** score change at each step

#### DQN Architecture

- **Input:** 84x84x4 grayscale image (stack of frames)
- **Network:**

  - 2 Convolutional layers
  - 2 Fully Connected layers
  - Output layer: one Q-value per action

---

### Case Study: AlphaGo

#### Key Achievements

- In 2016, **AlphaGo** (by DeepMind) beat a top human Go player (Fan Hui), and later world champion Lee Sedol.
- Combined:

  - **Convolutional neural networks (CNNs)** – extract features from Go board
  - **Reinforcement learning** – learn by playing itself
  - **Monte Carlo Tree Search (MCTS)** – to plan several moves ahead

#### How It Worked

- Trained first on expert human games (supervised learning).
- Then improved by playing against itself (reinforcement learning).
- Used tree search to evaluate possible moves.

---

### Gym by OpenAI

- **Gym** is an open-source toolkit to develop and compare RL algorithms.
- Provides environments like Atari, classic control tasks, etc.
- URL: [https://gym.openai.com](https://gym.openai.com)

---

### Final Takeaways

- **Deep RL** can tackle tasks with huge state spaces.
- **CNNs** aren't just for images – they help extract useful state features in games.
- **Combining methods** (supervised learning, RL, tree search) leads to strong AI systems.
- But: AI may play games well, but may never appreciate them like humans.

> "Robots will never understand the beauty of the game the same way that we humans do." – Lee Sedol

---
