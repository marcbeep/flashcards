---

## **Reinforcement Learning (RL)**

### **1. Solving Bellman Optimality Equation**

Goal: Find the best way to act to maximize future rewards.

Three main approaches:

- **Dynamic Programming (DP)**
- **Monte Carlo (MC)**
- **Temporal Difference (TD)**

---

## **2. Dynamic Programming (DP)**

- Used when you **know the full model of the environment** (i.e., probabilities and rewards).
- Works well if you have **enough memory and time**, and if the **Markov Property** holds (future depends only on current state, not past states).

### **DP Value Update Rule**:

$$
V(s_t) \leftarrow \mathbb{E}[r_{t+1} + \gamma V(s_{t+1})]
$$

- $V(s_t)$: value of current state
- $r_{t+1}$: reward received after taking action
- $\gamma$: discount factor (between 0 and 1), reduces value of future rewards
- $V(s_{t+1})$: estimated value of next state

**Key idea**: Update value of a state by averaging the expected reward plus future value.

**Limitations**: Rarely used directly because full environment info is usually unavailable.

---

## **3. Monte Carlo (MC) Methods**

- Learn directly from **complete episodes** of experience.
- **No need to know environment model**.
- **Only works for episodic tasks** (tasks that end eventually).

### **MC Policy Evaluation** (Estimate $V(s)$):

- **First-visit MC**: Average returns the first time a state is visited in each episode.
- **Every-visit MC**: Average all returns after every visit to a state.
- Both methods **converge over time** with enough episodes.

### **MC Value Update Rule (Simplified)**:

$$
V(s_t) \leftarrow V(s_t) + \alpha (R_t - V(s_t))
$$

- $\alpha$: learning rate
- $R_t$: return after visiting state $s_t$

### **Monte Carlo Estimation for Action-Value $Q(s,a)$**:

- Estimate expected return from a **state-action pair**.
- Needs **exploring starts**: every state-action pair must have a non-zero chance of being tried.

### **MC Control (Policy Improvement)**:

- Alternate between:

  1. **Evaluating current policy** using MC.
  2. **Improving policy** by choosing better actions (greedy).

- **Converges** with enough episodes and exploration.

**Advantages**:

- Great when state space is large but only a few actions matter (e.g., games like Go, Backgammon).

---

## **4. Temporal Difference (TD) Methods**

- Combine ideas of DP and MC.
- **Do not need full model** of the environment.
- Can **learn online, step-by-step**, without waiting for episodes to finish.

### **TD Value Update Rule**:

$$
V(s_t) \leftarrow V(s_t) + \alpha [r_{t+1} + \gamma V(s_{t+1}) - V(s_t)]
$$

- Update current value estimate based on:

  - actual reward received
  - estimated value of next state
  - current estimate

This is called **bootstrapping**—you update based on other estimates.

### **Tic-Tac-Toe Example**:

$$
V(a) \leftarrow V(a) + \alpha [V(c) - V(a)] \\
V(e) \leftarrow V(e) + \alpha [V(g) - V(e)]
$$

Where `a → c` and `e → g` are transitions between board states.

---

## **5. Summary Comparison**

| Method              | Needs Environment Model? | Learns From   | Updates Based On              | Works Online? |
| ------------------- | ------------------------ | ------------- | ----------------------------- | ------------- |
| Dynamic Programming | Yes                      | Full model    | Expected future values        | No            |
| Monte Carlo         | No                       | Full episodes | Actual returns                | No            |
| Temporal Difference | No                       | Step-by-step  | Reward + estimated next value | Yes           |

---
