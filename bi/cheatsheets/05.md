---

## **1. Reinforcement Learning**

### Roots and Key Features

- **Reinforcement Learning (RL)** is inspired by how agents (biological or artificial) learn from interactions.
- Focuses on **learning by trial and error**, with feedback from actions taken.

### Elements of RL

- **Agent**: Learner or decision maker.
- **Environment**: Everything the agent interacts with.
- **Actions and Rewards**: Agent performs actions and receives feedback (rewards).

---

## **2. Problem Solving from Nature**

- RL and other methods take inspiration from biological systems (e.g., learning mechanisms, swarm behavior).

---

## **3. Multi-Armed Bandit Problems**

### Core Scenario

- **You have multiple slot machines (arms)**, each with unknown reward probabilities.
- You must decide which to play to maximize total reward over a finite number of plays.

### Exploration vs. Exploitation

- **Exploration**: Try new actions to discover their potential.
- **Exploitation**: Choose the best-known action so far.
- **Dilemma**: Must balance both to perform well long-term.

  Formula:

  - Greedy choice: $a_t^* = \arg\max Q_t(a)$
  - Explore: pick $a_t \ne a_t^*$
  - $Q_t(a) \approx Q^*(a)$: estimated vs. true value

---

## **4. Evaluative Feedback**

### Feedback Types

- **Evaluative**: Based only on actions taken (no explicit instructions).
- **Instructive**: Tells what to do (ideal actions).

### Learning Styles

- **Associative**: Learn best output for each input.
- **Nonassociative**: Learn a single best action overall.
- Multi-armed bandits use **evaluative feedback** and are **nonassociative**.

---

## **5. Action-Value Methods**

### Sample Average Estimation

- If action $a$ was chosen $k$ times with rewards $r_1, ..., r_k$:

  $$
  Q_t(a) = \frac{r_1 + r_2 + \dots + r_k}{k}
  $$

### Incremental Update

- Update estimate without storing all data:

  $$
  Q_{n+1} = Q_n + \frac{1}{k+1} (r - Q_n)
  $$

- General form:

  $$
  \text{New Estimate} = \text{Old Estimate} + \text{Step Size} \times (\text{Target} - \text{Old Estimate})
  $$

---

## **6. Nonstationary Problems**

- When reward probabilities change over time:

  - Use **constant step-size** $\alpha$ instead of sample average.

  $$
  Q_{n+1} = Q_n + \alpha (r - Q_n)
  $$

- Equivalent to **recency-weighted average**.

---

## **7. Action Selection Methods**

### ε-Greedy Method

- Choose best-known action with probability $1 - \varepsilon$.
- Random action with probability $\varepsilon$.

  $$
  a =
  \begin{cases}
  a^* & \text{with probability } 1 - \varepsilon \\
  \text{random} & \text{with probability } \varepsilon
  \end{cases}
  $$

### Softmax Action Selection

- Uses probability distribution:

  $$
  P(a) = \frac{e^{Q(a)/\tau}}{\sum_b e^{Q(b)/\tau}}
  $$

  Where $\tau$ is the temperature parameter.

---

## **8. Example: 10-Armed Testbed**

- 10 arms with unknown rewards.
- Simulations with:

  - **Greedy**, **ε-Greedy**, and **Softmax** methods.

- Each experiment repeated 1000 times over 1000 plays and averaged.

---

## **9. Summary**

- **Multi-Armed Bandit Problem**
- **Exploration vs. Exploitation Dilemma**
- **Action-Value Methods**
- **Action Selection Strategies**
- **Stationary vs. Nonstationary Settings**

---
