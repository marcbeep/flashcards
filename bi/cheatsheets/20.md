---

## 🧠 Multi-Agent Reinforcement Learning (MARL)

### 🧩 The Problem: Single-Agent vs. Multi-Agent Learning

#### In **Single-Agent Reinforcement Learning (RL)**:

- The agent learns in a **stationary** environment (nothing changes over time).
- You can use tools like **Dynamic Programming**, **Monte Carlo methods**, or **Q-learning** to learn optimal behavior.

#### In **Multi-Agent Learning (MAL)**:

- Each agent is **learning simultaneously**, making the environment **non-stationary**.
- It’s more complex because other agents’ actions can change.
- Two main settings:

  - **Cooperative**: agents share goals.
  - **Self-interested (competitive)**: agents have conflicting goals.

- Agents may:

  - Not know others’ payoffs.
  - Not know others’ learning strategies.
  - Still need to make good decisions.

#### Goals in MAL:

- Learn a **stationary strategy**.
- Reach a **joint equilibrium** when all agents use the same algorithm (self-play).
- Perform well as a **best response** to others.
- Guarantee a **minimum payoff** even in worst-case scenarios.

---

## 🎲 Game Theory – The Foundation of Multi-Agent Systems

Game theory studies how agents make decisions when outcomes depend on other agents' choices.

### 🔍 Game Elements:

- **Players**: the agents.
- **Actions**: choices each player can make.
- **Payoffs**: rewards players receive based on all players' actions.

#### Types of Games:

- **Normal-form games** (matrix form): all at once, one-shot decisions.
- **Extensive-form games**: turns, sequences.
- **Repeated games**: played over multiple rounds.
- **Stochastic or Markov games**: stateful games with transitions.

### 🎮 Example: Rock-Paper-Scissors (RPS)

- **Zero-sum**: one player’s gain = the other’s loss.
- **Symmetric**: identical rules for both.
- **Mixed strategy equilibrium**: both play randomly with equal probabilities (1/3, 1/3, 1/3).

### 🤝 Cooperative vs. Competitive:

- **Cooperative** if payoff matrices are identical (shared rewards).
- **Competitive** if payoffs conflict (e.g. zero-sum).

---

## 🔁 Repeated Games & Learning Potential

### 🎭 Example: Prisoner's Dilemma

|              | Confess (Defect) | Hold Out (Cooperate) |
| ------------ | ---------------- | -------------------- |
| **Confess**  | (1,1)            | (5,0)                |
| **Hold Out** | (0,5)            | (3,3)                |

- **Dominant Strategy**: each player is best off confessing, no matter what the other does.
- **Equilibrium (Confess, Confess)**: stable but not socially optimal.
- **Opportunity in learning**: repeated play may allow agents to learn to **cooperate** (3,3), reaching better social outcomes over time.

---

## 🧠 Understanding Strategies and Equilibria

- **Pure strategy**: always pick the same action.
- **Mixed strategy**: choose randomly among actions with specific probabilities.

### Key Concepts:

- **Best Response**: best action assuming other players' strategies.
- **Dominant Strategy**: best action no matter what others do.
- **Nash Equilibrium**: set of strategies where no player benefits by changing alone.

  > Always exists but can be **multiple** and **hard to compute**.

---

## 📌 Key Takeaways

- Multi-agent learning is **fundamentally different** from single-agent due to non-stationarity.
- **Game theory** provides the basic tools for analyzing interactions.
- **Repeated interaction** opens the door to learning cooperative behavior.
- Understanding **dominant strategies** and **Nash equilibria** is critical.
- **Practical examples**: games (Poker, WoW), economics (OPEC), markets (auctions), negotiations.

---
