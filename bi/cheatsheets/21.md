---

## **1. Game Theory Basics in Multi-Agent Settings**

### **Prisoners’ Dilemma**

- **Setup:** Two players each choose between “Confess” (Defect) or “Hold out” (Cooperate).

  |                          | Prisoner 2: Confess | Prisoner 2: Hold Out |
  | ------------------------ | ------------------- | -------------------- |
  | **Prisoner 1: Confess**  | (1,1)               | (5,0)                |
  | **Prisoner 1: Hold Out** | (0,5)               | (3,3)                |

- **Dominant Strategy:** Always Confess (Defect), because it's the best response no matter what the other does.

- **Dominant Strategy Equilibrium:** (Confess, Confess), payoff = (1,1)

- **Social Conflict:** (3,3) is better for both, but rational players don’t pick it. Multi-agent learning might help reach the better (cooperative) outcome over time.

---

### **Battle of the Sexes**

- **Scenario:** Alice prefers Ballet, Bob prefers Football. They want to do something together.

  |                     | Bob: Ballet | Bob: Football |
  | ------------------- | ----------- | ------------- |
  | **Alice: Ballet**   | (2,1)       | (0,0)         |
  | **Alice: Football** | (0,0)       | (1,2)         |

- **No dominant strategy equilibrium**

- **Two Pure Nash Equilibria:**

  - (Ballet, Ballet): Alice gets 2, Bob gets 1
  - (Football, Football): Alice gets 1, Bob gets 2

- **Mixed Strategy Equilibrium:**
  Players choose their action randomly based on certain probabilities so each is indifferent.

  For Alice to be indifferent:

  $$
  2y_B = y_F \Rightarrow y_B = \frac{1}{3}, y_F = \frac{2}{3}
  $$

  (y = Bob’s strategy)

  Similarly, Bob will:

  $$
  x_F = \frac{1}{3}, x_B = \frac{2}{3}
  $$

  (x = Alice’s strategy)

- **Problem:** Mixed strategies lead to **lower expected payoffs** due to miscoordination risk.

---

## **2. Simple Multi-Agent Learning Approaches**

### **Fictitious Play**

- Each agent observes others’ past actions and assumes they'll keep using the same pattern.
- The agent then plays the best response to the **estimated frequency** of others’ actions.
- Variants include:

  - Recency-weighted observations
  - Softmax-style responses
  - Small adjustments toward best response

### **If all agents use fictitious play:**

- Strict Nash equilibria are **stable** outcomes.
- Sometimes leads to **cycling behaviors** (strategies go in loops).
- Empirical distributions (averaged actions over time) can still converge to Nash equilibrium.

---

## **3. Stochastic or Markov Games**

### **What are they?**

- Like repeated games, **but with states**.
- Each **state** is a separate game.
- The game transitions between states depending on **joint actions** taken by agents.

### **Key Components:**

- $n$: number of agents
- $S$: set of states
- $R$: rewards/payoffs
- $P$: transition probabilities (how actions lead to new states)
- $\gamma$: discount factor (how future rewards are valued)

### **Example:**

- A game with grid-like states and random movement depending on actions.
- Agents move between states with 30% or 50% probability.
- $\gamma = 0.9$ → future rewards matter a lot.

---

## **4. Learning in Stochastic Games**

### **Why Learn?**

- Nash Equilibria are hard to calculate in these games.
- Agents may **not know**:

  - Their own rewards fully
  - Others’ rewards
  - Transition model
  - Other agents' strategies

### **Learning Strategies:**

Adapted from **Single-Agent Reinforcement Learning (RL):**

- **Independent Learning:** Learn without modeling others.
- **Joint Action Learning:** Model other agents’ actions.
- **Minimax-Q Learning:** For zero-sum games (one's gain = other's loss).
- **Nash-Q Learning:** Learn equilibrium strategies using game theory.
- **Correlated Equilibrium (CE) Q-Learning:** More general than Nash; allows coordination based on shared signals.

---

## **5. Special Case: Zero-Sum Stochastic Games**

- Rewards always add up to a constant (e.g. one wins what the other loses).
- **Nice properties:**

  - All equilibria have the same value.
  - You can use **value iteration** (like in single-agent MDPs) with a **minimax operator** to find Nash equilibrium.
  - Has a Bellman-style update rule (for value estimation).

---

## **6. Spectrum of Multi-Agent Learning (MAL)**

From **Independent Q-Learners** (treat others as part of the environment)

TO

**Joint Action Learners** (fully model and respond to other agents)

> **Modern algorithms lie somewhere in between**, balancing complexity and coordination.

---
